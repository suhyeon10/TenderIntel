"""
ê¸°ì¡´ legal_chunks ë°ì´í„°ì˜ íŒŒì¼ë“¤ì„ Storageì— ì—…ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

1. legal_chunks í…Œì´ë¸”ì—ì„œ title, source_type, external_id ì¡°íšŒ
2. external_idë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ íŒŒì¼ëª… í™•ì¸
3. backend/data/legal/ í´ë”ì—ì„œ ì›ë³¸ íŒŒì¼ ì°¾ê¸°
4. Storage ë²„í‚· "legal-sources"ì— external_id ì´ë¦„ìœ¼ë¡œ ì—…ë¡œë“œ
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
from collections import defaultdict
from datetime import datetime
import hashlib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from supabase import create_client
from config import settings
from core.logging_config import get_logger

logger = get_logger(__name__)

STORAGE_BUCKET = "legal-sources"


def get_supabase_client():
    """Supabase í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    supabase_url = os.getenv("SUPABASE_URL") or settings.supabase_url
    supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or settings.supabase_service_role_key
    
    if not supabase_url or not supabase_key:
        raise ValueError("SUPABASE_URLê³¼ SUPABASE_SERVICE_ROLE_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤")
    
    return create_client(supabase_url, supabase_key)


def get_file_hash(file_path: Path) -> str:
    """íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ í•´ì‹œ ìƒì„± (external_idì™€ ë™ì¼í•œ ë°©ì‹)"""
    relative_path = str(file_path.relative_to(backend_dir))
    return hashlib.md5(relative_path.encode("utf-8")).hexdigest()


def find_file_by_external_id(
    external_id: str, 
    title: str, 
    source_type: str,
    file_path_hint: Optional[str] = None,
    metadata: Optional[Dict] = None
) -> Optional[Path]:
    """
    external_idì™€ ì—¬ëŸ¬ íŒíŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›ë³¸ íŒŒì¼ ì°¾ê¸°
    
    ìš°ì„ ìˆœìœ„:
    1. file_path_hint (DBì— ì €ì¥ëœ file_path)
    2. metadata.filename ë˜ëŠ” metadata.original_file_name
    3. titleë¡œ ì§ì ‘ ì°¾ê¸° + í•´ì‹œ í™•ì¸
    4. ëª¨ë“  íŒŒì¼ ìˆœíšŒí•˜ë©° í•´ì‹œ í™•ì¸
    
    Args:
        external_id: íŒŒì¼ í•´ì‹œ
        title: íŒŒì¼ëª… (í•œê¸€ í¬í•¨ ê°€ëŠ¥)
        source_type: 'law' | 'manual' | 'case' | 'standard_contract'
        file_path_hint: DBì— ì €ì¥ëœ file_path (ì˜ˆ: "standard_contracts/5989a3a3....pdf")
        metadata: metadata JSON ê°ì²´
    
    Returns:
        íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
    """
    # source_typeì— ë”°ë¥¸ í´ë”ëª… ë§¤í•‘
    folder_mapping = {
        "law": "laws",
        "manual": "manuals",
        "case": "cases",
        "standard_contract": "standard_contracts",
    }
    
    folder_name = folder_mapping.get(source_type, "other")
    legal_dir = backend_dir / "data" / "legal" / folder_name
    
    if not legal_dir.exists():
        logger.warning(f"  [WARN] í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {legal_dir}")
        return None
    
    # ë°©ë²• 1: file_path_hint ìš°ì„  ì‚¬ìš© (ê°€ì¥ í™•ì‹¤)
    if file_path_hint:
        # file_pathê°€ "standard_contracts/5989a3a3....pdf" í˜•íƒœë©´
        if "/" in file_path_hint:
            # ìƒëŒ€ ê²½ë¡œë¡œ í•´ì„
            candidate = backend_dir / "data" / "legal" / file_path_hint
        else:
            # íŒŒì¼ëª…ë§Œ ìˆìœ¼ë©´ í•´ë‹¹ í´ë”ì—ì„œ ì°¾ê¸°
            candidate = legal_dir / file_path_hint
        
        if candidate.is_file():
            # í•´ì‹œ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
            if get_file_hash(candidate) == external_id:
                return candidate
    
    # ë°©ë²• 2: metadata.filename ì‚¬ìš©
    if metadata:
        filename = metadata.get("filename") or metadata.get("original_file_name")
        if filename:
            candidate = legal_dir / filename
            if candidate.is_file():
                if get_file_hash(candidate) == external_id:
                    return candidate
    
    # ë°©ë²• 3: titleë¡œ ì§ì ‘ ì°¾ê¸° + í•´ì‹œ í™•ì¸
    possible_files = list(legal_dir.glob(title))
    if possible_files:
        for file_path in possible_files:
            if get_file_hash(file_path) == external_id:
                return file_path
    
    # ë°©ë²• 4: ëª¨ë“  íŒŒì¼ì„ ìˆœíšŒí•˜ë©° í•´ì‹œ í™•ì¸
    supported_extensions = ['.pdf', '.hwp', '.hwpx', '.txt', '.md']
    for ext in supported_extensions:
        for file_path in legal_dir.glob(f"*{ext}"):
            if get_file_hash(file_path) == external_id:
                return file_path
    
    # ë°©ë²• 5: titleê³¼ ìœ ì‚¬í•œ íŒŒì¼ëª… ì°¾ê¸° (í•œê¸€ íŒŒì¼ëª… ë¬¸ì œ ëŒ€ë¹„)
    title_without_ext = Path(title).stem
    for ext in supported_extensions:
        for file_path in legal_dir.glob(f"*{ext}"):
            if title_without_ext in file_path.stem or file_path.stem in title_without_ext:
                if get_file_hash(file_path) == external_id:
                    return file_path
    
    return None


def upload_file_to_storage(supabase, file_path: Path, external_id: str, source_type: str) -> Tuple[bool, str]:
    """
    íŒŒì¼ì„ Storageì— ì—…ë¡œë“œ
    
    Args:
        supabase: Supabase í´ë¼ì´ì–¸íŠ¸
        file_path: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        external_id: íŒŒì¼ ID (ì—…ë¡œë“œ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©)
        source_type: ì†ŒìŠ¤ íƒ€ì… (í´ë” êµ¬ì¡°ìš©)
    
    Returns:
        (ì„±ê³µ ì—¬ë¶€, Storage ê²½ë¡œ)
    """
    try:
        # í™•ì¥ì ì¶”ì¶œ
        ext = file_path.suffix.lower() or ".pdf"
        
        # Storage object path: {source_type}/{external_id}{ext}
        folder_mapping = {
            "law": "laws",
            "manual": "manuals",
            "case": "cases",
            "standard_contract": "standard_contracts",
        }
        folder_name = folder_mapping.get(source_type, "other")
        object_path = f"{folder_name}/{external_id}{ext}"
        
        # content-type ë§¤í•‘
        content_type_map = {
            ".pdf": "application/pdf",
            ".txt": "text/plain",
            ".md": "text/markdown",
            ".hwp": "application/x-hwp",
            ".hwpx": "application/x-hwpx",
        }
        content_type = content_type_map.get(ext, "application/octet-stream")
        
        # íŒŒì¼ ì½ê¸°
        with open(file_path, "rb") as f:
            file_data = f.read()
        
        # Storageì— ì—…ë¡œë“œ
        supabase.storage.from_(STORAGE_BUCKET).upload(
            object_path,
            file_data,
            {
                "content-type": content_type,
                "upsert": "true",  # ë¬¸ìì—´ë¡œ ì „ë‹¬
            }
        )
        
        logger.info(f"  âœ… ì—…ë¡œë“œ ì™„ë£Œ: {object_path}")
        return True, object_path
        
    except Exception as e:
        logger.error(f"  âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False, ""


def update_file_path_in_db(supabase, external_id: str, storage_path: str) -> bool:
    """
    legal_chunks í…Œì´ë¸”ì˜ file_pathë¥¼ Storage ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
    
    Args:
        supabase: Supabase í´ë¼ì´ì–¸íŠ¸
        external_id: íŒŒì¼ ID
        storage_path: Storage ê²½ë¡œ
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        # external_idë¡œ ëª¨ë“  ì²­í¬ ì—…ë°ì´íŠ¸
        result = supabase.table("legal_chunks")\
            .update({"file_path": storage_path})\
            .eq("external_id", external_id)\
            .execute()
        
        logger.info(f"  âœ… DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(result.data) if result.data else 0}ê°œ ì²­í¬")
        return True
        
    except Exception as e:
        logger.error(f"  âŒ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("[ì‹œì‘] legal_chunks íŒŒì¼ë“¤ì„ Storageì— ì—…ë¡œë“œ")
    logger.info("=" * 60)
    
    # Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    try:
        supabase = get_supabase_client()
        logger.info("âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return
    
    # ë²„í‚· í™•ì¸
    try:
        buckets = supabase.storage.list_buckets()
        bucket_names = {b["name"] for b in buckets} if isinstance(buckets, list) else set()
        
        if STORAGE_BUCKET not in bucket_names:
            logger.warning(f"[WARN] '{STORAGE_BUCKET}' ë²„í‚·ì´ ì—†ìŠµë‹ˆë‹¤!")
            logger.info(f"\n[NOTE] ë²„í‚· ìƒì„± ë°©ë²•:")
            logger.info(f"   1. Supabase ëŒ€ì‹œë³´ë“œ ì ‘ì†")
            logger.info(f"   2. Storage > Buckets > New bucket")
            logger.info(f"   3. Name: {STORAGE_BUCKET}")
            logger.info(f"   4. Public: Yes (ë˜ëŠ” Private + RLS ì •ì±… ì„¤ì •)")
            return
        else:
            logger.info(f"[OK] '{STORAGE_BUCKET}' ë²„í‚· í™•ì¸ë¨")
    except Exception as e:
        logger.error(f"[FAIL] ë²„í‚· í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return
    
    # legal_chunksì—ì„œ title, source_type, external_id, file_path, metadata ì¡°íšŒ
    logger.info("\n[INFO] legal_chunks í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    try:
        result = supabase.table("legal_chunks")\
            .select("title, source_type, external_id, file_path, metadata")\
            .range(0, 9999)\
            .execute()
        
        if not result.data:
            logger.warning("legal_chunksì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"[OK] {len(result.data)}ê°œ ì²­í¬ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"[FAIL] ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return
    
    # external_idë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ íŒŒì¼ì˜ ì—¬ëŸ¬ ì²­í¬ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
    file_info: Dict[str, Dict] = {}
    missing_ext_id = 0
    
    for chunk in result.data:
        external_id = chunk.get("external_id")
        if not external_id:
            missing_ext_id += 1
            continue
        
        # external_idë³„ë¡œ ì²« ë²ˆì§¸ ì •ë³´ë§Œ ì €ì¥ (ê°™ì€ íŒŒì¼ì´ë¯€ë¡œ)
        if external_id not in file_info:
            file_info[external_id] = {
                "title": chunk.get("title", ""),
                "source_type": chunk.get("source_type", "law"),
                "external_id": external_id,
                "file_path": chunk.get("file_path"),
                "metadata": chunk.get("metadata") or {},
            }
    
    if missing_ext_id > 0:
        logger.warning(f"[WARN] external_id ì—†ëŠ” ì²­í¬ ìˆ˜: {missing_ext_id}ê°œ")
    
    logger.info(f"ğŸ“ ê³ ìœ  íŒŒì¼ ê°œìˆ˜: {len(file_info)}ê°œ")
    
    # ê° íŒŒì¼ì„ ì°¾ì•„ì„œ ì—…ë¡œë“œ
    logger.info("\nğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘...")
    logger.info("=" * 60)
    
    success_count = 0
    failed_count = 0
    not_found_count = 0
    
    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    success_files: List[Dict] = []
    failed_files: List[Dict] = []
    not_found_files: List[Dict] = []
    
    for idx, (external_id, info) in enumerate(file_info.items(), 1):
        title = info["title"]
        source_type = info["source_type"]
        
        logger.info(f"\n[{idx}/{len(file_info)}] {title}")
        logger.info(f"  ğŸ“„ external_id: {external_id[:8]}...")
        logger.info(f"  ğŸ“‚ source_type: {source_type}")
        
        # ì›ë³¸ íŒŒì¼ ì°¾ê¸°
        file_path = find_file_by_external_id(external_id, title, source_type)
        
        if not file_path:
            logger.warning(f"  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {title}")
            not_found_count += 1
            not_found_files.append({
                "external_id": external_id,
                "title": title,
                "source_type": source_type,
                "reason": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            })
            continue
        
        logger.info(f"  âœ… íŒŒì¼ ì°¾ìŒ: {file_path.relative_to(backend_dir)}")
        
        # Storageì— ì—…ë¡œë“œ
        success, storage_path = upload_file_to_storage(supabase, file_path, external_id, source_type)
        
        if success:
            # DBì˜ file_path ì—…ë°ì´íŠ¸
            if update_file_path_in_db(supabase, external_id, storage_path):
                success_count += 1
                success_files.append({
                    "external_id": external_id,
                    "title": title,
                    "source_type": source_type,
                    "storage_path": storage_path,
                    "local_path": str(file_path.relative_to(backend_dir))
                })
            else:
                failed_count += 1
                failed_files.append({
                    "external_id": external_id,
                    "title": title,
                    "source_type": source_type,
                    "storage_path": storage_path,
                    "local_path": str(file_path.relative_to(backend_dir)),
                    "reason": "DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨"
                })
        else:
            failed_count += 1
            failed_files.append({
                "external_id": external_id,
                "title": title,
                "source_type": source_type,
                "local_path": str(file_path.relative_to(backend_dir)),
                "reason": "Storage ì—…ë¡œë“œ ì‹¤íŒ¨"
            })
    
    # ê²°ê³¼ ìš”ì•½
    logger.info("")
    logger.info("=" * 60)
    logger.info("[ì™„ë£Œ] ì—…ë¡œë“œ ê²°ê³¼:")
    logger.info(f"  âœ… ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"  âš ï¸  íŒŒì¼ ì—†ìŒ: {not_found_count}ê°œ")
    logger.info(f"  ğŸ“Š ì´ íŒŒì¼: {len(file_info)}ê°œ")
    logger.info("=" * 60)
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    output_dir = backend_dir / "output"
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"legal_files_upload_result_{timestamp}.json"
    
    result_data = {
        "summary": {
            "total_files": len(file_info),
            "success_count": success_count,
            "failed_count": failed_count,
            "not_found_count": not_found_count,
            "timestamp": datetime.now().isoformat()
        },
        "success_files": success_files,
        "failed_files": failed_files,
        "not_found_files": not_found_files
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ“„ ê²°ê³¼ íŒŒì¼ ì €ì¥: {output_file.relative_to(backend_dir)}")
    logger.info(f"   - ì„±ê³µ: {len(success_files)}ê°œ")
    logger.info(f"   - ì‹¤íŒ¨: {len(failed_files)}ê°œ")
    logger.info(f"   - íŒŒì¼ ì—†ìŒ: {len(not_found_files)}ê°œ")


if __name__ == "__main__":
    main()
