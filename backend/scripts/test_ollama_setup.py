"""
Ollama ì„¤ì • í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
ì¹œêµ¬ PCì—ì„œ Ollamaê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ìš©ë„
"""

import sys
import warnings
from pathlib import Path

# langchain-communityì˜ Ollama Deprecated ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning, module="langchain")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_ollama_setup():
    """Ollama ì„¤ì • í™•ì¸"""
    print("=" * 60)
    print("Ollama ì„¤ì • í™•ì¸")
    print("=" * 60)
    
    # 1. langchain-ollama íŒ¨í‚¤ì§€ í™•ì¸
    print("\n1. langchain-ollama íŒ¨í‚¤ì§€ í™•ì¸...")
    try:
        import langchain_ollama
        print("   âœ… langchain-ollama ì„¤ì¹˜ë¨")
    except ImportError:
        print("   âŒ langchain-ollama ì„¤ì¹˜ ì•ˆ ë¨")
        print("   í•´ê²°: pip install langchain-ollama")
        return False
    
    # 2. config ì„¤ì • í™•ì¸
    print("\n2. ì„¤ì • í™•ì¸...")
    try:
        from config import settings
        print(f"   LLM Provider: {settings.llm_provider}")
        print(f"   Ollama URL: {settings.ollama_base_url}")
        print(f"   Ollama Model: {settings.ollama_model}")
        print(f"   use_ollama: {settings.use_ollama}")
        
        if not settings.use_ollama:
            print("   âš ï¸ use_ollamaê°€ Falseì…ë‹ˆë‹¤.")
            print("   í•´ê²°: .env íŒŒì¼ì— LLM_PROVIDER=ollama ì„¤ì •")
    except Exception as e:
        print(f"   âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return False
    
    # 3. Ollama ì„œë²„ ì—°ê²° í™•ì¸
    print("\n3. Ollama ì„œë²„ ì—°ê²° í™•ì¸...")
    try:
        import httpx
        import asyncio
        
        async def check_ollama_server():
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(f"{settings.ollama_base_url}/api/tags")
                    if response.status_code == 200:
                        models_data = response.json()
                        available_models = [model.get("name", "") for model in models_data.get("models", [])]
                        print(f"   âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
                        print(f"   ì„¤ì¹˜ëœ ëª¨ë¸: {', '.join(available_models) if available_models else '(ì—†ìŒ)'}")
                        
                        # ì„¤ì •ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
                        model_name = settings.ollama_model.split(":")[0] if ":" in settings.ollama_model else settings.ollama_model
                        available_model_names = [name.split(":")[0] for name in available_models]
                        
                        if model_name in available_model_names:
                            print(f"   âœ… ì„¤ì •ëœ ëª¨ë¸ '{settings.ollama_model}' ì„¤ì¹˜ë¨")
                            return True
                        else:
                            print(f"   âŒ ì„¤ì •ëœ ëª¨ë¸ '{settings.ollama_model}' ì—†ìŒ")
                            print(f"   í•´ê²°: ollama pull {settings.ollama_model}")
                            return False
                    else:
                        print(f"   âŒ Ollama ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨ (HTTP {response.status_code})")
                        return False
            except Exception as e:
                print(f"   âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
                print(f"   í•´ê²°: ollama serve ì‹¤í–‰ í™•ì¸")
                return False
        
        result = asyncio.run(check_ollama_server())
        if not result:
            return False
    except ImportError:
        print("   âš ï¸ httpxê°€ ì—†ì–´ì„œ ì„œë²„ í™•ì¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("   í•´ê²°: pip install httpx")
    except Exception as e:
        print(f"   âŒ ì„œë²„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False
    
    # 4. ì‹¤ì œ LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸
    print("\n4. LLM í˜¸ì¶œ í…ŒìŠ¤íŠ¸...")
    # langchain-community ìš°ì„  ì‚¬ìš© (think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ë°©ì§€)
    try:
        from langchain_community.llms import Ollama
        llm = Ollama(
            base_url=settings.ollama_base_url,
            model=settings.ollama_model
        )
        print("   langchain-community.llms.Ollama ì‚¬ìš©")
    except ImportError:
        print("   âš ï¸ langchain-communityë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   langchain-ollamaë¡œ ì‹œë„...")
        try:
            from langchain_ollama import OllamaLLM
            llm = OllamaLLM(
                base_url=settings.ollama_base_url,
                model=settings.ollama_model
            )
            print("   langchain-ollama.OllamaLLM ì‚¬ìš©")
        except ImportError:
            print("   âŒ Ollama ì§€ì› íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        except Exception as e:
            if "think" in str(e).lower():
                print("   âš ï¸ langchain-ollamaì—ì„œ think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ë°œìƒ.")
                print("   langchain-communityë¡œ ì¬ì‹œë„...")
                try:
                    from langchain_community.llms import Ollama
                    llm = Ollama(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                    print("   langchain-community.llms.Ollama ì‚¬ìš© (fallback)")
                except Exception as e2:
                    print(f"   âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e2)}")
                    return False
            else:
                print(f"   âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                return False
    
    try:
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompt = "í•œ ì¤„ë¡œ ë‹µë³€: ì•ˆë…•í•˜ì„¸ìš”"
        print(f"   í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: '{test_prompt}'")
        print("   LLM ì‘ë‹µ ëŒ€ê¸° ì¤‘...")
        
        response = llm.invoke(test_prompt)
        if response and len(response) > 0:
            print(f"   âœ… LLM í˜¸ì¶œ ì„±ê³µ!")
            print(f"   ì‘ë‹µ: {response[:100]}...")
            return True
        else:
            print("   âŒ LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return False
    except Exception as e:
        print(f"   âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        print(f"   ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        return False
    
    return True


if __name__ == "__main__":
    print("\nğŸš€ Ollama ì„¤ì • í™•ì¸ ì‹œì‘\n")
    
    try:
        success = test_ollama_setup()
        
        print("\n" + "=" * 60)
        if success:
            print("âœ… ëª¨ë“  í™•ì¸ ì™„ë£Œ! Ollamaê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("\nì´ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            print("  python scripts/performance_test.py")
        else:
            print("âŒ ì¼ë¶€ í™•ì¸ ì‹¤íŒ¨. ìœ„ì˜ í•´ê²° ë°©ë²•ì„ ì°¸ê³ í•˜ì„¸ìš”.")
        print("=" * 60)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ í™•ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

