"""
backend/data/legal/ í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ëª¨ë“  legal ë°ì´í„°ëŠ” legal_chunks í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
- standard_contracts/ â†’ legal_chunks (source_type: "standard_contract")
- laws/ â†’ legal_chunks (source_type: "law")
- manuals/ â†’ legal_chunks (source_type: "manual")
- cases/ â†’ legal_chunks (source_type: "case")
"""

import os
import sys
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import uuid
import hashlib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from core.document_processor_v2 import DocumentProcessor
from core.generator_v2 import LLMGenerator
from core.supabase_vector_store import SupabaseVectorStore
from core.logging_config import get_logger
from supabase import create_client, Client
from config import settings

logger = get_logger(__name__)

# Supabase Storage ì„¤ì •
STORAGE_BUCKET = "legal-files"
_supabase_client: Optional[Client] = None


def get_supabase_client() -> Client:
    """Supabase í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤"""
    global _supabase_client
    if _supabase_client is None:
        supabase_url = os.getenv("SUPABASE_URL") or settings.supabase_url
        supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY") or settings.supabase_service_role_key
        
        if not supabase_url or not supabase_key:
            raise ValueError("SUPABASE_URLê³¼ SUPABASE_SERVICE_ROLE_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        _supabase_client = create_client(supabase_url, supabase_key)
    return _supabase_client


def make_external_id(file_path: Path) -> str:
    """íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ external_id ìƒì„± (í•´ì‹œ)"""
    relative_path = str(file_path.relative_to(backend_dir))
    return hashlib.md5(relative_path.encode("utf-8")).hexdigest()


def source_type_to_folder(source_type: str) -> str:
    """DB source_typeì„ Storage í´ë”ëª…ìœ¼ë¡œ ë³€í™˜"""
    mapping = {
        "law": "laws",
        "manual": "manuals",
        "case": "cases",
        "standard_contract": "standard_contracts",
    }
    return mapping.get(source_type, "other")


def upload_legal_file(file_path: Path, source_type: str, external_id: str) -> Tuple[str, str]:
    """
    Supabase Storageì— íŒŒì¼ ì—…ë¡œë“œí•˜ê³  (bucket, object_path) ë¦¬í„´
    
    Args:
        file_path: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        source_type: 'law' | 'manual' | 'case' | 'standard_contract'
        external_id: íŒŒì¼ ê³ ìœ  ID (hash)
    
    Returns:
        (bucket, object_path) - ì˜ˆ: ("legal-files", "laws/abcd1234.pdf")
    """
    supabase = get_supabase_client()
    
    # í™•ì¥ì ì¶”ì¶œ (ì—†ìœ¼ë©´ .pdfë¡œ ê°€ì •)
    ext = file_path.suffix.lower() or ".pdf"
    
    # Storage í´ë”ëª…ìœ¼ë¡œ ë³€í™˜
    folder_name = source_type_to_folder(source_type)
    
    # Storage object path ìƒì„± (í•œê¸€/ê³µë°± ì—†ì´ external_id + í™•ì¥ìë§Œ ì‚¬ìš©)
    object_path = f"{folder_name}/{external_id}{ext}"
    
    try:
        # íŒŒì¼ ì½ê¸°
        with open(file_path, "rb") as f:
            file_data = f.read()
        
        # Storageì— ì—…ë¡œë“œ (upsert=Trueë¡œ ë®ì–´ì“°ê¸° í—ˆìš©)
        supabase.storage.from_(STORAGE_BUCKET).upload(
            object_path,
            file_data,
            file_options={"upsert": True, "content-type": f"application/{ext[1:]}" if ext else "application/pdf"}
        )
        
        logger.info(f"  ğŸ“¤ Storage ì—…ë¡œë“œ ì™„ë£Œ: {object_path}")
        return STORAGE_BUCKET, object_path
        
    except Exception as e:
        logger.error(f"  âŒ Storage ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        # ì—…ë¡œë“œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (file_pathëŠ” Noneìœ¼ë¡œ ì„¤ì •)
        raise


def get_source_type_from_path(file_path: Path) -> str:
    """íŒŒì¼ ê²½ë¡œì—ì„œ source_type ì¶”ì¶œ"""
    path_str = str(file_path)
    if "standard_contracts" in path_str:
        return "standard_contract"
    elif "laws" in path_str:
        return "law"
    elif "manuals" in path_str:
        return "manual"
    elif "cases" in path_str:
        return "case"
    else:
        return "unknown"




async def process_legal_file(
    file_path: Path,
    processor: DocumentProcessor,
    generator: LLMGenerator,
    vector_store: SupabaseVectorStore,
    upload_to_storage: bool = False,
) -> Dict[str, Any]:
    """
    ëª¨ë“  legal íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ legal_chunksì— ì €ì¥
    
    Returns:
        {
            "file": str,
            "status": "success" | "failed",
            "external_id": str,
            "chunks_count": int,
            "error": str (optional)
        }
    """
    file_name = file_path.name
    source_type = get_source_type_from_path(file_path)
    
    # external_id ìƒì„± (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ í•´ì‹œ)
    external_id = make_external_id(file_path)
    
    try:
        # 0. ì¤‘ë³µ ì²´í¬: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì¸ì§€ í™•ì¸
        logger.info(f"  ğŸ“„ ì†ŒìŠ¤ íƒ€ì…: {source_type}")
        logger.info(f"  ğŸ” ì¤‘ë³µ ì²´í¬ ì¤‘... (external_id: {external_id[:8]}...)")
        
        if vector_store.check_legal_chunks_exist(external_id):
            logger.info(f"  â­ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            # ê¸°ì¡´ ì²­í¬ ê°œìˆ˜ í™•ì¸
            try:
                result = vector_store.sb.table("legal_chunks")\
                    .select("id", count="exact")\
                    .eq("external_id", external_id)\
                    .execute()
                existing_count = result.count if result.count is not None else len(result.data) if result.data else 0
                logger.info(f"  â„¹ï¸  ê¸°ì¡´ ì²­í¬ ê°œìˆ˜: {existing_count}ê°œ")
            except:
                existing_count = 0
            
            return {
                "file": file_name,
                "status": "skipped",
                "external_id": external_id,
                "chunks_count": existing_count,
                "error": None
            }
        
        logger.info(f"  âœ“ ì‹ ê·œ íŒŒì¼ì…ë‹ˆë‹¤. ì²˜ë¦¬ ì‹œì‘...")
        
        # 0-1. íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” Storage ê²½ë¡œ)
        relative_path = str(file_path.relative_to(backend_dir))
        storage_path = None
        storage_bucket = None
        
        if upload_to_storage:
            # Storageì— íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ ì‚¬í•­)
            logger.info(f"  ğŸ“¤ Storage ì—…ë¡œë“œ ì¤‘...")
            try:
                storage_bucket, storage_path = upload_legal_file(
                    file_path=file_path,
                    source_type=source_type,
                    external_id=external_id,
                )
                logger.info(f"  âœ“ Storage ì—…ë¡œë“œ ì™„ë£Œ: {storage_path}")
            except Exception as storage_err:
                logger.warning(f"  âš ï¸  Storage ì—…ë¡œë“œ ì‹¤íŒ¨ (ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©): {str(storage_err)}")
                # Storage ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©
                storage_path = relative_path
        else:
            # ê¸°ë³¸ê°’: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
            storage_path = relative_path
            logger.info(f"  ğŸ“ ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©: {relative_path}")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        logger.info(f"  ğŸ” í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        extracted_text, _ = processor.process_file(str(file_path), file_type=None)
        
        if not extracted_text or extracted_text.strip() == "":
            return {
                "file": file_name,
                "status": "failed",
                "external_id": external_id,
                "chunks_count": 0,
                "error": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë¹ˆ íŒŒì¼)"
            }
        
        logger.info(f"  âœ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text):,}ì")
        
        # 2. ì²­í‚¹ (í‘œì¤€ê³„ì•½ì„œëŠ” ì¡°í•­ ë‹¨ìœ„, ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ ì²­í‚¹)
        logger.info(f"  âœ‚ï¸  ì²­í‚¹ ì¤‘...")
        # file_pathëŠ” Storage ê²½ë¡œë¥¼ ì‚¬ìš© (ì—†ìœ¼ë©´ None)
        file_path_for_chunks = storage_path if storage_path else None
        
        if source_type == "standard_contract":
            # í‘œì¤€ê³„ì•½ì„œëŠ” ì¡°í•­ ë‹¨ìœ„ ì²­í‚¹ ì‹œë„
            try:
                chunks = processor.to_contract_chunks(
                    text=extracted_text,
                    base_meta={
                        "external_id": external_id,
                        "source_type": source_type,
                        "title": file_name,  # í•œê¸€ íŒŒì¼ëª… (UI í‘œì‹œìš©)
                        "filename": file_name,
                        "file_path": file_path_for_chunks,  # Storage ê²½ë¡œ
                    }
                )
                # ì¡°í•­ ë‹¨ìœ„ ì²­í‚¹ ì„±ê³µ ì‹œ ë©”íƒ€ë°ì´í„°ì— article_number ë“± í¬í•¨
            except:
                # ì¡°í•­ ë‹¨ìœ„ ì²­í‚¹ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì²­í‚¹ìœ¼ë¡œ í´ë°±
                chunks = processor.to_chunks(
                    text=extracted_text,
                    base_meta={
                        "external_id": external_id,
                        "source_type": source_type,
                        "title": file_name,
                        "filename": file_name,
                        "file_path": file_path_for_chunks,
                    }
                )
        else:
            # ë²•ë ¹/ë§¤ë‰´ì–¼/ì¼€ì´ìŠ¤ëŠ” ì¼ë°˜ ì²­í‚¹
            chunks = processor.to_chunks(
                text=extracted_text,
                base_meta={
                    "external_id": external_id,
                    "source_type": source_type,
                    "title": file_name,
                    "filename": file_name,
                    "file_path": file_path_for_chunks,
                }
            )
        
        if not chunks:
            return {
                "file": file_name,
                "status": "failed",
                "external_id": external_id,
                "chunks_count": 0,
                "error": "ì²­í¬ ìƒì„± ì‹¤íŒ¨"
            }
        
        logger.info(f"  âœ“ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        
        # 3. ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ )
        import time
        start_time = time.time()
        logger.info(f"  ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)}ê°œ ì²­í¬)")
        logger.info(f"     â±ï¸  ì˜ˆìƒ ì‹œê°„: ì•½ {len(chunks) * 0.3:.0f}~{len(chunks) * 1.0:.0f}ì´ˆ (CPU ëª¨ë“œ)")
        chunk_texts = [chunk.content for chunk in chunks]
        
        # ì„ë² ë”© ìƒì„± (ì§„í–‰ ìƒí™©ì€ sentence-transformersê°€ ìë™ìœ¼ë¡œ í‘œì‹œ)
        embeddings = generator.embed(chunk_texts)
        
        elapsed_time = time.time() - start_time
        logger.info(f"  âœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
        logger.info(f"     â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ (í‰ê· : {elapsed_time/len(chunks):.3f}ì´ˆ/ì²­í¬)")
        
        # 4. legal_chunks í…Œì´ë¸”ì— ì €ì¥
        logger.info(f"  ğŸ’¾ DB ì €ì¥ ì¤‘...")
        # bulk_upsert_legal_chunksëŠ” metadata ì•ˆì— ì •ë³´ë¥¼ ë„£ì–´ì•¼ í•¨
        chunk_payload = []
        for idx, chunk in enumerate(chunks):
            # metadataì— ëª¨ë“  ì •ë³´ í¬í•¨ (bulk_upsert_legal_chunksê°€ metadataì—ì„œ ì¶”ì¶œ)
            chunk_metadata = {
                **chunk.metadata,
                # ì´ 5ê°œëŠ” ì»¬ëŸ¼ìœ¼ë¡œ ë¹ ì ¸ê°
                "external_id": external_id,
                "source_type": source_type,
                "title": file_name,  # í•œê¸€ íŒŒì¼ëª… (UI í‘œì‹œìš©)
                "file_path": storage_path,  # Storage ê²½ë¡œ (ì˜ˆ: "laws/abcd1234.pdf")
                "chunk_index": chunk.index,
                # ì´ ì•„ë˜ëŠ” metadata JSONBì— ë“¤ì–´ê°
                "storage_bucket": storage_bucket,
                "original_file_name": file_name,  # ì›ë³¸ í•œê¸€ íŒŒì¼ëª…
                "filename": file_name,  # í•˜ìœ„ í˜¸í™˜ì„±
            }
            
            chunk_payload.append({
                "content": chunk.content,
                "embedding": embeddings[idx],
                "metadata": chunk_metadata,
            })
        
        vector_store.bulk_upsert_legal_chunks(chunk_payload)
        
        logger.info(f"  âœ“ ì €ì¥ ì™„ë£Œ: external_id={external_id[:8]}...")
        
        return {
            "file": file_name,
            "status": "success",
            "external_id": external_id,
            "chunks_count": len(chunk_payload),
            "error": None
        }
        
    except Exception as e:
        logger.error(f"[ì²˜ë¦¬ ì‹¤íŒ¨] {file_name}: {str(e)}", exc_info=True)
        return {
            "file": file_name,
            "status": "failed",
            "external_id": external_id if 'external_id' in locals() else None,
            "chunks_count": 0,
            "error": str(e)
        }


async def main():
    """ë©”ì¸ í•¨ìˆ˜: data/legal/ í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬"""
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    import argparse
    parser = argparse.ArgumentParser(description="ë²•ë ¹ íŒŒì¼ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--upload-to-storage",
        action="store_true",
        help="Supabase Storageì— íŒŒì¼ ì—…ë¡œë“œ (ê¸°ë³¸ê°’: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--files",
        nargs="+",
        help="íŠ¹ì • íŒŒì¼ë§Œ ì²˜ë¦¬ (íŒŒì¼ëª… ë˜ëŠ” ê²½ë¡œ, ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥). ì˜ˆ: --files 'íŒŒì¼ëª….pdf' 'ë‹¤ë¥¸íŒŒì¼.pdf'"
    )
    parser.add_argument(
        "--pattern",
        type=str,
        help="íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ í•„í„°ë§ (glob íŒ¨í„´). ì˜ˆ: --pattern '*í‘œì¤€ê³„ì•½ì„œ*.pdf'"
    )
    parser.add_argument(
        "--folder",
        type=str,
        choices=["standard_contracts", "laws", "manuals", "cases"],
        help="íŠ¹ì • í´ë”ë§Œ ì²˜ë¦¬"
    )
    args = parser.parse_args()
    upload_to_storage = args.upload_to_storage
    
    if upload_to_storage:
        logger.info("ğŸ“¤ Storage ì—…ë¡œë“œ ëª¨ë“œ: íŒŒì¼ì„ Supabase Storageì— ì—…ë¡œë“œí•©ë‹ˆë‹¤")
    else:
        logger.info("ğŸ“ ë¡œì»¬ íŒŒì¼ ëª¨ë“œ: ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ê¸°ë³¸ê°’)")
    
    # ë°ì´í„° í´ë” ê²½ë¡œ
    legal_dir = backend_dir / "data" / "legal"
    
    if not legal_dir.exists():
        logger.error(f"ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {legal_dir}")
        return
    
    # ì§€ì› íŒŒì¼ í˜•ì‹
    supported_extensions = ['.pdf', '.hwp', '.hwpx', '.txt', '.md']
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    all_files = []
    
    # íŠ¹ì • íŒŒì¼ ì§€ì •ëœ ê²½ìš°
    if args.files:
        logger.info(f"[INFO] íŠ¹ì • íŒŒì¼ë§Œ ì²˜ë¦¬: {len(args.files)}ê°œ")
        for file_spec in args.files:
            # ì ˆëŒ€ ê²½ë¡œì¸ì§€ í™•ì¸
            if Path(file_spec).is_absolute():
                file_path = Path(file_spec)
            else:
                # ìƒëŒ€ ê²½ë¡œë©´ legal_dir ê¸°ì¤€ìœ¼ë¡œ ì°¾ê¸°
                file_path = None
                # ëª¨ë“  í•˜ìœ„ í´ë”ì—ì„œ ì°¾ê¸°
                for subfolder in ["standard_contracts", "laws", "manuals", "cases"]:
                    subfolder_dir = legal_dir / subfolder
                    if subfolder_dir.exists():
                        candidate = subfolder_dir / file_spec
                        if candidate.is_file():
                            file_path = candidate
                            break
                
                if not file_path:
                    logger.warning(f"  [WARN] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_spec}")
                    continue
            
            if file_path.is_file():
                all_files.append(file_path)
                logger.info(f"  [OK] {file_path.relative_to(backend_dir)}")
            else:
                logger.warning(f"  [WARN] íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {file_path}")
    
    # íŒ¨í„´ ì§€ì •ëœ ê²½ìš°
    elif args.pattern:
        logger.info(f"[INFO] íŒ¨í„´ìœ¼ë¡œ í•„í„°ë§: {args.pattern}")
        folders_to_search = [args.folder] if args.folder else ["standard_contracts", "laws", "manuals", "cases"]
        
        for subfolder in folders_to_search:
            subfolder_dir = legal_dir / subfolder
            if subfolder_dir.exists():
                for ext in supported_extensions:
                    # íŒ¨í„´ì— í™•ì¥ìê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                    pattern = args.pattern if args.pattern.endswith(ext) else f"{args.pattern}{ext}"
                    all_files.extend(list(subfolder_dir.glob(pattern)))
    
    # í´ë”ë§Œ ì§€ì •ëœ ê²½ìš°
    elif args.folder:
        logger.info(f"[INFO] íŠ¹ì • í´ë”ë§Œ ì²˜ë¦¬: {args.folder}")
        subfolder_dir = legal_dir / args.folder
        if subfolder_dir.exists():
            for ext in supported_extensions:
                all_files.extend(list(subfolder_dir.glob(f"*{ext}")))
                all_files.extend(list(subfolder_dir.glob(f"**/*{ext}")))
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ (ê¸°ë³¸ê°’)
    else:
        # ëª¨ë“  í•˜ìœ„ í´ë”ì—ì„œ íŒŒì¼ ìˆ˜ì§‘
        for subfolder in ["standard_contracts", "laws", "manuals", "cases"]:
            subfolder_dir = legal_dir / subfolder
            if subfolder_dir.exists():
                for ext in supported_extensions:
                    all_files.extend(list(subfolder_dir.glob(f"*{ext}")))
                    all_files.extend(list(subfolder_dir.glob(f"**/*{ext}")))
    
    # ì¤‘ë³µ ì œê±°
    all_files = list(set(all_files))
    
    if not all_files:
        logger.warning(f"ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {legal_dir}")
        return
    
    logger.info("=" * 60)
    logger.info(f"[ì‹œì‘] data/legal/ í´ë” ì „ì²´ ì²˜ë¦¬")
    logger.info(f"  - ì´ íŒŒì¼: {len(all_files)}ê°œ (ëª¨ë‘ legal_chunksì— ì €ì¥)")
    logger.info("=" * 60)
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ì—¬ ì†ë„ ê°œì„ )
    logger.info("[ì´ˆê¸°í™” ì¤‘] DocumentProcessor, LLMGenerator, SupabaseVectorStore...")
    processor = DocumentProcessor()
    generator = LLMGenerator()  # ì„ë² ë”© ëª¨ë¸ ë¡œë”© (ì²˜ìŒì—ë§Œ ëŠë¦¼)
    vector_store = SupabaseVectorStore()
    logger.info("[ì´ˆê¸°í™” ì™„ë£Œ]")
    
    # ê²°ê³¼ ì €ì¥
    results = []
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    logger.info(f"\n[ì²˜ë¦¬ ì‹œì‘] ì´ {len(all_files)}ê°œ íŒŒì¼")
    logger.info("=" * 60)
    
    for idx, file_path in enumerate(all_files, 1):
        progress_percent = (idx / len(all_files)) * 100
        logger.info("")
        logger.info(f"[{idx}/{len(all_files)}] ({progress_percent:.1f}%) {file_path.name}")
        logger.info(f"  â””â”€ ê²½ë¡œ: {file_path.relative_to(backend_dir)}")
        
        result = await process_legal_file(
            file_path=file_path,
            processor=processor,
            generator=generator,
            vector_store=vector_store,
            upload_to_storage=upload_to_storage,
        )
        
        results.append({
            **result,
            "type": get_source_type_from_path(file_path),
            "target_table": "legal_chunks"
        })
        
        if result["status"] == "success":
            logger.info(f"  âœ… ì„±ê³µ: {result['chunks_count']}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
        elif result["status"] == "skipped":
            logger.info(f"  â­ï¸  ìŠ¤í‚µ: ì´ë¯¸ ì¡´ì¬í•¨ ({result['chunks_count']}ê°œ ì²­í¬)")
        else:
            logger.error(f"  âŒ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # ì§„í–‰ ìƒí™© ìš”ì•½ (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ íŒŒì¼)
        if idx % 10 == 0 or idx == len(all_files):
            success_so_far = sum(1 for r in results if r["status"] == "success")
            skipped_so_far = sum(1 for r in results if r["status"] == "skipped")
            failed_so_far = sum(1 for r in results if r["status"] == "failed")
            logger.info(f"  ğŸ“Š í˜„ì¬ê¹Œì§€: ì„±ê³µ {success_so_far}ê°œ, ìŠ¤í‚µ {skipped_so_far}ê°œ, ì‹¤íŒ¨ {failed_so_far}ê°œ")
    
    logger.info("")
    logger.info("=" * 60)
    
    # ê²°ê³¼ ìš”ì•½ (source_typeë³„)
    success_count = sum(1 for r in results if r["status"] == "success")
    skipped_count = sum(1 for r in results if r["status"] == "skipped")
    failed_count = sum(1 for r in results if r["status"] == "failed")
    total_chunks = sum(r["chunks_count"] for r in results if r["status"] == "success")
    
    # source_typeë³„ í†µê³„
    type_stats = {}
    for r in results:
        source_type = r.get("type", "unknown")
        if source_type not in type_stats:
            type_stats[source_type] = {"total": 0, "success": 0, "skipped": 0, "failed": 0, "chunks": 0}
        type_stats[source_type]["total"] += 1
        if r["status"] == "success":
            type_stats[source_type]["success"] += 1
            type_stats[source_type]["chunks"] += r["chunks_count"]
        elif r["status"] == "skipped":
            type_stats[source_type]["skipped"] += 1
        else:
            type_stats[source_type]["failed"] += 1
    
    logger.info("=" * 60)
    logger.info(f"[ì™„ë£Œ] ì²˜ë¦¬ ê²°ê³¼:")
    logger.info(f"  - ì´ íŒŒì¼: {len(results)}ê°œ")
    for source_type, stats in type_stats.items():
        logger.info(f"    * {source_type}: {stats['total']}ê°œ (ì„±ê³µ: {stats['success']}ê°œ, ìŠ¤í‚µ: {stats['skipped']}ê°œ, ì‹¤íŒ¨: {stats['failed']}ê°œ, ì²­í¬: {stats['chunks']}ê°œ)")
    logger.info(f"  - ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ (ì´ë¯¸ ì¡´ì¬)")
    logger.info(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"  - ì‹ ê·œ ì €ì¥ ì²­í¬: {total_chunks}ê°œ")
    logger.info("=" * 60)
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
    if failed_count > 0:
        logger.warning("ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡:")
        for r in results:
            if r["status"] == "failed":
                logger.warning(f"  - {r['file']} ({r.get('target_table', 'unknown')}): {r.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    import json
    from datetime import datetime
    
    report_dir = backend_dir / "data" / "indexed" / "reports"
    report_dir.mkdir(parents=True, exist_ok=True)
    
    report_file = report_dir / f"legal_data_indexing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    report = {
        "total": len(results),
        "by_source_type": type_stats,
        "summary": {
            "success": success_count,
            "skipped": skipped_count,
            "failed": failed_count,
            "total_chunks": total_chunks
        },
        "results": results,
        "processed_at": datetime.now().isoformat()
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"[ë¦¬í¬íŠ¸ ì €ì¥] {report_file}")


if __name__ == "__main__":
    asyncio.run(main())

