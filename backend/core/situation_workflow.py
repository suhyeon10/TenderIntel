"""
ìƒí™©ë¶„ì„ LangGraph ì›Œí¬í”Œë¡œìš°
ë‹¨ì¼ ìŠ¤í… â†’ ë©€í‹° ìŠ¤í… ëª¨ë“ˆí˜• ê·¸ë˜í”„ ê¸°ë°˜ ì‹¤í–‰
"""

from typing import TypedDict, List, Optional, Dict, Any
import asyncio
import logging
import json
import re
import warnings

# langchain-communityì˜ Ollama Deprecated ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning, module="langchain")

logger = logging.getLogger(__name__)

try:
    from langgraph.graph import StateGraph, END
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    logger.warning("LangGraphê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langgraphë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

from models.schemas import LegalGroundingChunk, LegalCasePreview
from core.supabase_vector_store import SupabaseVectorStore
from core.generator_v2 import LLMGenerator
from core.prompts import (
    build_situation_classify_prompt,
    build_situation_action_guide_prompt,
    build_situation_summary_prompt,
    build_situation_findings_prompt,
    build_situation_scripts_prompt,
    build_situation_organizations_prompt,
)

logger = logging.getLogger(__name__)


# ============================================================================
# State ëª¨ë¸ ì •ì˜
# ============================================================================

class SituationWorkflowState(TypedDict):
    """ìƒí™©ë¶„ì„ ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    # ì…ë ¥ ë°ì´í„°
    situation_text: str
    category_hint: Optional[str]
    summary: Optional[str]
    details: Optional[str]
    employment_type: Optional[str]
    work_period: Optional[str]
    weekly_hours: Optional[int]
    is_probation: Optional[bool]
    social_insurance: Optional[str]
    
    # ì¤‘ê°„ ê²°ê³¼
    query_text: Optional[str]  # summary + details ë˜ëŠ” situation_text
    query_embedding: Optional[List[float]]  # ì„ë² ë”© ë²¡í„°
    
    # ë¶„ë¥˜ ê²°ê³¼
    classification: Optional[Dict[str, Any]]  # {classified_type, risk_score, categories}
    
    # ê·œì • í•„í„°ë§ ê²°ê³¼
    filtered_categories: Optional[List[str]]  # ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡
    
    # RAG ê²€ìƒ‰ ê²°ê³¼
    grounding_chunks: Optional[List[LegalGroundingChunk]]  # ë²•ë ¹/ë§¤ë‰´ì–¼
    related_cases: Optional[List[LegalCasePreview]]  # ì¼€ì´ìŠ¤
    legal_basis: Optional[List[Dict[str, Any]]]  # ë²•ì  ê·¼ê±° êµ¬ì¡° (criteria ê°€ê³µìš©)
    
    # ì•¡ì…˜ ê°€ì´ë“œ ìƒì„± ê²°ê³¼ (ë³‘ë ¬ ìƒì„±)
    action_plan: Optional[Dict[str, Any]]  # {steps: [...]}
    scripts: Optional[Dict[str, str]]  # {to_company, to_advisor}
    criteria: Optional[List[Dict[str, Any]]]  # ë²•ì  íŒë‹¨ ê¸°ì¤€
    findings: Optional[List[Dict[str, Any]]]  # ë²•ì  ìŸì  ë°œê²¬ í•­ëª©
    organizations: Optional[List[Dict[str, Any]]]  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
    
    # ìµœì¢… ê²°ê³¼
    summary_report: Optional[str]  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸
    final_output: Optional[Dict[str, Any]]  # ìµœì¢… JSON ì¶œë ¥


# ============================================================================
# ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì •ì˜
# ============================================================================

class SituationWorkflow:
    """ìƒí™©ë¶„ì„ LangGraph ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraphê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install langgraphë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        self.vector_store = SupabaseVectorStore()
        self.generator = LLMGenerator()
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„± (ë³‘ë ¬ ì‹¤í–‰ êµ¬ì¡°)"""
        workflow = StateGraph(SituationWorkflowState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("prepare_query", self.prepare_query_node)
        workflow.add_node("classify_situation", self.classify_situation_node)
        workflow.add_node("filter_rules", self.filter_rules_node)
        workflow.add_node("retrieve_guides", self.retrieve_guides_node)
        
        # ë³‘ë ¬ ì‹¤í–‰ ë…¸ë“œ (retrieve_guides ì´í›„ ëª¨ë“  í•„ë“œë¥¼ ë³‘ë ¬ë¡œ ìƒì„±)
        workflow.add_node("generate_all_fields", self.generate_all_fields_node)
        
        workflow.add_node("merge_output", self.merge_output_node)
        
        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("prepare_query")
        workflow.add_edge("prepare_query", "classify_situation")
        workflow.add_edge("classify_situation", "filter_rules")
        workflow.add_edge("filter_rules", "retrieve_guides")
        workflow.add_edge("retrieve_guides", "generate_all_fields")
        workflow.add_edge("generate_all_fields", "merge_output")
        workflow.add_edge("merge_output", END)
        
        return workflow.compile()
    
    # ==================== ë…¸ë“œ í•¨ìˆ˜ë“¤ ====================
    
    async def prepare_query_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° ì„ë² ë”© ìƒì„±"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] prepare_query_node ì‹œì‘")
        
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„±
        query_text = state.get("situation_text", "")
        if state.get("summary"):
            query_text = state["summary"]
            if state.get("details"):
                query_text = f"{state['summary']}\n\n{state['details']}"
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = await self._get_embedding(query_text)
        
        return {
            **state,
            "query_text": query_text,
            "query_embedding": query_embedding,
        }
    
    async def classify_situation_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """2. ìƒí™© ë¶„ë¥˜ (ì¹´í…Œê³ ë¦¬ + ìœ„í—˜ë„)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] classify_situation_node ì‹œì‘")
        
        query_text = state.get("query_text", "")
        category_hint = state.get("category_hint")
        
        # LLMìœ¼ë¡œ ë¶„ë¥˜ ìˆ˜í–‰
        classification = await self._llm_classify(
            situation_text=query_text,
            category_hint=category_hint,
            employment_type=state.get("employment_type"),
            work_period=state.get("work_period"),
            weekly_hours=state.get("weekly_hours"),
            is_probation=state.get("is_probation"),
            social_insurance=state.get("social_insurance"),
        )
        
        return {
            **state,
            "classification": classification,
        }
    
    async def filter_rules_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """3. ë¶„ë¥˜ ê²°ê³¼ ê¸°ë°˜ ê·œì • í•„í„°ë§"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] filter_rules_node ì‹œì‘")
        
        classification = state.get("classification", {})
        classified_type = classification.get("classified_type", "unknown")
        
        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í•„í„°ë§ ê·œì¹™ ìƒì„±
        filtered_categories = await self._filter_rules_by_classification(
            classified_type=classified_type,
            classification=classification,
        )
        
        return {
            **state,
            "filtered_categories": filtered_categories,
        }
    
    async def retrieve_guides_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """4. RAG ê²€ìƒ‰ (í•„í„°ë§ëœ ì¹´í…Œê³ ë¦¬ë§Œ) + legalBasis ì¶”ì¶œ"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] retrieve_guides_node ì‹œì‘")
        
        query_embedding = state.get("query_embedding")
        filtered_categories = state.get("filtered_categories", [])
        
        if not query_embedding:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] query_embeddingì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {
                **state,
                "grounding_chunks": [],
                "related_cases": [],
                "legal_basis": [],
            }
        
        # ë³‘ë ¬ ê²€ìƒ‰
        grounding_chunks, related_cases = await asyncio.gather(
            self._search_legal_with_filter(
                query_embedding=query_embedding,
                categories=filtered_categories,
                top_k=8,
            ),
            self._search_cases_with_embedding(
                query_embedding=query_embedding,
                top_k=3,
            ),
            return_exceptions=False
        )
        
        # RAG ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] RAG ê²€ìƒ‰ ì™„ë£Œ: ë²•ë ¹/ê°€ì´ë“œ {len(grounding_chunks)}ê°œ, ì¼€ì´ìŠ¤ {len(related_cases)}ê°œ")
        if grounding_chunks:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] ê²€ìƒ‰ëœ ë²•ë ¹/ê°€ì´ë“œ ëª©ë¡:")
            for idx, chunk in enumerate(grounding_chunks[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ë¡œê¹…
                logger.info(f"  {idx}. [{chunk.source_type}] {chunk.title} (score: {chunk.score:.3f})")
                logger.info(f"     ë‚´ìš©: {chunk.snippet[:100]}...")
        
        # legalBasis êµ¬ì¡° ì¶”ì¶œ (criteria ê°€ê³µìš©)
        legal_basis = self._extract_legal_basis(grounding_chunks)
        
        return {
            **state,
            "grounding_chunks": grounding_chunks,
            "related_cases": related_cases[:3],  # ìµœëŒ€ 3ê°œë§Œ
            "legal_basis": legal_basis,
        }
    
    async def generate_all_fields_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """5. ëª¨ë“  í•„ë“œ ë³‘ë ¬ ìƒì„± (summary, findings, scripts, organizations)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] generate_all_fields_node ì‹œì‘ - ë³‘ë ¬ ì‹¤í–‰")
        
        classification = state.get("classification", {})
        grounding_chunks = state.get("grounding_chunks", [])
        legal_basis = state.get("legal_basis", [])
        query_text = state.get("query_text", "")
        
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] ì…ë ¥ ë°ì´í„° í™•ì¸ - classification: {bool(classification)}, grounding_chunks: {len(grounding_chunks)}ê°œ, legal_basis: {len(legal_basis)}ê°œ, query_text ê¸¸ì´: {len(query_text)}ì")
        
        # legal_basisê°€ ë¹ˆ ë°°ì—´ì¼ ë•Œ fallback
        if not legal_basis:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] legal_basisê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ criteria ìƒì„±")
            legal_basis = [{
                "title": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                "snippet": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                "source_type": "unknown",
            }]
        
        # ë³‘ë ¬ë¡œ ëª¨ë“  í•„ë“œ ìƒì„±
        logger.info("[ì›Œí¬í”Œë¡œìš°] ë³‘ë ¬ LLM í˜¸ì¶œ ì‹œì‘ (summary, findings, scripts, organizations)...")
        start_time = asyncio.get_event_loop().time()
        
        summary_result, findings_result, scripts_result, organizations_result = await asyncio.gather(
            self._llm_generate_summary(
                situation_text=query_text,
                classification=classification,
                grounding_chunks=grounding_chunks,
                legal_basis=legal_basis,
                employment_type=state.get("employment_type"),
                work_period=state.get("work_period"),
                weekly_hours=state.get("weekly_hours"),
                is_probation=state.get("is_probation"),
                social_insurance=state.get("social_insurance"),
            ),
            self._llm_generate_findings(
                situation_text=query_text,
                classification=classification,
                grounding_chunks=grounding_chunks,
                legal_basis=legal_basis,
                employment_type=state.get("employment_type"),
                work_period=state.get("work_period"),
                weekly_hours=state.get("weekly_hours"),
                is_probation=state.get("is_probation"),
                social_insurance=state.get("social_insurance"),
            ),
            self._llm_generate_scripts(
                situation_text=query_text,
                classification=classification,
                grounding_chunks=grounding_chunks,
                legal_basis=legal_basis,
                employment_type=state.get("employment_type"),
                work_period=state.get("work_period"),
                weekly_hours=state.get("weekly_hours"),
                is_probation=state.get("is_probation"),
                social_insurance=state.get("social_insurance"),
            ),
            self._llm_generate_organizations(
                situation_text=query_text,
                classification=classification,
            ),
            return_exceptions=True
        )
        
        elapsed_time = asyncio.get_event_loop().time() - start_time
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] ë³‘ë ¬ LLM í˜¸ì¶œ ì™„ë£Œ - ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        if isinstance(summary_result, Exception):
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] summary ìƒì„± ì‹¤íŒ¨: {summary_result}", exc_info=summary_result)
            # ê¸°ë³¸ summary ë°˜í™˜ (4ê°œ ì„¹ì…˜ êµ¬ì¡° ìœ ì§€)
            summary_result = "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\nìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²•ì  ê´€ì ê³¼ í–‰ë™ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ ìƒí™©\n\nê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        elif not summary_result or (isinstance(summary_result, str) and len(summary_result.strip()) == 0):
            logger.warning("[ì›Œí¬í”Œë¡œìš°] summaryê°€ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
            summary_result = "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\nìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²•ì  ê´€ì ê³¼ í–‰ë™ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ ìƒí™©\n\nê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        
        if isinstance(findings_result, Exception):
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] findings ìƒì„± ì‹¤íŒ¨: {findings_result}", exc_info=findings_result)
            findings_result = []
        if isinstance(scripts_result, Exception):
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] scripts ìƒì„± ì‹¤íŒ¨: {scripts_result}", exc_info=scripts_result)
            scripts_result = {}
        if isinstance(organizations_result, Exception):
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] organizations ìƒì„± ì‹¤íŒ¨: {organizations_result}", exc_info=organizations_result)
            organizations_result = []
        
        # findings ìƒì„± í›„ chunk ê¸°ë°˜ìœ¼ë¡œ source ì •ë³´ ë§¤í•‘
        if isinstance(findings_result, list) and findings_result and grounding_chunks:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] findingsì— chunk ê¸°ë°˜ source ì •ë³´ ë§¤í•‘ ì‹œì‘: {len(findings_result)}ê°œ")
            findings_result = self._map_findings_to_chunks(findings_result, grounding_chunks)
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] findings source ì •ë³´ ë§¤í•‘ ì™„ë£Œ")
        
        return {
            **state,
            "summary_report": summary_result if isinstance(summary_result, str) else "",
            "scripts": scripts_result if isinstance(scripts_result, dict) else {},
            "findings": findings_result if isinstance(findings_result, list) else [],
            "organizations": organizations_result if isinstance(organizations_result, list) else [],
        }
    
    # ê¸°ì¡´ í•¨ìˆ˜ëŠ” ì£¼ì„ ì²˜ë¦¬ (ë ˆê±°ì‹œ í˜¸í™˜ì„± ìœ ì§€)
    async def generate_action_guide_node_OLD(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """5. í–‰ë™ ê°€ì´ë“œ ìƒì„± (summary, criteria, actionPlan, scripts ëª¨ë‘ ìƒì„±)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] generate_action_guide_node ì‹œì‘")
        
        classification = state.get("classification", {})
        grounding_chunks = state.get("grounding_chunks", [])
        legal_basis = state.get("legal_basis", [])
        query_text = state.get("query_text", "")
        
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] ì…ë ¥ ë°ì´í„° í™•ì¸ - classification: {bool(classification)}, grounding_chunks: {len(grounding_chunks)}ê°œ, legal_basis: {len(legal_basis)}ê°œ, query_text ê¸¸ì´: {len(query_text)}ì")
        
        # legal_basisê°€ ë¹ˆ ë°°ì—´ì¼ ë•Œ fallback
        if not legal_basis:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] legal_basisê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ criteria ìƒì„±")
            legal_basis = [{
                "title": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                "snippet": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                "source_type": "unknown",
            }]
        
        # ì•¡ì…˜ ê°€ì´ë“œ ìƒì„± (summary í¬í•¨)
        logger.info("[ì›Œí¬í”Œë¡œìš°] _llm_generate_action_guide í˜¸ì¶œ ì‹œì‘...")
        action_result = await self._llm_generate_action_guide(
            situation_text=query_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=state.get("employment_type"),
            work_period=state.get("work_period"),
            weekly_hours=state.get("weekly_hours"),
            is_probation=state.get("is_probation"),
            social_insurance=state.get("social_insurance"),
        )
        
        # ê²°ê³¼ ê²€ì¦ ë° ì •ê·œí™”
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] _reformat_action_result í˜¸ì¶œ ì „ action_result: summary ê¸¸ì´={len(action_result.get('summary', ''))}, criteria ê°œìˆ˜={len(action_result.get('criteria', []))}")
        normalized_result = self._reformat_action_result(action_result, legal_basis)
        
        # action_plan ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        action_plan_safe = normalized_result.get('action_plan', {})
        if isinstance(action_plan_safe, dict):
            action_plan_steps_count = len(action_plan_safe.get('steps', []))
        else:
            action_plan_steps_count = 0
        
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] _reformat_action_result í˜¸ì¶œ í›„ normalized_result: summary ê¸¸ì´={len(normalized_result.get('summary', ''))}, criteria ê°œìˆ˜={len(normalized_result.get('criteria', []))}, action_plan steps={action_plan_steps_count}")
        
        return {
            **state,
            "summary_report": normalized_result.get("summary", ""),  # 4ê°œ ì„¹ì…˜ ë§ˆí¬ë‹¤ìš´
            "action_plan": normalized_result.get("action_plan", {"steps": []}),  # steps êµ¬ì¡°
            "scripts": normalized_result.get("scripts", {}),  # toCompany, toAdvisor
            "criteria": normalized_result.get("criteria", []),  # name, status, reason
            "findings": normalized_result.get("findings", []),  # ë²•ì  ìŸì  ë°œê²¬ í•­ëª©
            "organizations": normalized_result.get("organizations", []),  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
        }
    
    
    async def merge_output_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """7. ìµœì¢… ì¶œë ¥ ë³‘í•©"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] merge_output_node ì‹œì‘")
        
        classification = state.get("classification", {})
        related_cases = state.get("related_cases", [])
        action_plan = state.get("action_plan", {})
        scripts = state.get("scripts", {})
        # criteriaê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì²´í¬
        criteria_raw = state.get("criteria")
        criteria = criteria_raw if criteria_raw is not None and isinstance(criteria_raw, list) else []
        findings = state.get("findings", [])  # ë²•ì  ìŸì  ë°œê²¬ í•­ëª©
        organizations = state.get("organizations", [])  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
        summary_report = state.get("summary_report", "")  # generate_action_guideì—ì„œ ìƒì„±ë¨
        legal_basis = state.get("legal_basis", [])  # legal_basis ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        
        # grounding_chunks ê°€ì ¸ì˜¤ê¸°
        grounding_chunks = state.get("grounding_chunks", [])
        
        # ìµœì¢… JSON ì¶œë ¥ êµ¬ì„±
        # related_casesëŠ” ì´ë¯¸ retrieve_guidesì—ì„œ ìµœëŒ€ 3ê°œë¡œ ì œí•œë¨
        # related_casesëŠ” dict í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ dict ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©
        formatted_related_cases = []
        for case in related_cases[:3]:  # ìµœëŒ€ 3ê°œë§Œ (ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
            if isinstance(case, dict):
                case_id = case.get("id", "")
                case_title = case.get("title", "")
                case_situation = case.get("situation", "")
                case_source_type = case.get("source_type")
            else:
                # ê°ì²´ì¸ ê²½ìš° (Legacy ì§€ì›)
                case_id = getattr(case, "id", "")
                case_title = getattr(case, "title", "")
                case_situation = getattr(case, "situation", "")
                case_source_type = getattr(case, "source_type", None)
            
            formatted_related_cases.append({
                "id": case_id,  # external_id
                "title": case_title,
                "summary": case_situation[:200] if len(case_situation) > 200 else case_situation,
                "source_type": case_source_type,  # source_type ì •ë³´ ì¶”ê°€
            })
        
        # grounding_chunksë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_sources = [
            {
                "source_id": chunk.source_id,
                "source_type": chunk.source_type,
                "title": chunk.title,
                "snippet": chunk.snippet,
                "score": chunk.score,
                "external_id": getattr(chunk, 'external_id', None),
                "file_url": getattr(chunk, 'file_url', None),
            }
            for chunk in grounding_chunks[:8]  # ìµœëŒ€ 8ê°œ
        ]
        
        # criteriaë¥¼ grounding_chunksì—ì„œ ì§ì ‘ ìƒì„± (ìƒˆë¡œìš´ RAG ê¸°ë°˜ êµ¬ì¡°)
        # grounding_chunksë¥¼ criteria í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        from core.file_utils import get_document_file_url
        
        criteria_items = []
        for chunk in grounding_chunks[:8]:  # ìµœëŒ€ 8ê°œ
            external_id = getattr(chunk, 'external_id', None)
            source_type = chunk.source_type
            file_url = getattr(chunk, 'file_url', None)
            
            # file_urlì´ ì—†ìœ¼ë©´ ìƒì„± (external_idê°€ ìˆëŠ” ê²½ìš°)
            if not file_url and external_id:
                try:
                    file_url = get_document_file_url(
                        external_id=external_id,
                        source_type=source_type,
                        expires_in=3600
                    )
                except Exception as e:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] fileUrl ìƒì„± ì‹¤íŒ¨ (external_id={external_id}, source_type={source_type}): {str(e)}")
                    file_url = None
            
            # usageReason ìƒì„± (ìš°ì„ ìˆœìœ„: LLM criteria reason > snippet ê¸°ë°˜ êµ¬ì²´ì  ìƒì„± > ê¸°ë³¸ ë©”ì‹œì§€)
            usage_reason = ""
            chunk_snippet_prefix = chunk.snippet[:50].strip() if chunk.snippet else ""
            chunk_snippet = chunk.snippet[:200].strip() if chunk.snippet else ""
            
            # 1. LLMì´ ìƒì„±í•œ criteriaì—ì„œ í•´ë‹¹ ë¬¸ì„œì™€ ê´€ë ¨ëœ reason ì°¾ê¸°
            for criterion in criteria:
                if isinstance(criterion, dict):
                    criterion_name = criterion.get("name", "")
                    criterion_reason = criterion.get("reason", "")
                    criterion_legal_basis = criterion.get("legalBasis", [])
                    
                    # ë¬¸ì„œ ì œëª© ë§¤ì¹­
                    if chunk.title in criterion_name or criterion_name in chunk.title:
                        # reasonì´ ë„ˆë¬´ ê¸¸ë©´ (snippet ì „ì²´ê°€ ë“¤ì–´ê°„ ê²½ìš°) ë‹¤ìŒ ë‹¨ê³„ë¡œ
                        if len(criterion_reason) > 200:
                            break
                        # ì¼ë°˜ì ì¸ í…œí”Œë¦¿ ë¬¸ì¥ì¸ì§€ í™•ì¸ ("í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬", "ë²•ì  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤" ë“±)
                        elif "í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬" in criterion_reason and "ë²•ì  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤" in criterion_reason:
                            # ì¼ë°˜ì ì¸ ë¬¸ì¥ì´ë©´ snippet ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì  ìƒì„± ì‹œë„
                            break
                        else:
                            usage_reason = criterion_reason
                            break
                    
                    # legalBasisì—ì„œ snippet ë§¤ì¹­ ì‹œë„
                    if criterion_legal_basis and isinstance(criterion_legal_basis, list):
                        for basis in criterion_legal_basis:
                            if isinstance(basis, dict):
                                basis_snippet = basis.get("snippet", "")
                                if chunk_snippet_prefix and basis_snippet:
                                    if chunk_snippet_prefix[:30] in basis_snippet[:100] or basis_snippet[:30] in chunk_snippet_prefix[:100]:
                                        # ì¼ë°˜ì ì¸ í…œí”Œë¦¿ ë¬¸ì¥ì¸ì§€ í™•ì¸
                                        if "í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬" in criterion_reason and "ë²•ì  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤" in criterion_reason:
                                            break
                                        else:
                                            usage_reason = criterion_reason if len(criterion_reason) <= 200 else ""
                                            break
                        if usage_reason:
                            break
                else:
                    criterion_name = getattr(criterion, "name", "")
                    criterion_reason = getattr(criterion, "reason", "")
                    if chunk.title in criterion_name or criterion_name in chunk.title:
                        # ì¼ë°˜ì ì¸ í…œí”Œë¦¿ ë¬¸ì¥ì¸ì§€ í™•ì¸
                        if "í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬" in criterion_reason and "ë²•ì  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤" in criterion_reason:
                            break
                        usage_reason = criterion_reason if len(criterion_reason) <= 200 else ""
                        if usage_reason:
                            break
            
            # 2. snippet ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ usageReason ìƒì„± (LLM reasonì´ ì—†ê±°ë‚˜ ì¼ë°˜ì ì¸ ê²½ìš°)
            if not usage_reason and chunk_snippet:
                # snippetì—ì„œ í•µì‹¬ ìŸì  í‚¤ì›Œë“œ ì¶”ì¶œ
                issue_keywords = []
                if any(kw in chunk_snippet for kw in ["í–‰ì‚¬ê¸°ê°„", "í–‰ì‚¬ ê¸°ê°„", "í–‰ì‚¬ê¸°í•œ"]):
                    issue_keywords.append("í–‰ì‚¬ê¸°ê°„")
                if any(kw in chunk_snippet for kw in ["ì¬ì§", "ì¬ì„", "ê·¼ë¬´ê¸°ê°„"]):
                    issue_keywords.append("ì¬ì§ìš”ê±´")
                if any(kw in chunk_snippet for kw in ["í•´ê³ ", "ê³„ì•½í•´ì§€", "í•´ì§€"]):
                    issue_keywords.append("í•´ê³  ì˜ˆê³ ")
                if any(kw in chunk_snippet for kw in ["ì„ ê¸‰ê¸ˆ", "ì„ ê¸ˆ", "ê³„ì•½ê¸ˆ"]):
                    issue_keywords.append("ì„ ê¸‰ê¸ˆ")
                if any(kw in chunk_snippet for kw in ["ì§€ì—°", "ë°°ìƒ", "ì´ì"]):
                    issue_keywords.append("ì§€ì—°ë°°ìƒ")
                if any(kw in chunk_snippet for kw in ["ì„ê¸ˆ", "ê¸‰ì—¬", "ì§€ê¸‰ì¼"]):
                    issue_keywords.append("ì„ê¸ˆì§€ê¸‰ì¼")
                if any(kw in chunk_snippet for kw in ["ìˆ˜ìŠµ", "ìˆ˜ìŠµê¸°ê°„"]):
                    issue_keywords.append("ìˆ˜ìŠµê¸°ê°„")
                if any(kw in chunk_snippet for kw in ["ì—°ì¥ê·¼ë¡œ", "ì•¼ê°„ê·¼ë¡œ", "íœ´ì¼ê·¼ë¡œ"]):
                    issue_keywords.append("ì—°ì¥ê·¼ë¡œìˆ˜ë‹¹")
                
                # snippet í•µì‹¬ ë‚´ìš© ìš”ì•½ (ì²« 100ì)
                snippet_summary = chunk_snippet[:100].replace("\n", " ").strip()
                
                # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ íŒë‹¨ í¬ì¸íŠ¸
                if issue_keywords:
                    issue_text = ", ".join(issue_keywords[:2])  # ìµœëŒ€ 2ê°œë§Œ
                    if "í‘œì¤€" in chunk.title and "ê³„ì•½" in chunk.title:
                        usage_reason = f"ì´ ì¡°í•­ì€ {issue_text}ì— ëŒ€í•œ ê·œì •ì„ í¬í•¨í•˜ê³  ìˆì–´, í˜„ì¬ ì‚¬ìš©ì ê³„ì•½ì„œì˜ í•´ë‹¹ ì¡°í•­ì´ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ê³¼ë„í•˜ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ ë¹„êµÂ·íŒë‹¨í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
                    elif "ë²•" in chunk.title or "ê·œì¹™" in chunk.title:
                        usage_reason = f"ì´ ì¡°í•­ì€ {issue_text}ì— ëŒ€í•œ ë²•ì  ìš”ê±´ì„ ê·œì •í•˜ê³  ìˆì–´, í˜„ì¬ ìƒí™©ì—ì„œ í•´ë‹¹ ìš”ê±´ì´ ì¶©ì¡±ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê·¼ê±°ë¡œ í™œìš©í–ˆìŠµë‹ˆë‹¤."
                    else:
                        usage_reason = f"ì´ ì¡°í•­ì€ {issue_text}ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆì–´, í˜„ì¬ ì‚¬ìš©ì ìƒí™©/ê³„ì•½ì„œì—ì„œ í•´ë‹¹ ë¶€ë¶„ì„ í‰ê°€í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
                else:
                    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ snippet ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                    if "í‘œì¤€" in chunk.title and "ê³„ì•½" in chunk.title:
                        usage_reason = f"ì´ ì¡°í•­ì€ '{snippet_summary}...'ì˜ ë‚´ìš©ì„ ê·œì •í•˜ê³  ìˆì–´, í˜„ì¬ ê³„ì•½ì„œì˜ ê´€ë ¨ ì¡°í•­ê³¼ ë¹„êµí•˜ì—¬ ì ì ˆì„±ì„ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
                    elif "ë²•" in chunk.title or "ê·œì¹™" in chunk.title:
                        usage_reason = f"ì´ ì¡°í•­ì€ '{snippet_summary}...'ì˜ ë²•ì  ìš”ê±´ì„ ëª…ì‹œí•˜ê³  ìˆì–´, í˜„ì¬ ìƒí™©ì—ì„œ í•´ë‹¹ ìš”ê±´ ì¶©ì¡± ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ê·¼ê±°ë¡œ í™œìš©í–ˆìŠµë‹ˆë‹¤."
                    else:
                        usage_reason = f"ì´ ì¡°í•­ì€ '{snippet_summary}...'ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆì–´, í˜„ì¬ ì‚¬ìš©ì ìƒí™©ê³¼ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
            
            # 3. usageReasonì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ìƒì„± (ìµœí›„ì˜ ìˆ˜ë‹¨)
            if not usage_reason:
                if "í‘œì¤€" in chunk.title and "ê³„ì•½" in chunk.title:
                    usage_reason = f"í˜„ì¬ ê³„ì•½ì„œì˜ ê´€ë ¨ ì¡°í•­ì´ ë¶ˆëª…í™•í•œ ë¶€ë¶„ì´ ìˆì–´, {chunk.title}ì˜ ê·œì •ì„ ë¹„êµ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
                elif "ë²•" in chunk.title or "ê·œì¹™" in chunk.title:
                    usage_reason = f"í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬ {chunk.title}ì˜ ë²•ë ¹ ì¡°í•­ì„ íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
                else:
                    usage_reason = f"í˜„ì¬ ìƒí™©ê³¼ ê´€ë ¨í•˜ì—¬ {chunk.title}ì˜ ë‚´ìš©ì„ ë²•ì  íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤."
            
            criteria_item = {
                "documentTitle": chunk.title,
                "fileUrl": file_url,
                "sourceType": source_type,
                "similarityScore": float(chunk.score),
                "snippet": chunk.snippet,
                "usageReason": usage_reason,
            }
            criteria_items.append(criteria_item)
        
        # findingsëŠ” ì´ë¯¸ generate_all_fields_nodeì—ì„œ chunk ê¸°ë°˜ìœ¼ë¡œ source ì •ë³´ê°€ ë§¤í•‘ë˜ì–´ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”)
        findings_processed = findings if findings else []
        
        if findings_processed:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] findings ì‚¬ìš© (ì´ë¯¸ source ì •ë³´ ë§¤í•‘ë¨): {len(findings_processed)}ê°œ")
        else:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] findingsê°€ ë¹„ì–´ìˆê±°ë‚˜ Noneì…ë‹ˆë‹¤.")
        
        final_output = {
            "classified_type": classification.get("classified_type", "unknown"),
            "risk_score": classification.get("risk_score", 50),
            "summary": summary_report,  # generate_action_guideì—ì„œ ìƒì„±ëœ 4ê°œ ì„¹ì…˜ ë§ˆí¬ë‹¤ìš´
            "criteria": criteria_items,  # RAG ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ (ìƒˆë¡œìš´ êµ¬ì¡°)
            "findings": findings_processed,  # ë²•ì  ìŸì  ë°œê²¬ í•­ëª©
            "action_plan": action_plan,  # steps êµ¬ì¡°
            "scripts": scripts,  # toCompany, toAdvisor
            "related_cases": formatted_related_cases,
            "grounding_chunks": formatted_sources,  # sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            "organizations": organizations,  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
        }
        
        return {
            **state,
            "final_output": final_output,
        }
    
    # ==================== ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ ====================
    
    def _map_findings_to_chunks(
        self,
        findings: List[Dict[str, Any]],
        grounding_chunks: List[Any]
    ) -> List[Dict[str, Any]]:
        """findingsë¥¼ grounding_chunksì™€ ë§¤í•‘í•˜ì—¬ source ì •ë³´ ì¶”ê°€"""
        from core.file_utils import get_document_file_url
        
        findings_mapped = []
        for finding in findings:
            if not isinstance(finding, dict):
                findings_mapped.append(finding)
                continue
            
            document_title = finding.get("documentTitle", "").strip()
            refined_snippet = finding.get("refinedSnippet", "").strip()  # LLMì´ ìƒì„±í•œ refinedSnippet
            
            if not document_title:
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] findingì— documentTitleì´ ì—†ìŒ: {finding.get('title', 'unknown')}")
                findings_mapped.append(finding)
                continue
            
            # grounding_chunksì—ì„œ ë§¤ì¹­ë˜ëŠ” chunk ì°¾ê¸°
            matched_chunk = None
            best_match_score = 0.0
            
            for chunk in grounding_chunks:
                chunk_title = getattr(chunk, 'title', '').strip() if hasattr(chunk, 'title') else ''
                
                # ì •í™•í•œ ì œëª© ë§¤ì¹­
                if chunk_title == document_title:
                    matched_chunk = chunk
                    break
                
                # ë¶€ë¶„ ì œëª© ë§¤ì¹­ (ì–‘ë°©í–¥) - ë” ìœ ì—°í•œ ë§¤ì¹­
                if document_title in chunk_title or chunk_title in document_title:
                    match_score = min(len(document_title), len(chunk_title)) / max(len(document_title), len(chunk_title))
                    if match_score > best_match_score:
                        best_match_score = match_score
                        matched_chunk = chunk
                
                # í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ (ì œëª©ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ)
                # ì˜ˆ: "ì§ì¥ ë‚´ ê´´ë¡­í˜ íŒë‹¨ ë° ì˜ˆë°© ëŒ€ì‘ ë§¤ë‰´ì–¼.pdf" -> "ì§ì¥ ë‚´ ê´´ë¡­í˜", "ë§¤ë‰´ì–¼"
                doc_keywords = self._extract_title_keywords(document_title)
                chunk_keywords = self._extract_title_keywords(chunk_title)
                
                if doc_keywords and chunk_keywords:
                    # ê³µí†µ í‚¤ì›Œë“œê°€ 2ê°œ ì´ìƒì´ë©´ ë§¤ì¹­
                    common_keywords = set(doc_keywords) & set(chunk_keywords)
                    if len(common_keywords) >= 2:
                        keyword_match_score = len(common_keywords) / max(len(doc_keywords), len(chunk_keywords))
                        if keyword_match_score > best_match_score:
                            best_match_score = keyword_match_score
                            matched_chunk = chunk
            
            # chunkì—ì„œ source ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            external_id = None
            source_type = 'law'
            chunk_score = 0.0
            chunk_snippet = ''
            
            if matched_chunk:
                external_id = getattr(matched_chunk, 'external_id', None)
                source_type = getattr(matched_chunk, 'source_type', 'law')
                chunk_score = float(getattr(matched_chunk, 'score', 0.0))
                chunk_snippet = getattr(matched_chunk, 'snippet', '')
                chunk_title_attr = getattr(matched_chunk, 'title', '')
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] finding '{finding.get('title', 'unknown')}' chunk ë§¤ì¹­ ì„±ê³µ: documentTitle='{document_title}', chunkTitle='{chunk_title_attr}'")
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ DBì—ì„œ titleë¡œ ê²€ìƒ‰
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] finding '{finding.get('title', 'unknown')}' chunk ë§¤ì¹­ ì‹¤íŒ¨, DBì—ì„œ ê²€ìƒ‰ ì‹œë„: documentTitle='{document_title}'")
                try:
                    db_chunk = self.vector_store.get_legal_chunk_by_title(document_title)
                    if db_chunk:
                        external_id = db_chunk.get('external_id')
                        source_type = db_chunk.get('source_type', 'law')
                        logger.info(f"[ì›Œí¬í”Œë¡œìš°] DBì—ì„œ ë¬¸ì„œ ì°¾ìŒ: external_id={external_id}, source_type={source_type}, title='{db_chunk.get('title', '')}'")
                    else:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] DBì—ì„œë„ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨: documentTitle='{document_title}'")
                except Exception as e:
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] DB ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            
            # fileUrl ìƒì„±
            file_url = None
            if matched_chunk:
                chunk_file_url = getattr(matched_chunk, 'file_url', None)
                if chunk_file_url and chunk_file_url.strip():
                    file_url = chunk_file_url
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] finding fileUrlì„ chunk.file_urlì—ì„œ ê°€ì ¸ì˜´: {file_url[:50]}...")
            
            if not file_url and external_id:
                try:
                    file_url = get_document_file_url(
                        external_id=external_id,
                        source_type=source_type,
                        expires_in=3600
                    )
                    if file_url:
                        logger.info(f"[ì›Œí¬í”Œë¡œìš°] finding fileUrl ìƒì„± ì„±ê³µ: external_id={external_id}, source_type={source_type}")
                    else:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] finding fileUrl ìƒì„± ê²°ê³¼ None: external_id={external_id}, source_type={source_type}")
                except Exception as e:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] finding fileUrl ìƒì„± ì‹¤íŒ¨ (external_id={external_id}): {str(e)}")
            
            # sourceType ë§¤í•‘ (guideline -> manual, statute -> law)
            mapped_source_type = source_type
            if source_type == "guideline":
                mapped_source_type = "manual"
            elif source_type == "statute":
                mapped_source_type = "law"
            
            # refinedSnippet: LLMì´ ìƒì„±í•œ ê²ƒì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ chunkì˜ snippet ì‚¬ìš©
            final_refined_snippet = refined_snippet if refined_snippet else chunk_snippet
            
            # source ì •ë³´ ì¶”ê°€
            finding["source"] = {
                "documentTitle": document_title,
                "fileUrl": file_url or "",
                "sourceType": mapped_source_type,
                "refinedSnippet": final_refined_snippet,  # LLMì´ ìƒì„±í•œ refinedSnippet ìš°ì„  ì‚¬ìš©
                "similarityScore": chunk_score,
            }
            
            file_url_status = 'ìˆìŒ' if file_url else 'ì—†ìŒ'
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] finding '{finding.get('title', 'unknown')}' source ì •ë³´ ì„¤ì • ì™„ë£Œ: fileUrl={file_url_status}, sourceType={mapped_source_type}, similarityScore={chunk_score}")
            
            findings_mapped.append(finding)
        
        return findings_mapped
    
    def _extract_title_keywords(self, title: str) -> List[str]:
        """ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ë§¤ì¹­ìš©)"""
        if not title:
            return []
        
        # ì œëª© ì •ë¦¬ (í™•ì¥ì ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        import re
        title_clean = re.sub(r'\.(pdf|hwp|hwpx|docx?)$', '', title, flags=re.IGNORECASE)
        title_clean = re.sub(r'[^\w\sê°€-í£]', ' ', title_clean)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ë°', 'ì˜', 'ê³¼', 'ì™€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ê°€', 'ì´', 'ì€', 'ëŠ”', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°']
        
        # ë‹¨ì–´ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        words = [w.strip() for w in title_clean.split() if len(w.strip()) >= 2 and w.strip() not in stopwords]
        
        # í•µì‹¬ í‚¤ì›Œë“œë§Œ ì„ íƒ (ê¸´ ë‹¨ì–´ ìš°ì„ )
        keywords = sorted(set(words), key=lambda x: -len(x))[:5]  # ìµœëŒ€ 5ê°œ
        
        return keywords
    
    async def _get_embedding(self, text: str) -> List[float]:
        """ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)"""
        # legal_rag_serviceì˜ _get_embeddingê³¼ ë™ì¼í•œ ë¡œì§
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ generator ì‚¬ìš©
        return await asyncio.to_thread(self.generator.embed_one, text)
    
    async def _llm_classify(
        self,
        situation_text: str,
        category_hint: Optional[str] = None,
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> Dict[str, Any]:
        """LLMìœ¼ë¡œ ìƒí™© ë¶„ë¥˜"""
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒˆë¡œ ë§Œë“¤ì–´ì•¼ í•¨)
        prompt = build_situation_classify_prompt(
            situation_text=situation_text,
            category_hint=category_hint,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        
        # LLM í˜¸ì¶œ
        response = await self._call_llm(prompt)
        
        # JSON íŒŒì‹±
        import json
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            classification = json.loads(json_match.group())
            
            # classified_type ì •ê·œí™”: íŒŒì´í”„ë¡œ êµ¬ë¶„ëœ ê°’ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
            classified_type = classification.get("classified_type", category_hint or "unknown")
            if isinstance(classified_type, str) and "|" in classified_type:
                # íŒŒì´í”„ë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
                classified_type = classified_type.split("|")[0].strip()
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] classified_typeì— ì—¬ëŸ¬ ê°’ì´ í¬í•¨ë¨, ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©: {classification.get('classified_type')} -> {classified_type}")
            
            # ìœ íš¨í•œ ë¶„ë¥˜ ìœ í˜•ì¸ì§€ í™•ì¸
            valid_types = ["harassment", "unpaid_wage", "unfair_dismissal", "overtime", "probation", "freelancer", "stock_option", "other", "unknown"]
            if classified_type not in valid_types:
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ìœ íš¨í•˜ì§€ ì•Šì€ classified_type: {classified_type}, ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€ê²½")
                classified_type = category_hint or "unknown"
            
            classification["classified_type"] = classified_type
            return classification
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        return {
            "classified_type": category_hint or "unknown",
            "risk_score": 50,
            "categories": [],
        }
    
    async def _filter_rules_by_classification(
        self,
        classified_type: str,
        classification: Dict[str, Any],
    ) -> List[str]:
        """ë¶„ë¥˜ ê²°ê³¼ ê¸°ë°˜ ê·œì • í•„í„°ë§"""
        # classificationì—ì„œ categories ì¶”ì¶œ (LLMì´ ë°˜í™˜í•œ ê²½ìš°)
        llm_categories = classification.get("categories", [])
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (fallback)
        category_mapping = {
            "harassment": ["ì§ì¥ë‚´ê´´ë¡­í˜", "ëª¨ìš•", "ì¸ê²©ê¶Œ"],
            "unpaid_wage": ["ì„ê¸ˆì²´ë¶ˆ", "ìµœì €ì„ê¸ˆ", "ì„ê¸ˆì§€ê¸‰", "ì—°ì¥ê·¼ë¡œìˆ˜ë‹¹", "ë¬´ê¸‰ì•¼ê·¼"],
            "unfair_dismissal": ["ë¶€ë‹¹í•´ê³ ", "ê³„ì•½í•´ì§€", "í•´ê³ í†µì§€"],
            "overtime": ["ì—°ì¥ê·¼ë¡œ", "ì•¼ê°„ê·¼ë¡œ", "íœ´ì¼ê·¼ë¡œ", "ê·¼ë¡œì‹œê°„"],
            "probation": ["ìˆ˜ìŠµ", "ì¸í„´", "ê³„ì•½ê¸°ê°„"],
            "freelancer": ["í”„ë¦¬ëœì„œ", "ìš©ì—­", "ëŒ€ê¸ˆë¯¸ì§€ê¸‰", "ê³„ì•½ìœ„ë°˜"],
            "stock_option": ["ìŠ¤í†¡ì˜µì…˜", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ", "ì§€ë¶„"],
            "other": [],
            "unknown": [],
        }
        
        # LLMì´ ë°˜í™˜í•œ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ë§¤í•‘ ì‚¬ìš©
        if llm_categories:
            return llm_categories
        
        return category_mapping.get(classified_type, [])
    
    async def _search_legal_with_filter(
        self,
        query_embedding: List[float],
        categories: List[str],
        top_k: int = 8,
    ) -> List[LegalGroundingChunk]:
        """ì¹´í…Œê³ ë¦¬ í•„í„°ë§ëœ ë²•ë ¹ ê²€ìƒ‰"""
        # í•„í„° êµ¬ì„±
        filters = None
        if categories:
            # metadataì— category í•„ë“œê°€ ìˆë‹¤ê³  ê°€ì •
            # ì‹¤ì œ êµ¬í˜„ì€ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„
            filters = {"category": categories}
        
        rows = self.vector_store.search_similar_legal_chunks(
            query_embedding=query_embedding,
            top_k=top_k,
            filters=filters,
        )
        
        results: List[LegalGroundingChunk] = []
        for r in rows:
            source_type = r.get("source_type", "law")
            title = r.get("title", "ì œëª© ì—†ìŒ")
            content = r.get("content", "")
            score = r.get("score", 0.0)
            file_path = r.get("file_path", None)
            external_id = r.get("external_id", None)
            chunk_index = r.get("chunk_index", None)
            
            # file_pathê°€ ì—†ìœ¼ë©´ external_idë¡œ ìƒì„±
            if not file_path and external_id:
                from core.legal_rag_service import LegalRAGService
                service = LegalRAGService()
                file_path = service._build_file_path(source_type, external_id)
            
            # ìŠ¤í† ë¦¬ì§€ íŒŒì¼ URL ìƒì„±
            file_url = None
            if external_id:
                try:
                    file_url = self.vector_store.get_storage_file_url(
                        external_id=external_id,
                        source_type=source_type,
                        expires_in=3600  # 1ì‹œê°„
                    )
                except Exception as e:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ìŠ¤í† ë¦¬ì§€ URL ìƒì„± ì‹¤íŒ¨ (external_id={external_id}, source_type={source_type}): {str(e)}")
            
            results.append(
                LegalGroundingChunk(
                    source_id=r.get("id", ""),
                    source_type=source_type,
                    title=title,
                    snippet=content[:300],
                    score=score,
                    file_path=file_path,
                    external_id=external_id,
                    chunk_index=chunk_index,
                    file_url=file_url,  # íŒŒì¼ URL ì¶”ê°€
                )
            )
        return results
    
    async def _search_cases_with_embedding(
        self,
        query_embedding: List[float],
        top_k: int = 3,
    ) -> List[Dict[str, Any]]:
        """
        ì¼€ì´ìŠ¤ ê²€ìƒ‰ (case ë˜ëŠ” standard_contract íƒ€ì…)
        source_type ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜
        """
        # caseì™€ standard_contract ëª¨ë‘ ê²€ìƒ‰ (í•„í„° ì œê±°)
        rows = self.vector_store.search_similar_legal_chunks(
            query_embedding=query_embedding,
            top_k=top_k * 2,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        )
        
        cases: List[Dict[str, Any]] = []
        for row in rows:
            source_type = row.get("source_type", "case")
            # case ë˜ëŠ” standard_contractë§Œ í¬í•¨
            if source_type not in ["case", "standard_contract"]:
                continue
            
            external_id = row.get("external_id", "")
            title = row.get("title", "ì œëª© ì—†ìŒ")
            content = row.get("content", "")
            metadata = row.get("metadata", {})
            
            cases.append({
                "id": external_id,
                "title": title,
                "situation": metadata.get("situation", content[:200]),
                "main_issues": metadata.get("issues", []),
                "source_type": source_type,  # source_type ì •ë³´ í¬í•¨
            })
            
            if len(cases) >= top_k:
                break
        
        return cases
    
    def _extract_legal_basis(self, grounding_chunks: List[LegalGroundingChunk]) -> List[Dict[str, Any]]:
        """RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ legalBasis êµ¬ì¡° ì¶”ì¶œ"""
        legal_basis = []
        for chunk in grounding_chunks[:5]:  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            legal_basis.append({
                "title": chunk.title,
                "snippet": chunk.snippet,
                "source_type": chunk.source_type,
                "source_id": chunk.source_id,
            })
        return legal_basis
    
    async def _llm_generate_action_guide(
        self,
        situation_text: str,
        classification: Dict[str, Any],
        grounding_chunks: List[LegalGroundingChunk],
        legal_basis: List[Dict[str, Any]],
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> Dict[str, Any]:
        """í–‰ë™ ê°€ì´ë“œ ìƒì„± (summary, criteria, actionPlan, scripts ëª¨ë‘)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] _llm_generate_action_guide ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        prompt = build_situation_action_guide_prompt(
            situation_text=situation_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ - ê¸¸ì´: {len(prompt)}ì, grounding_chunks: {len(grounding_chunks)}ê°œ, legal_basis: {len(legal_basis)}ê°œ")
        
        logger.info("[ì›Œí¬í”Œë¡œìš°] LLM í˜¸ì¶œ ì‹œì‘ (í–‰ë™ ê°€ì´ë“œ ìƒì„±)...")
        response = await self._call_llm(prompt)
        logger.info("[ì›Œí¬í”Œë¡œìš°] LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ - JSON íŒŒì‹± ì‹œì‘...")
        
        # JSON íŒŒì‹±
        import json
        import re
        
        # ì‘ë‹µ ë¡œê¹… (ë””ë²„ê¹…ìš©) - ê°•í™”
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM raw ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM raw ì‘ë‹µ (ì²˜ìŒ 1500ì): {response[:1500]}")
        if len(response) > 1500:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM raw ì‘ë‹µ (ë§ˆì§€ë§‰ 500ì): {response[-500:]}")
        
        # ì½”ë“œ ë¸”ë¡ ì œê±°
        response_clean = response.strip()
        if response_clean.startswith("```json"):
            response_clean = response_clean[7:]
        elif response_clean.startswith("```"):
            response_clean = response_clean[3:]
        if response_clean.endswith("```"):
            response_clean = response_clean[:-3]
        response_clean = response_clean.strip()
        
        # JSON ê°ì²´ ì¶”ì¶œ (ë” robustí•œ íŒŒì‹±)
        json_match = re.search(r'\{.*\}', response_clean, re.DOTALL)
        if not json_match:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. response_clean (ì²˜ìŒ 500ì): {response_clean[:500]}")
        
        if json_match:
            try:
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì‹œë„, response_clean ê¸¸ì´: {len(response_clean)}")
                # ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ìœ íš¨í•œ JSON ì¶”ì¶œ
                json_str = json_match.group()
                brace_count = 0
                last_valid_pos = -1
                for i, char in enumerate(json_str):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_valid_pos = i + 1
                            break
                
                if last_valid_pos > 0:
                    json_str = json_str[:last_valid_pos]
                
                # summary í•„ë“œì˜ ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                # JSON ë¬¸ìì—´ ë‚´ì—ì„œ ì œì–´ ë¬¸ì(ê°œí–‰, íƒ­ ë“±)ë¥¼ ì´ìŠ¤ì¼€ì´í”„
                def escape_control_chars_in_json_string(json_str: str) -> str:
                    """JSON ë¬¸ìì—´ ë‚´ì˜ ì œì–´ ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬"""
                    # summary í•„ë“œ ì°¾ê¸°
                    summary_pattern = r'"summary"\s*:\s*"([^"]*(?:\\.[^"]*)*)"'
                    
                    def escape_summary(match):
                        field_name = match.group(0).split(':')[0]  # "summary"
                        value = match.group(1)  # ì‹¤ì œ ê°’
                        
                        # ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìëŠ” ìœ ì§€í•˜ë©´ì„œ ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                        # ê°œí–‰ ë¬¸ì ì²˜ë¦¬
                        value = value.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                        # ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ (ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ ê²ƒì€ ì œì™¸)
                        value = re.sub(r'(?<!\\)"', '\\"', value)
                        
                        return f'{field_name}: "{value}"'
                    
                    # summary í•„ë“œë§Œ ì²˜ë¦¬
                    json_str = re.sub(summary_pattern, escape_summary, json_str, flags=re.DOTALL)
                    return json_str
                
                # summary í•„ë“œì˜ ì œì–´ ë¬¸ì ì²˜ë¦¬ (legal_rag_serviceì™€ ë™ì¼í•œ ë¡œì§)
                def clean_summary_field_in_json(json_str: str) -> str:
                    """summary í•„ë“œ ë‚´ë¶€ì˜ ì œì–´ ë¬¸ìë¥¼ JSON ì´ìŠ¤ì¼€ì´í”„ë¡œ ë³€í™˜"""
                    try:
                        # summary í•„ë“œì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
                        summary_start = json_str.find('"summary"')
                        if summary_start == -1:
                            return json_str
                        
                        # summary í•„ë“œì˜ ê°’ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (ì½œë¡ ê³¼ ë”°ì˜´í‘œ ì´í›„)
                        value_start = json_str.find('"', summary_start + 9)  # "summary" ê¸¸ì´ + 1
                        if value_start == -1:
                            return json_str
                        
                        value_start += 1  # ë”°ì˜´í‘œ ë‹¤ìŒë¶€í„°
                        
                        # ë¬¸ìì—´ ë ì°¾ê¸° (ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ ê³ ë ¤)
                        value_end = value_start
                        while value_end < len(json_str):
                            char = json_str[value_end]
                            
                            # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ì ê±´ë„ˆë›°ê¸°
                            if char == '\\' and value_end + 1 < len(json_str):
                                value_end += 2
                                continue
                            
                            # ë”°ì˜´í‘œ ì²˜ë¦¬
                            if char == '"':
                                # ì•ì˜ ë°±ìŠ¬ë˜ì‹œ ê°œìˆ˜ ì„¸ê¸°
                                backslash_count = 0
                                i = value_end - 1
                                while i >= value_start and json_str[i] == '\\':
                                    backslash_count += 1
                                    i -= 1
                                # í™€ìˆ˜ ê°œì˜ ë°±ìŠ¬ë˜ì‹œë©´ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ, ì§ìˆ˜ ê°œë©´ ë¬¸ìì—´ ë
                                if backslash_count % 2 == 0:
                                    break
                            
                            value_end += 1
                        
                        if value_end >= len(json_str):
                            # ë¬¸ìì—´ ëì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë‹¤ìŒ í°ë”°ì˜´í‘œê¹Œì§€ ì°¾ê¸°
                            next_quote = json_str.find('"', value_start)
                            if next_quote > value_start:
                                value_end = next_quote
                            else:
                                return json_str
                        
                        # summary í•„ë“œ ë‚´ìš© ì¶”ì¶œ
                        content = json_str[value_start:value_end]
                        
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜ (ì¼ì‹œì )
                        content_decoded = content.replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t').replace('\\"', '"').replace('\\\\', '\\')
                        
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                        content_decoded = re.sub(r'```markdown\s*', '', content_decoded, flags=re.IGNORECASE)
                        content_decoded = re.sub(r'```\s*', '', content_decoded, flags=re.MULTILINE)
                        
                        # ì œì–´ ë¬¸ìë¥¼ JSON ì´ìŠ¤ì¼€ì´í”„ë¡œ ë³€í™˜
                        result = []
                        for char in content_decoded:
                            if char == '\n':
                                result.append('\\n')
                            elif char == '\r':
                                result.append('\\r')
                            elif char == '\t':
                                result.append('\\t')
                            elif char == '"':
                                result.append('\\"')
                            elif char == '\\':
                                result.append('\\\\')
                            elif ord(char) < 32:  # ì œì–´ ë¬¸ì
                                result.append(f'\\u{ord(char):04x}')
                            else:
                                result.append(char)
                        
                        # summary í•„ë“œ êµì²´
                        cleaned_content = ''.join(result)
                        return json_str[:value_start] + cleaned_content + json_str[value_end:]
                    except Exception as e:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summary í•„ë“œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}, ì›ë³¸ JSON ì‚¬ìš©")
                        return json_str
                
                # summary í•„ë“œ ì •ë¦¬
                json_str_cleaned = clean_summary_field_in_json(json_str)
                
                # JSON íŒŒì‹±
                result = json.loads(json_str_cleaned)
                
                # summary í•„ë“œì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (ìˆëŠ” ê²½ìš°)
                if "summary" in result and isinstance(result["summary"], str):
                    summary = result["summary"]
                    # ```markdown ... ``` ì œê±°
                    summary = re.sub(r'```markdown\s*', '', summary, flags=re.IGNORECASE)
                    summary = re.sub(r'```\s*$', '', summary, flags=re.MULTILINE)
                    # ë”°ì˜´í‘œ escape ì²˜ë¦¬ (JSON ë‚´ë¶€ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
                    
                    # í•œì/ì¼ë³¸ì–´ ë¬¸ìë¥¼ í•œê¸€ë¡œ ë³€í™˜ ë˜ëŠ” ì œê±°
                    def remove_cjk_japanese(text: str) -> str:
                        """í•œì, ì¼ë³¸ì–´ ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ í•œê¸€ë¡œ ë³€í™˜"""
                        import unicodedata
                        
                        # ì¼ë°˜ì ì¸ í•œì-í•œê¸€ ë§¤í•‘
                        hanja_to_hangul = {
                            'æœ€è¿‘': 'ìµœê·¼',
                            'å…¸å‹': 'ì „í˜•',
                            'å…¸å‹ì ì¸': 'ì „í˜•ì ì¸',
                        }
                        
                        # ë§¤í•‘ëœ í•œì ë³€í™˜
                        for hanja, hangul in hanja_to_hangul.items():
                            text = text.replace(hanja, hangul)
                        
                        # í•œì ë²”ìœ„ (CJK í†µí•© í•œì: U+4E00â€“U+9FFF, í•œì ë³´ì¶©: U+3400â€“U+4DBF)
                        # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: U+3040â€“U+309F, ê°€íƒ€ì¹´ë‚˜: U+30A0â€“U+30FF
                        result = []
                        for char in text:
                            code = ord(char)
                            # í•œì ë²”ìœ„ ì²´í¬
                            is_hanja = (0x4E00 <= code <= 0x9FFF) or (0x3400 <= code <= 0x4DBF)
                            # ì¼ë³¸ì–´ ë²”ìœ„ ì²´í¬
                            is_japanese = (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF)
                            
                            if is_hanja or is_japanese:
                                # í•œì/ì¼ë³¸ì–´ ë¬¸ìëŠ” ì œê±°
                                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•œì/ì¼ë³¸ì–´ ë¬¸ì ì œê±°: {char} (U+{code:04X})")
                                continue
                            result.append(char)
                        
                        return ''.join(result)
                    
                    summary = remove_cjk_japanese(summary)
                    
                    result["summary"] = summary.strip()
                
                # action_planì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                action_plan = result.get('action_plan', {})
                if isinstance(action_plan, dict):
                    action_plan_steps = len(action_plan.get('steps', []))
                elif isinstance(action_plan, list):
                    action_plan_steps = len(action_plan)
                else:
                    action_plan_steps = 0
                
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì„±ê³µ - summary ê¸¸ì´: {len(result.get('summary', ''))}ì, criteria ê°œìˆ˜: {len(result.get('criteria', []))}ê°œ, action_plan steps: {action_plan_steps}ê°œ")
                
                # action_plan ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                action_plan_safe = result.get('action_plan', {})
                if isinstance(action_plan_safe, dict):
                    action_plan_steps_count = len(action_plan_safe.get('steps', []))
                else:
                    action_plan_steps_count = 0
                
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] íŒŒì‹±ëœ action_result: summary ê¸¸ì´={len(result.get('summary', ''))}, criteria ê°œìˆ˜={len(result.get('criteria', []))}, findings ê°œìˆ˜={len(result.get('findings', []))}, action_plan steps={action_plan_steps_count}")
                return result
            except json.JSONDecodeError as e:
                logger.error(f"[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                if hasattr(e, 'lineno') and hasattr(e, 'colno'):
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] ì—ëŸ¬ ìœ„ì¹˜: line {e.lineno}, column {e.colno}")
                logger.error(f"[ì›Œí¬í”Œë¡œìš°] ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 1000ì): {response_clean[:1000]}")
                if 'json_str_cleaned' in locals():
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] json_str_cleaned ê¸¸ì´: {len(json_str_cleaned)}")
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] json_str_cleaned (ì²˜ìŒ 500ì): {json_str_cleaned[:500]}")
                
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ë¶€ë¶„ì ìœ¼ë¡œ íŒŒì‹± ì‹œë„
                try:
                    # summary, criteria, action_plan, scripts í•„ë“œ ì¶”ì¶œ ì‹œë„
                    json_to_search = json_str_cleaned if 'json_str_cleaned' in locals() else (json_str if 'json_str' in locals() else response_clean)
                    
                    # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ summary í•„ë“œ ì°¾ê¸°
                    summary_patterns = [
                        r'"summary"\s*:\s*"((?:[^"\\]|\\.)*)"',  # ì¼ë°˜ì ì¸ JSON ë¬¸ìì—´
                        r'"summary"\s*:\s*"([^"]*)"',  # ê°„ë‹¨í•œ íŒ¨í„´
                        r'summary["\s]*:["\s]*([^",}]+)',  # ë” ìœ ì—°í•œ íŒ¨í„´
                    ]
                    
                    summary_text = None
                    for pattern in summary_patterns:
                        summary_match = re.search(pattern, json_to_search, re.DOTALL | re.IGNORECASE)
                        if summary_match:
                            summary_text = summary_match.group(1)
                            # ì´ìŠ¤ì¼€ì´í”„ ì œê±°
                            summary_text = summary_text.replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t').replace('\\"', '"')
                            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summary í•„ë“œ ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´: {pattern[:30]}...)")
                            break
                    
                    # criteria í•„ë“œ ì¶”ì¶œ ì‹œë„
                    criteria_list = []
                    try:
                        # criteria ë°°ì—´ íŒ¨í„´ ì°¾ê¸° (ë” ê°•ë ¥í•œ íŒ¨í„´)
                        # ë¨¼ì € criteria ë°°ì—´ì˜ ì‹œì‘ê³¼ ëì„ ì°¾ê¸°
                        criteria_start = json_to_search.find('"criteria"')
                        if criteria_start != -1:
                            # criteria ë‹¤ìŒì˜ [ ì°¾ê¸°
                            bracket_start = json_to_search.find('[', criteria_start)
                            if bracket_start != -1:
                                # ì¤‘ì²©ëœ ì¤‘ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ë¥¼ ê³ ë ¤í•˜ì—¬ ë°°ì—´ ë ì°¾ê¸°
                                bracket_count = 0
                                bracket_end = bracket_start
                                in_string = False
                                escape_next = False
                                
                                for i in range(bracket_start, len(json_to_search)):
                                    char = json_to_search[i]
                                    
                                    if escape_next:
                                        escape_next = False
                                        continue
                                    
                                    if char == '\\':
                                        escape_next = True
                                        continue
                                    
                                    if char == '"' and not escape_next:
                                        in_string = not in_string
                                        continue
                                    
                                    if not in_string:
                                        if char == '[':
                                            bracket_count += 1
                                        elif char == ']':
                                            bracket_count -= 1
                                            if bracket_count == 0:
                                                bracket_end = i + 1
                                                break
                                
                                if bracket_end > bracket_start:
                                    criteria_array_str = json_to_search[bracket_start:bracket_end]
                                    # ê° ê°ì²´ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
                                    # { "name": "...", "status": "...", "reason": "..." }
                                    item_pattern = r'\{\s*"name"\s*:\s*"((?:[^"\\]|\\.)*)"\s*,\s*"status"\s*:\s*"((?:[^"\\]|\\.)*)"\s*,\s*"reason"\s*:\s*"((?:[^"\\]|\\.)*)"'
                                    items = re.findall(item_pattern, criteria_array_str, re.DOTALL)
                                    for name, status, reason in items:
                                        # ì´ìŠ¤ì¼€ì´í”„ ì œê±°
                                        name = name.replace('\\"', '"').replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
                                        status = status.replace('\\"', '"').replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
                                        reason = reason.replace('\\"', '"').replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
                                        criteria_list.append({
                                            "name": name,
                                            "status": status,
                                            "reason": reason,
                                        })
                                    if criteria_list:
                                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] criteria í•„ë“œ ì¶”ì¶œ ì„±ê³µ: {len(criteria_list)}ê°œ")
                    except Exception as criteria_error:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] criteria ì¶”ì¶œ ì‹¤íŒ¨: {str(criteria_error)}")
                    
                    if summary_text:
                        # ë¶€ë¶„ íŒŒì‹± ê²°ê³¼ ë°˜í™˜ (criteria í¬í•¨)
                        return {
                            "summary": summary_text.strip(),
                            "action_plan": {"steps": []},
                            "scripts": {},
                            "criteria": criteria_list,  # ì¶”ì¶œëœ criteria ì‚¬ìš©
                        }
                    else:
                        logger.warning("[ì›Œí¬í”Œë¡œìš°] summary í•„ë“œë„ ì¶”ì¶œ ì‹¤íŒ¨")
                except Exception as partial_error:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ë¶€ë¶„ íŒŒì‹±ë„ ì‹¤íŒ¨: {str(partial_error)}", exc_info=True)
        
        # ê¸°ë³¸ê°’ (4ê°œ ì„¹ì…˜ êµ¬ì¡° ìœ ì§€)
        logger.warning("[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
        if 'response_clean' in locals():
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] response_clean (ì²˜ìŒ 500ì): {response_clean[:500]}")
        return {
            "summary": "## ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\në¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n## ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ìƒí™©\n\në²•ì  ê·¼ê±°ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n## ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”\n\n## ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
            "action_plan": {"steps": []},
            "scripts": {},
            "criteria": [],
        }
    
    def _reconstruct_summary_sections(self, summary: str, section_patterns: List[Dict[str, Any]], classified_type: str = "unknown") -> Optional[str]:
        """LLMì´ ìƒì„±í•œ summaryë¥¼ íŒŒì‹±í•˜ì—¬ ì˜¬ë°”ë¥¸ ì„¹ì…˜ í˜•ì‹ìœ¼ë¡œ ì¬êµ¬ì„±"""
        try:
            # summaryë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
            lines = summary.split('\n')
            reconstructed_parts = []
            
            # ê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ
            section_contents = {}
            current_section_key = None
            
            for i, line in enumerate(lines):
                line_stripped = line.strip()
                
                # ì„¹ì…˜ í—¤ë” ì°¾ê¸° (ì´ëª¨ì§€ + í‚¤ì›Œë“œ ê¸°ë°˜)
                for section_info in section_patterns:
                    # ë¨¼ì € ì •í™•í•œ í—¤ë” í˜•ì‹ í™•ì¸ (ìƒˆë¡œìš´ í˜•ì‹: ì´ëª¨ì§€ + êµµì€ ì œëª©)
                    if section_info["title"] in line_stripped:
                        current_section_key = section_info["title"]
                        if current_section_key not in section_contents:
                            section_contents[current_section_key] = []
                        break
                    
                    # ì´ëª¨ì§€ë¡œ í™•ì¸
                    emoji = section_info.get("emoji")
                    if emoji and emoji in line_stripped:
                        # ì´ëª¨ì§€ ë’¤ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        for keyword in section_info["keywords"]:
                            if keyword != emoji and keyword in line_stripped:
                                current_section_key = section_info["title"]
                                if current_section_key not in section_contents:
                                    section_contents[current_section_key] = []
                                break
                        if current_section_key:
                            break
                    
                    # í‚¤ì›Œë“œë¡œ í™•ì¸ (ë ˆê±°ì‹œ í˜•ì‹ ì§€ì›)
                    for keyword in section_info["keywords"]:
                        if keyword == emoji:
                            continue  # ì´ë¯¸ ì´ëª¨ì§€ë¡œ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                        # í—¤ë” í˜•ì‹ í™•ì¸ (## í‚¤ì›Œë“œ, # í‚¤ì›Œë“œ, ë˜ëŠ” í‚¤ì›Œë“œë§Œ) - ë” ìœ ì—°í•œ ë§¤ì¹­
                        keyword_in_line = keyword.lower() in line_stripped.lower()
                        is_header_format = re.match(r'^##?\s*', line_stripped) is not None
                        is_short_line = len(line_stripped) < 80  # ë” ê¸´ ì¤„ë„ í—ˆìš©
                        
                        if keyword_in_line and (is_header_format or is_short_line):
                            current_section_key = section_info["title"]
                            if current_section_key not in section_contents:
                                section_contents[current_section_key] = []
                            break
                    if current_section_key:
                        break
                
                # í˜„ì¬ ì„¹ì…˜ì— ë‚´ìš© ì¶”ê°€
                if current_section_key:
                    # í—¤ë” ë¼ì¸ì´ ì•„ë‹ˆë©´ ë‚´ìš©ìœ¼ë¡œ ì¶”ê°€ - ë” ìœ ì—°í•œ í—¤ë” ê°ì§€
                    is_header = False
                    for section_info in section_patterns:
                        # ì •í™•í•œ í—¤ë” í˜•ì‹ í™•ì¸
                        if section_info["title"] in line_stripped:
                            is_header = True
                            break
                        
                        # ì´ëª¨ì§€ë¡œ í™•ì¸
                        emoji = section_info.get("emoji")
                        if emoji and emoji in line_stripped:
                            # ì´ëª¨ì§€ ë’¤ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                            for keyword in section_info["keywords"]:
                                if keyword != emoji and keyword in line_stripped:
                                    is_header = True
                                    break
                            if is_header:
                                break
                        
                        # í‚¤ì›Œë“œë¡œ í™•ì¸ (ë ˆê±°ì‹œ í˜•ì‹ ì§€ì›)
                        for keyword in section_info["keywords"]:
                            if keyword == emoji:
                                continue
                            keyword_in_line = keyword.lower() in line_stripped.lower()
                            is_header_format = re.match(r'^##?\s*', line_stripped) is not None
                            is_short_line = len(line_stripped) < 80
                            # í‚¤ì›Œë“œê°€ í¬í•¨ë˜ê³  í—¤ë” í˜•ì‹ì´ê±°ë‚˜ ì§§ì€ ì¤„ì´ë©´ í—¤ë”ë¡œ ì¸ì‹
                            if keyword_in_line and (is_header_format or is_short_line):
                                is_header = True
                                break
                        if is_header:
                            break
                    
                    if not is_header:
                        section_contents[current_section_key].append(line)
                else:
                    # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì„¹ì…˜ìœ¼ë¡œ ê°„ì£¼
                    if not section_contents:
                        first_section = section_patterns[0]["title"]
                        section_contents[first_section] = [line]
            
            # ì¬êµ¬ì„±ëœ summary ìƒì„±
            for section_info in section_patterns:
                title = section_info["title"]
                if title in section_contents and section_contents[title]:
                    reconstructed_parts.append(title)
                    reconstructed_parts.append("")
                    reconstructed_parts.extend(section_contents[title])
                    reconstructed_parts.append("")
                else:
                    # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
                    section_key = section_info.get("keywords", [""])[0] if section_info.get("keywords") else ""
                    
                    default_content_by_type = {
                        "unpaid_wage": {
                            "ë²•ì  ê´€ì ": "ì„ê¸ˆì²´ë¶ˆì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ43ì¡°(ì„ê¸ˆì§€ê¸‰), ì œ36ì¡°(ì„ê¸ˆì˜ ì§€ê¸‰)ì™€ ê´€ë ¨ëœ ì‚¬ì•ˆì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ê·¼ë¡œìì—ê²Œ ì„ê¸ˆì„ ì •ê¸°ì ìœ¼ë¡œ ì§€ê¸‰í•  ì˜ë¬´ê°€ ìˆìœ¼ë©°, ì´ë¥¼ ìœ„ë°˜í•  ê²½ìš° í˜•ì‚¬ì²˜ë²Œê³¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ê·¼ë¡œê³„ì•½ì„œì™€ ê¸‰ì—¬ëª…ì„¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”\n- ì¶œí‡´ê·¼ ê¸°ë¡ê³¼ ê·¼ë¬´ì‹œê°„ì„ ì •ë¦¬í•˜ì„¸ìš”\n- ì„ê¸ˆ ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ì •ì¤‘í•˜ê²Œ ì„ê¸ˆ ì§€ê¸‰ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "harassment": {
                            "ë²•ì  ê´€ì ": "ì§ì¥ ë‚´ ê´´ë¡­í˜ì€ ì§ì¥ ë‚´ ê´´ë¡­í˜ ë°©ì§€ ë° ê·¼ë¡œì ë³´í˜¸ ë“±ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¼ ê¸ˆì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—…ë¬´ìƒ ì§€ìœ„ë‚˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ ê·¼ë¡œìì—ê²Œ ì‹ ì²´ì Â·ì •ì‹ ì  ê³ í†µì„ ì£¼ëŠ” í–‰ìœ„ëŠ” ë²•ì  ì²˜ë²Œ ëŒ€ìƒì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ê´´ë¡­í˜ ê´€ë ¨ ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”\n- ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì‹œë¥¼ ê¸°ë¡í•˜ì„¸ìš”\n- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ê´´ë¡­í˜ ìƒí™©ì„ ì •ì¤‘í•˜ê²Œ ì•Œë¦¬ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "unfair_dismissal": {
                            "ë²•ì  ê´€ì ": "ë¶€ë‹¹í•´ê³ ëŠ” ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°(í•´ê³ ì˜ ì œí•œ)ì— ë”°ë¼ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ í•´ê³ í•˜ëŠ” ê²½ìš° ë³µì§ ì²­êµ¬ë‚˜ ì†í•´ë°°ìƒ ì²­êµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- í•´ê³  í†µì§€ì„œì™€ ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ê·¼ë¬´ ê¸°ê°„ê³¼ ì„±ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”\n- íšŒì‚¬ì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— í•´ê³  ì‚¬ìœ ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "overtime": {
                            "ë²•ì  ê´€ì ": "ê·¼ë¡œì‹œê°„ì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ50ì¡°(ê·¼ë¡œì‹œê°„), ì œ53ì¡°(ì—°ì¥ê·¼ë¡œ)ì— ë”°ë¼ ê·œì œë©ë‹ˆë‹¤. ë²•ì • ê·¼ë¡œì‹œê°„ì„ ì´ˆê³¼í•˜ëŠ” ì—°ì¥ê·¼ë¡œì— ëŒ€í•´ì„œëŠ” ê°€ì‚°ì„ê¸ˆì„ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ê·¼ë¬´ì‹œê°„ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”\n- ì—°ì¥ê·¼ë¡œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”\n- íœ´ê²Œì‹œê°„ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ê·¼ë¡œì‹œê°„ê³¼ ê°€ì‚°ì„ê¸ˆì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "probation": {
                            "ë²•ì  ê´€ì ": "ìˆ˜ìŠµê¸°ê°„ì€ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¸ì •ë©ë‹ˆë‹¤. ìˆ˜ìŠµê¸°ê°„ ì¤‘ì—ë„ ê·¼ë¡œê¸°ì¤€ë²•ìƒ ë³´í˜¸ë¥¼ ë°›ìœ¼ë©°, ë¶€ë‹¹í•œ í•´ê³ ëŠ” ì œí•œë©ë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìˆ˜ìŠµ ê¸°ê°„ê³¼ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”\n- ê·¼ë¡œê³„ì•½ì„œì˜ ìˆ˜ìŠµ ì¡°í•­ì„ ê²€í† í•˜ì„¸ìš”\n- ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ í‰ê°€ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ìˆ˜ìŠµê¸°ê°„ê³¼ í‰ê°€ ê¸°ì¤€ì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "freelancer": {
                            "ë²•ì  ê´€ì ": "í”„ë¦¬ëœì„œ/ìš©ì—­ ê³„ì•½ì—ì„œ ëŒ€ê¸ˆ ë¯¸ì§€ê¸‰ì€ ë¯¼ë²•ìƒ ì±„ë¬´ë¶ˆì´í–‰ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì•½ì„œì— ëª…ì‹œëœ ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹¤ì œ ì§€ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìš©ì—­ ê³„ì•½ì„œì™€ ëŒ€ê¸ˆ ì§€ê¸‰ ì•½ì •ì„ í™•ì¸í•˜ì„¸ìš”\n- ì‘ì—… ì™„ë£Œ ì¦ë¹™ ìë£Œë¥¼ ì •ë¦¬í•˜ì„¸ìš”\n- ëŒ€ê¸ˆ ì§€ê¸‰ ë‚´ì—­ê³¼ ë¯¸ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "ë°œì£¼ì‚¬ì— ëŒ€ê¸ˆ ì§€ê¸‰ì„ ìš”ì²­í•˜ëŠ” ì •ì¤‘í•œ ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "stock_option": {
                            "ë²•ì  ê´€ì ": "ìŠ¤í†¡ì˜µì…˜ì´ë‚˜ ì„±ê³¼ê¸‰ì€ ê³„ì•½ì„œë‚˜ ì•½ì •ì„œì— ëª…ì‹œëœ ì¡°ê±´ì— ë”°ë¼ ì§€ê¸‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. êµ¬ë‘ ì•½ì†ë§Œìœ¼ë¡œëŠ” ë²•ì  êµ¬ì†ë ¥ì´ ì•½í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ì„œí™”ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìŠ¤í†¡ì˜µì…˜/ì„±ê³¼ê¸‰ ì•½ì • ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”\n- ê³„ì•½ì„œë‚˜ ì•½ì •ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹œê¸°ë¥¼ ì •ë¦¬í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ìŠ¤í†¡ì˜µì…˜/ì„±ê³¼ê¸‰ ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹œê¸°ì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "other": {
                            "ë²•ì  ê´€ì ": "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
                            "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”\n- ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                            "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ë‚˜ ìƒë‹´ ê¸°ê´€ì— ìƒí™©ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                    }
                    
                    default_content = default_content_by_type.get(classified_type, {
                        "ë²•ì  ê´€ì ": "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”\n- ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ë‚˜ ìƒë‹´ ê¸°ê´€ì— ìƒí™©ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    })
                    
                    # ì„¹ì…˜ í‚¤ì›Œë“œë¡œ ë§¤ì¹­
                    section_key_matched = None
                    for key in default_content.keys():
                        if any(keyword in section_key for keyword in section_info.get("keywords", [])):
                            section_key_matched = key
                            break
                    
                    default_text = default_content.get(section_key_matched or section_key, "í•´ë‹¹ ì„¹ì…˜ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
                    # ê¸°ë³¸ê°’ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ì„¹ì…˜ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                    if default_text and default_text != "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤." and default_text != "í•´ë‹¹ ì„¹ì…˜ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.":
                        reconstructed_parts.append(title)
                        reconstructed_parts.append("")
                        reconstructed_parts.append(default_text)
                        reconstructed_parts.append("")
            
            return '\n'.join(reconstructed_parts).strip()
        except Exception as e:
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summary ì„¹ì…˜ ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _remove_cjk_japanese(self, text: str) -> str:
        """í•œì, ì¼ë³¸ì–´ ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ í•œê¸€ë¡œ ë³€í™˜ (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜)"""
        if not isinstance(text, str):
            return text
        
        # ì¼ë°˜ì ì¸ í•œì-í•œê¸€ ë§¤í•‘
        hanja_to_hangul = {
            'æœ€è¿‘': 'ìµœê·¼',
            'å…¸å‹': 'ì „í˜•',
            'å…¸å‹ì ì¸': 'ì „í˜•ì ì¸',
        }
        
        # ë§¤í•‘ëœ í•œì ë³€í™˜
        for hanja, hangul in hanja_to_hangul.items():
            text = text.replace(hanja, hangul)
        
        # í•œì ë²”ìœ„ (CJK í†µí•© í•œì: U+4E00â€“U+9FFF, í•œì ë³´ì¶©: U+3400â€“U+4DBF)
        # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: U+3040â€“U+309F, ê°€íƒ€ì¹´ë‚˜: U+30A0â€“U+30FF
        result = []
        for char in text:
            code = ord(char)
            # í•œì ë²”ìœ„ ì²´í¬
            is_hanja = (0x4E00 <= code <= 0x9FFF) or (0x3400 <= code <= 0x4DBF)
            # ì¼ë³¸ì–´ ë²”ìœ„ ì²´í¬
            is_japanese = (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF)
            
            if is_hanja or is_japanese:
                # í•œì/ì¼ë³¸ì–´ ë¬¸ìëŠ” ì œê±°
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•œì/ì¼ë³¸ì–´ ë¬¸ì ì œê±°: {char} (U+{code:04X})")
                continue
            result.append(char)
        
        return ''.join(result)
    
    def _reformat_action_result(self, action_result: Dict[str, Any], legal_basis: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì•¡ì…˜ ê²°ê³¼ ì •ê·œí™” ë° ê²€ì¦"""
        import json
        import re
        
        result = action_result.copy()
        
        # 1. criteria ê²€ì¦ ë° fallback
        criteria = result.get("criteria", [])
        if not criteria or len(criteria) == 0:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] criteriaê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. legal_basis ê¸°ë°˜ fallback ì‹œë„")
            # legal_basis ê¸°ë°˜ fallback
            if legal_basis and len(legal_basis) > 0:
                criteria = []
                for basis in legal_basis[:3]:
                    criteria.append({
                        "name": basis.get("title", "ë²•ì  ê·¼ê±°"),
                        "status": "unclear",
                        "reason": basis.get("snippet", "")[:300] if basis.get("snippet") else "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                    })
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] legal_basis ê¸°ë°˜ criteria ìƒì„±: {len(criteria)}ê°œ")
            else:
                criteria = [{
                    "name": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                    "status": "unclear",
                    "reason": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤."
                }]
        else:
            # criteria êµ¬ì¡° ê²€ì¦ ë° í•„í„°ë§
            validated_criteria = []
            for item in criteria:
                if not isinstance(item, dict):
                    continue
                
                name = item.get("name", "").strip()
                status = item.get("status", "unclear")
                reason = item.get("reason", "").strip()
                
                # ì™„ì „íˆ ë¹„ì–´ìˆëŠ” í•­ëª©ì€ ì œì™¸
                if not name and not reason:
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] ë¹„ì–´ìˆëŠ” criteria í•­ëª© ì œì™¸: {item}")
                    continue
                
                # nameì´ë‚˜ reasonì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                validated_criteria.append({
                    "name": name or "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                    "status": status or "unclear",
                    "reason": reason or "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                })
            
            # validated_criteriaê°€ ë¹„ì–´ìˆìœ¼ë©´ legal_basis ê¸°ë°˜ fallback
            if not validated_criteria and legal_basis and len(legal_basis) > 0:
                logger.warning("[ì›Œí¬í”Œë¡œìš°] ëª¨ë“  criteriaê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. legal_basis ê¸°ë°˜ fallback ì‹œë„")
                for basis in legal_basis[:3]:
                    validated_criteria.append({
                        "name": basis.get("title", "ë²•ì  ê·¼ê±°"),
                        "status": "unclear",
                        "reason": basis.get("snippet", "")[:300] if basis.get("snippet") else "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                    })
            
            criteria = validated_criteria if validated_criteria else [{
                "name": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                "status": "unclear",
                "reason": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤."
            }]
        
        # 2. action_plan ê²€ì¦ ë° ì •ê·œí™”
        action_plan = result.get("action_plan", {})
        if not isinstance(action_plan, dict):
            action_plan = {"steps": []}
        
        steps = action_plan.get("steps", [])
        if not isinstance(steps, list):
            steps = []
        
        # ê° step ê²€ì¦
        validated_steps = []
        for step in steps:
            if not isinstance(step, dict):
                continue
            
            title = step.get("title", "")
            items = step.get("items", [])
            
            # itemsê°€ ë°°ì—´ì´ ì•„ë‹ˆë©´ ë³€í™˜ ì‹œë„
            if not isinstance(items, list):
                if isinstance(items, str):
                    # ë¬¸ìì—´ì„ ë°°ì—´ë¡œ ë³€í™˜ (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
                    items = [item.strip() for item in items.split('\n') if item.strip()]
                elif isinstance(items, dict):
                    # ê°ì²´ë¥¼ ë°°ì—´ë¡œ ë³€í™˜ (ê°’ë§Œ ì¶”ì¶œ)
                    items = [str(v) for v in items.values() if v]
                else:
                    items = []
            
            # itemsì—ì„œ ë§ˆí¬ë‹¤ìš´ ì¡°ê° ì œê±° (ì˜ˆ: "- " ì œê±°) ë° ì¤‘ë³µ í•„í„°ë§
            # ì‹ ê³ /ìƒë‹´ ê´€ë ¨ í•­ëª©ì„ ì „ë¶€ ì‚­ì œí•˜ì§€ ì•Šê³ , í•˜ë“œ ì œì™¸ë§Œ ì ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë¶„ë¦¬
            # í•˜ë“œ ì œì™¸: ì „í™”ë²ˆí˜¸ + ìƒë‹´ì„¼í„° ê°™ì´ ë…¸ê³¨ì ì¸ ê²ƒë§Œ
            hard_exclude_keywords = [
                r'\d+.*ìƒë‹´ì„¼í„°',  # ì „í™”ë²ˆí˜¸ + ìƒë‹´ì„¼í„°
                r'ì²­ë…„ë…¸ë™ì„¼í„°',
            ]
            
            # ìƒë‹´/ì‹ ê³  ê´€ë ¨ í‚¤ì›Œë“œ (ë³„ë„ ë¶„ë¥˜ìš©)
            consult_keywords = [
                r'ë…¸ë¬´ì‚¬',
                r'ë…¸ë™ì²­',
                r'ê³ ìš©ë…¸ë™ë¶€',
                r'ìƒë‹´',
                r'ì‹ ê³ ',
            ]
            
            normal_items = []
            consult_items = []
            
            for item in items:
                if isinstance(item, str):
                    # "- " ë˜ëŠ” "* " ì œê±°
                    cleaned = re.sub(r'^[-*]\s+', '', item.strip())
                    if not cleaned:
                        continue
                    
                    # í•˜ë“œ ì œì™¸: ë„ˆë¬´ ë…¸ê³¨ì ì¸ "ê¸°ê´€ í™ë³´/ì „í™”ë²ˆí˜¸" ë¥˜ë§Œ ì™„ì „ ì œì™¸
                    should_hard_exclude = any(
                        re.search(pattern, cleaned, re.IGNORECASE)
                        for pattern in hard_exclude_keywords
                    )
                    if should_hard_exclude:
                        logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•˜ë“œ ì œì™¸: {cleaned}")
                        continue
                    
                    # ìƒë‹´/ì‹ ê³  ê´€ë ¨ì´ë©´ ë”°ë¡œ ëª¨ì•„ë‘ê¸°
                    is_consult = any(
                        re.search(pattern, cleaned, re.IGNORECASE)
                        for pattern in consult_keywords
                    )
                    
                    if is_consult:
                        consult_items.append(cleaned)
                    else:
                        normal_items.append(cleaned)
            
            # ìš°ì„  normal_itemsì—ì„œ ìµœëŒ€ 3ê°œ
            cleaned_items = normal_items[:3]
            
            # normalì´ ë„ˆë¬´ ì ìœ¼ë©´ ìƒë‹´ ê³„ì—´ì—ì„œ 1~2ê°œê¹Œì§€ ë³´ì¶©
            if len(cleaned_items) < 2 and consult_items:
                additional_count = min(2 - len(cleaned_items), len(consult_items))
                cleaned_items.extend(consult_items[:additional_count])
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] ìƒë‹´ ê´€ë ¨ í•­ëª© {additional_count}ê°œ ë³´ì¶© (step: {title})")
            
            # ìµœëŒ€ 3ê°œ í•­ëª©ìœ¼ë¡œ ì œí•œ (ê° stepë³„)
            if len(cleaned_items) > 3:
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•­ëª© ìˆ˜ ì œí•œ: {len(cleaned_items)}ê°œ â†’ 3ê°œ (step: {title})")
                cleaned_items = cleaned_items[:3]
            
            if title or cleaned_items:  # titleì´ ì—†ì–´ë„ itemsê°€ ìˆìœ¼ë©´ ìœ ì§€
                # cleaned_itemsê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if cleaned_items:
                    validated_steps.append({
                        "title": title or "ê¸°íƒ€",
                        "items": cleaned_items,
                    })
        
        # stepsê°€ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë“  stepì˜ itemsê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’
        has_any_items = any(step.get("items", []) for step in validated_steps)
        if not validated_steps or not has_any_items:
            # classified_typeì— ë”°ë¥¸ ê¸°ë³¸ action items
            classified_type = result.get("classified_type", "unknown")
            default_items_by_type = {
                "unpaid_wage": [
                    "ê·¼ë¡œê³„ì•½ì„œì™€ ê¸‰ì—¬ëª…ì„¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”",
                    "ì¶œí‡´ê·¼ ê¸°ë¡ê³¼ ê·¼ë¬´ì‹œê°„ì„ ì •ë¦¬í•˜ì„¸ìš”",
                    "ì„ê¸ˆ ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”"
                ],
                "harassment": [
                    "ê´´ë¡­í˜ ê´€ë ¨ ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                    "ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì‹œë¥¼ ê¸°ë¡í•˜ì„¸ìš”",
                    "ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”"
                ],
                "unfair_dismissal": [
                    "í•´ê³  í†µì§€ì„œì™€ ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”",
                    "ê·¼ë¬´ ê¸°ê°„ê³¼ ì„±ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”",
                    "íšŒì‚¬ì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”"
                ],
                "overtime": [
                    "ê·¼ë¬´ì‹œê°„ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”",
                    "ì—°ì¥ê·¼ë¡œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”",
                    "íœ´ê²Œì‹œê°„ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”"
                ],
                "probation": [
                    "ìˆ˜ìŠµ ê¸°ê°„ê³¼ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”",
                    "ê·¼ë¡œê³„ì•½ì„œì˜ ìˆ˜ìŠµ ì¡°í•­ì„ ê²€í† í•˜ì„¸ìš”",
                    "ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ í‰ê°€ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”"
                ],
                "freelancer": [
                    "ìš©ì—­ ê³„ì•½ì„œì™€ ëŒ€ê¸ˆ ì§€ê¸‰ ì•½ì •ì„ í™•ì¸í•˜ì„¸ìš”",
                    "ì‘ì—… ì™„ë£Œ ì¦ë¹™ ìë£Œë¥¼ ì •ë¦¬í•˜ì„¸ìš”",
                    "ëŒ€ê¸ˆ ì§€ê¸‰ ë‚´ì—­ê³¼ ë¯¸ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”"
                ],
                "stock_option": [
                    "ìŠ¤í†¡ì˜µì…˜/ì„±ê³¼ê¸‰ ì•½ì • ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”",
                    "ê³„ì•½ì„œë‚˜ ì•½ì •ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”",
                    "ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹œê¸°ë¥¼ ì •ë¦¬í•˜ì„¸ìš”"
                ],
                "other": [
                    "ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                    "ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”",
                    "ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”"
                ],
            }
            default_items = default_items_by_type.get(classified_type, [
                "ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                "ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”",
                "ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”"
            ])
            
            validated_steps = [{
                "title": "ì¦‰ì‹œ ì¡°ì¹˜",
                "items": default_items[:3]
            }]
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] ê¸°ë³¸ action_plan ìƒì„± (classified_type: {classified_type})")
        
        action_plan = {"steps": validated_steps}
        
        # 3. scripts ê²€ì¦ (ì´ë©”ì¼ í…œí”Œë¦¿ êµ¬ì¡°: {subject, body})
        scripts = result.get("scripts", {})
        if not isinstance(scripts, dict):
            scripts = {}
        
        # to_company ê²€ì¦
        to_company_raw = scripts.get("to_company", {})
        if isinstance(to_company_raw, str):
            # ë ˆê±°ì‹œ í˜•ì‹ (ë¬¸ìì—´)ì¸ ê²½ìš° ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜
            to_company = {
                "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                "body": to_company_raw[:200] if len(to_company_raw) > 200 else to_company_raw
            }
        elif isinstance(to_company_raw, dict):
            to_company = {
                "subject": to_company_raw.get("subject", "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­"),
                "body": to_company_raw.get("body", "")[:200] if len(to_company_raw.get("body", "")) > 200 else to_company_raw.get("body", "")
            }
        else:
            to_company = {
                "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                "body": ""
            }
        
        # to_advisor ê²€ì¦
        to_advisor_raw = scripts.get("to_advisor", {})
        if isinstance(to_advisor_raw, str):
            # ë ˆê±°ì‹œ í˜•ì‹ (ë¬¸ìì—´)ì¸ ê²½ìš° ê¸°ë³¸ êµ¬ì¡°ë¡œ ë³€í™˜
            to_advisor = {
                "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                "body": to_advisor_raw[:200] if len(to_advisor_raw) > 200 else to_advisor_raw
            }
        elif isinstance(to_advisor_raw, dict):
            to_advisor = {
                "subject": to_advisor_raw.get("subject", "ë…¸ë¬´ ìƒë‹´ ìš”ì²­"),
                "body": to_advisor_raw.get("body", "")[:200] if len(to_advisor_raw.get("body", "")) > 200 else to_advisor_raw.get("body", "")
            }
        else:
            to_advisor = {
                "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                "body": ""
            }
        
        validated_scripts = {
            "to_company": to_company,
            "to_advisor": to_advisor,
        }
        
        result["criteria"] = criteria
        result["action_plan"] = action_plan
        result["scripts"] = validated_scripts
        
        # 3. findings ê²€ì¦ ë° ì •ê·œí™”
        findings = result.get("findings", [])
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] findings ì›ë³¸ ê°œìˆ˜: {len(findings) if isinstance(findings, list) else 0}")
        if findings and isinstance(findings, list):
            validated_findings = []
            for idx, finding in enumerate(findings):
                if not isinstance(finding, dict):
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] finding[{idx}]ì´ dictê°€ ì•„ë‹˜: {type(finding)}")
                    continue
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                id_val = finding.get("id")
                title = finding.get("title", "").strip()
                status_label = finding.get("statusLabel", "").strip()
                basis_text = finding.get("basisText", "").strip()
                source = finding.get("source", {})
                
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] finding[{idx}] ê²€ì¦ ì‹œì‘: title={title[:30] if title else '(ì—†ìŒ)'}, statusLabel={status_label}, basisText ê¸¸ì´={len(basis_text)}")
                
                # source ê²€ì¦ (basisText ë³´ì™„ì„ ìœ„í•´ ë¨¼ì € í™•ì¸)
                if not isinstance(source, dict):
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] finding[{idx}] sourceê°€ dictê°€ ì•„ë‹˜: {type(source)}")
                    source = {}
                
                # source í•„ìˆ˜ í•„ë“œ ê²€ì¦
                document_title = source.get("documentTitle", "").strip()
                source_type = source.get("sourceType", "law").strip()
                refined_snippet = source.get("refinedSnippet", "").strip()
                # refinedSnippetì´ ì—†ìœ¼ë©´ snippetì„ ì‚¬ìš© (fallback)
                if not refined_snippet:
                    refined_snippet = source.get("snippet", "").strip()
                similarity_score = source.get("similarityScore", 0.0)
                
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] finding[{idx}] source í•„ë“œ: documentTitle={document_title[:30] if document_title else '(ì—†ìŒ)'}, refinedSnippet ê¸¸ì´={len(refined_snippet)}")
                
                # documentTitleì´ ì—†ìœ¼ë©´ titleì„ ì‚¬ìš© (fallback)
                if not document_title:
                    document_title = source.get("title", "").strip()
                
                # source í•„ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë©´ ì œì™¸
                if not document_title or not refined_snippet:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] finding[{idx}] source í•„ìˆ˜ í•„ë“œ ëˆ„ë½ìœ¼ë¡œ ì œì™¸: documentTitle={bool(document_title)}, refinedSnippet={bool(refined_snippet)}, finding={finding}")
                    continue
                
                # basisTextì— "{documentTitle}ì— ë”°ë¥´ë©´" í¬í•¨ ì—¬ë¶€ í™•ì¸ ë° ë³´ì™„
                if document_title:
                    # "ì— ë”°ë¥´ë©´" íŒ¨í„´ í™•ì¸
                    if "ì— ë”°ë¥´ë©´" not in basis_text:
                        # basisText ì‹œì‘ ë¶€ë¶„ì— "{documentTitle}ì— ë”°ë¥´ë©´" ì¶”ê°€
                        if basis_text:
                            basis_text = f"{document_title}ì— ë”°ë¥´ë©´, {basis_text}"
                        else:
                            basis_text = f"{document_title}ì— ë”°ë¥´ë©´, ê´€ë ¨ ë²•ì  ê¸°ì¤€ì— ë¶€í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                        logger.debug(f"[ì›Œí¬í”Œë¡œìš°] basisTextì— '{document_title}ì— ë”°ë¥´ë©´' ì¶”ê°€: {basis_text[:100]}...")
                    # "{documentTitle}ì— ë”°ë¥´ë©´"ì´ ìˆì§€ë§Œ documentTitleì´ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° ë³´ì™„
                    elif document_title not in basis_text:
                        # ê¸°ì¡´ "ì— ë”°ë¥´ë©´" ì•ì— documentTitle ì¶”ê°€
                        basis_text = basis_text.replace("ì— ë”°ë¥´ë©´", f"{document_title}ì— ë”°ë¥´ë©´", 1)
                        logger.debug(f"[ì›Œí¬í”Œë¡œìš°] basisTextì— documentTitle ì¶”ê°€: {basis_text[:100]}...")
                
                # í•„ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë©´ ì œì™¸
                if not title or not status_label or not basis_text:
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•„ìˆ˜ í•„ë“œê°€ ì—†ëŠ” finding ì œì™¸: {finding}")
                    continue
                
                validated_findings.append({
                    "id": id_val if id_val is not None else len(validated_findings) + 1,
                    "title": title,
                    "statusLabel": status_label,
                    "basisText": basis_text,
                    "source": {
                        "documentTitle": document_title,
                        "fileUrl": source.get("fileUrl"),  # ì„ íƒì  í•„ë“œ
                        "sourceType": source_type,
                        "refinedSnippet": refined_snippet,
                        "similarityScore": float(similarity_score) if similarity_score else 0.0,
                    }
                })
            
            result["findings"] = validated_findings
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] findings ê²€ì¦ ì™„ë£Œ: {len(validated_findings)}ê°œ (ì›ë³¸ {len(findings)}ê°œ ì¤‘)")
        else:
            result["findings"] = []
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] findingsê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ: findings íƒ€ì…={type(findings)}, ê°’={findings}")
        
        # 4. organizations ê²€ì¦ ë° ì •ê·œí™”
        organizations = result.get("organizations", [])
        if not organizations or len(organizations) == 0:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] organizationsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ organizations ìƒì„±")
            # classified_typeì— ë”°ë¼ ê¸°ë³¸ ê¸°ê´€ ìƒì„±
            classified_type = result.get("classified_type", "unknown")
            default_orgs = {
                "unpaid_wage": ["moel", "labor_attorney", "comwel"],
                "harassment": ["moel_complaint", "human_rights", "labor_attorney"],
                "unfair_dismissal": ["moel", "labor_attorney", "comwel"],
                "overtime": ["moel", "labor_attorney", "comwel"],
                "probation": ["moel", "labor_attorney", "comwel"],
                "freelancer": ["labor_attorney", "moel", "comwel"],
                "stock_option": ["labor_attorney", "moel", "comwel"],
                "other": ["labor_attorney", "moel", "comwel"],
                "unknown": ["labor_attorney", "moel", "comwel"],
            }
            org_ids = default_orgs.get(classified_type, default_orgs["unknown"])
            # ê¸°ë³¸ ê¸°ê´€ ì •ë³´
            org_map = {
                "moel": {
                    "id": "moel",
                    "name": "ë…¸ë™ì²­",
                    "description": "ì²´ë¶ˆì„ê¸ˆ ì¡°ì‚¬ ë° ì‹œì • ëª…ë ¹, ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì¡°ì‚¬",
                    "capabilities": ["ì²´ë¶ˆì„ê¸ˆ ì¡°ì‚¬", "ì‹œì • ëª…ë ¹", "ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì¡°ì‚¬"],
                    "requiredDocs": ["ê·¼ë¡œê³„ì•½ì„œ", "ì¶œí‡´ê·¼ ê¸°ë¡", "ê¸‰ì—¬ëª…ì„¸ì„œ"],
                    "legalBasis": "ê·¼ë¡œê¸°ì¤€ë²• ì œ110ì¡°: ê·¼ë¡œê°ë…ê´€ì˜ ê¶Œí•œ",
                    "website": "https://www.moel.go.kr",
                    "phone": "1350"
                },
                "labor_attorney": {
                    "id": "labor_attorney",
                    "name": "ë…¸ë¬´ì‚¬",
                    "description": "ìƒë‹´ ë° ì†Œì†¡ ëŒ€ë¦¬, ê·¼ë¡œ ë¶„ìŸ í•´ê²° ì „ë¬¸",
                    "capabilities": ["ìƒë‹´", "ì†Œì†¡ ëŒ€ë¦¬", "ê·¼ë¡œ ë¶„ìŸ í•´ê²°"],
                    "requiredDocs": ["ê·¼ë¡œê³„ì•½ì„œ", "ë¬¸ì/ì¹´í†¡ ëŒ€í™”", "ê¸°íƒ€ ì¦ê±° ìë£Œ"],
                    "legalBasis": "ë…¸ë¬´ì‚¬ë²•: ê·¼ë¡œ ë¶„ìŸ ì „ë¬¸ ë²•ë¥  ì„œë¹„ìŠ¤"
                },
                "comwel": {
                    "id": "comwel",
                    "name": "ê·¼ë¡œë³µì§€ê³µë‹¨",
                    "description": "ì—°ì°¨ìˆ˜ë‹¹, íœ´ì¼ìˆ˜ë‹¹, ì‹¤ì—…ê¸‰ì—¬ ìƒë‹´",
                    "capabilities": ["ì—°ì°¨ìˆ˜ë‹¹ ìƒë‹´", "íœ´ì¼ìˆ˜ë‹¹ ìƒë‹´", "ì‹¤ì—…ê¸‰ì—¬ ì•ˆë‚´"],
                    "requiredDocs": ["ê·¼ë¡œê³„ì•½ì„œ", "ì¶œí‡´ê·¼ ê¸°ë¡", "ê¸‰ì—¬ëª…ì„¸ì„œ"],
                    "legalBasis": "ê·¼ë¡œê¸°ì¤€ë²• ì œ60ì¡°: ì—°ì°¨ ìœ ê¸‰íœ´ê°€",
                    "website": "https://www.comwel.or.kr",
                    "phone": "1588-0075"
                },
                "moel_complaint": {
                    "id": "moel_complaint",
                    "name": "ê³ ìš©ë…¸ë™ë¶€ ê³ ê°ìƒë‹´ì„¼í„°",
                    "description": "ì§ì¥ ë‚´ ê´´ë¡­í˜, ì°¨ë³„ ìƒë‹´ ë° ì¡°ì‚¬, ê³ ìš©Â·ë…¸ë™ ì „ë°˜ ìƒë‹´",
                    "capabilities": ["ì§ì¥ ë‚´ ê´´ë¡­í˜ ìƒë‹´", "ì°¨ë³„ ìƒë‹´", "ì¡°ì‚¬ ì§€ì›", "ê³ ìš©Â·ë…¸ë™ ì „ë°˜ ìƒë‹´"],
                    "requiredDocs": ["ì¦ê±° ìë£Œ", "ë¬¸ì/ì¹´í†¡ ëŒ€í™”", "ë…¹ìŒ íŒŒì¼"],
                    "legalBasis": "ì§ì¥ ë‚´ ê´´ë¡­í˜ ë°©ì§€ë²• ì œ13ì¡°: ê³ ì¶© ì²˜ë¦¬",
                    "website": "https://1350.moel.go.kr/home/hp/main/hpmain.do",
                    "phone": "1350"
                },
                "human_rights": {
                    "id": "human_rights",
                    "name": "êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ",
                    "description": "ì¸ê¶Œ ì¹¨í•´ ìƒë‹´ ë° ì¡°ì‚¬, ì°¨ë³„ êµ¬ì œ",
                    "capabilities": ["ì¸ê¶Œ ì¹¨í•´ ìƒë‹´", "ì°¨ë³„ êµ¬ì œ", "ì¡°ì‚¬ ë° êµ¬ì œ"],
                    "requiredDocs": ["ì¦ê±° ìë£Œ", "ì°¨ë³„ ì‚¬ë¡€ ê¸°ë¡"],
                    "legalBasis": "êµ­ê°€ì¸ê¶Œìœ„ì›íšŒë²•: ì¸ê¶Œ ì¹¨í•´ êµ¬ì œ",
                    "website": "https://www.humanrights.go.kr",
                    "phone": "1331"
                }
            }
            organizations = [org_map.get(org_id, {}) for org_id in org_ids if org_id in org_map]
        else:
            # organizations êµ¬ì¡° ê²€ì¦
            validated_orgs = []
            for org in organizations:
                if isinstance(org, dict):
                    validated_orgs.append({
                        "id": org.get("id", ""),
                        "name": org.get("name", ""),
                        "description": org.get("description", ""),
                        "capabilities": org.get("capabilities", []),
                        "requiredDocs": org.get("requiredDocs", []),
                        "legalBasis": org.get("legalBasis"),
                        "website": org.get("website"),
                        "phone": org.get("phone"),
                    })
            organizations = validated_orgs
        
        result["organizations"] = organizations
        
        # 4. summary ê²€ì¦ (4ê°œ ì„¹ì…˜ í™•ì¸)
        summary = result.get("summary", "")
        if not isinstance(summary, str):
            summary = ""
        
        # summaryì— 4ê°œ ì„¹ì…˜ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸ (ìœ ì—°í•œ ë§¤ì¹­)
        # ìƒˆë¡œìš´ í˜•ì‹: ğŸ“Š ìƒí™© ë¶„ì„, âš–ï¸ ë²•ì  íŒë‹¨, ğŸ”® ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤, ğŸ’¡ ì£¼ì˜ì‚¬í•­
        section_patterns = [
            {
                "title": "ğŸ“Š **ìƒí™© ë¶„ì„**:",
                "keywords": ["ğŸ“Š", "ìƒí™© ë¶„ì„", "ìƒí™© ë¶„ì„ì˜ ê²°ê³¼"],
                "emoji": "ğŸ“Š"
            },
            {
                "title": "âš–ï¸ **ë²•ì  íŒë‹¨**:",
                "keywords": ["âš–ï¸", "ë²•ì  íŒë‹¨", "ë²•ì  ê´€ì ", "ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ìƒí™©"],
                "emoji": "âš–ï¸"
            },
            {
                "title": "ğŸ”® **ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤**:",
                "keywords": ["ğŸ”®", "ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤", "ì˜ˆìƒ", "ì‹œë‚˜ë¦¬ì˜¤"],
                "emoji": "ğŸ”®"
            },
            {
                "title": "ğŸ’¡ **ì£¼ì˜ì‚¬í•­**:",
                "keywords": ["ğŸ’¡", "ì£¼ì˜ì‚¬í•­", "ì£¼ì˜", "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”", "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™"],
                "emoji": "ğŸ’¡"
            },
        ]
        
        # ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìœ ì—°í•œ ë§¤ì¹­)
        found_sections = []
        missing_sections = []
        
        for section_info in section_patterns:
            found = False
            # ì •í™•í•œ í—¤ë” í˜•ì‹ í™•ì¸ (ìƒˆë¡œìš´ í˜•ì‹: ì´ëª¨ì§€ + êµµì€ ì œëª©)
            if section_info["title"] in summary:
                found = True
            else:
                # ì´ëª¨ì§€ë¡œ ë¨¼ì € í™•ì¸
                emoji = section_info.get("emoji")
                if emoji and emoji in summary:
                    # ì´ëª¨ì§€ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
                    emoji_pos = summary.find(emoji)
                    start = max(0, emoji_pos - 20)
                    end = min(len(summary), emoji_pos + 50)
                    context = summary[start:end]
                    
                    # ì´ëª¨ì§€ ë’¤ì— "ìƒí™© ë¶„ì„", "ë²•ì  íŒë‹¨" ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                    for keyword in section_info["keywords"]:
                        if keyword in context and keyword != emoji:
                            found = True
                            break
                
                # ì´ëª¨ì§€ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° í‚¤ì›Œë“œë¡œ í™•ì¸ (ë ˆê±°ì‹œ í˜•ì‹ ì§€ì›)
                if not found:
                    for keyword in section_info["keywords"]:
                        if keyword == emoji:
                            continue  # ì´ë¯¸ ì´ëª¨ì§€ë¡œ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
                        keyword_lower = keyword.lower()
                        summary_lower = summary.lower()
                        
                        # í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if keyword_lower in summary_lower:
                            # í‚¤ì›Œë“œ ì£¼ë³€ì˜ ì»¨í…ìŠ¤íŠ¸ í™•ì¸ (í—¤ë” í˜•ì‹ì¸ì§€)
                            keyword_pos = summary_lower.find(keyword_lower)
                            # í‚¤ì›Œë“œ ì•ë’¤ë¡œ ìµœëŒ€ 100ì í™•ì¸
                            start = max(0, keyword_pos - 50)
                            end = min(len(summary), keyword_pos + len(keyword) + 50)
                            context = summary[start:end]
                            
                            # í—¤ë” í˜•ì‹(## ë˜ëŠ” #)ì´ ìˆê±°ë‚˜, í‚¤ì›Œë“œê°€ ì¤„ì˜ ì‹œì‘ ë¶€ë¶„ì— ìˆìœ¼ë©´ ì„¹ì…˜ìœ¼ë¡œ ì¸ì‹
                            has_header_marker = re.search(r'##?\s*', context, re.IGNORECASE) is not None
                            is_line_start = keyword_pos == 0 or summary[keyword_pos - 1] == '\n'
                            
                            if has_header_marker or is_line_start:
                                found = True
                                break
            
            if found:
                found_sections.append(section_info["title"])
            else:
                missing_sections.append(section_info)
        
        if missing_sections:
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summaryì— ëˆ„ë½ëœ ì„¹ì…˜: {[s['title'] for s in missing_sections]}")
            
            # LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ ì„¹ì…˜ ì¬êµ¬ì„± ì‹œë„
            classified_type = result.get("classified_type", "unknown")
            summary_reconstructed = self._reconstruct_summary_sections(summary, section_patterns, classified_type)
            if summary_reconstructed:
                summary = summary_reconstructed
                logger.info("[ì›Œí¬í”Œë¡œìš°] summary ì„¹ì…˜ ì¬êµ¬ì„± ì™„ë£Œ")
            else:
                # ì¬êµ¬ì„± ì‹¤íŒ¨ ì‹œ ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ê°’ ì œê³µ)
                classified_type = result.get("classified_type", "unknown")
                default_content_by_type = {
                    "unpaid_wage": {
                        "ë²•ì  ê´€ì ": "ì„ê¸ˆì²´ë¶ˆì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ43ì¡°(ì„ê¸ˆì§€ê¸‰), ì œ36ì¡°(ì„ê¸ˆì˜ ì§€ê¸‰)ì™€ ê´€ë ¨ëœ ì‚¬ì•ˆì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ê·¼ë¡œìì—ê²Œ ì„ê¸ˆì„ ì •ê¸°ì ìœ¼ë¡œ ì§€ê¸‰í•  ì˜ë¬´ê°€ ìˆìœ¼ë©°, ì´ë¥¼ ìœ„ë°˜í•  ê²½ìš° í˜•ì‚¬ì²˜ë²Œê³¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬´ê¸‰ ì•¼ê·¼ë„ ì—°ì¥ê·¼ë¡œ ìˆ˜ë‹¹ ë¯¸ì§€ê¸‰ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ê·¼ë¡œê³„ì•½ì„œì™€ ê¸‰ì—¬ëª…ì„¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”\n- ì¶œí‡´ê·¼ ê¸°ë¡ê³¼ ê·¼ë¬´ì‹œê°„ì„ ì •ë¦¬í•˜ì„¸ìš”\n- ì„ê¸ˆ ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ì •ì¤‘í•˜ê²Œ ì„ê¸ˆ ì§€ê¸‰ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "harassment": {
                        "ë²•ì  ê´€ì ": "ì§ì¥ ë‚´ ê´´ë¡­í˜ì€ ì§ì¥ ë‚´ ê´´ë¡­í˜ ë°©ì§€ ë° ê·¼ë¡œì ë³´í˜¸ ë“±ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¼ ê¸ˆì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—…ë¬´ìƒ ì§€ìœ„ë‚˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ ê·¼ë¡œìì—ê²Œ ì‹ ì²´ì Â·ì •ì‹ ì  ê³ í†µì„ ì£¼ëŠ” í–‰ìœ„ëŠ” ë²•ì  ì²˜ë²Œ ëŒ€ìƒì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ê´´ë¡­í˜ ê´€ë ¨ ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”\n- ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì‹œë¥¼ ê¸°ë¡í•˜ì„¸ìš”\n- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ê´´ë¡­í˜ ìƒí™©ì„ ì •ì¤‘í•˜ê²Œ ì•Œë¦¬ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "unfair_dismissal": {
                        "ë²•ì  ê´€ì ": "ë¶€ë‹¹í•´ê³ ëŠ” ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°(í•´ê³ ì˜ ì œí•œ)ì— ë”°ë¼ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ í•´ê³ í•˜ëŠ” ê²½ìš° ë³µì§ ì²­êµ¬ë‚˜ ì†í•´ë°°ìƒ ì²­êµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- í•´ê³  í†µì§€ì„œì™€ ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ê·¼ë¬´ ê¸°ê°„ê³¼ ì„±ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”\n- íšŒì‚¬ì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— í•´ê³  ì‚¬ìœ ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "overtime": {
                        "ë²•ì  ê´€ì ": "ê·¼ë¡œì‹œê°„ì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ50ì¡°(ê·¼ë¡œì‹œê°„), ì œ53ì¡°(ì—°ì¥ê·¼ë¡œ)ì— ë”°ë¼ ê·œì œë©ë‹ˆë‹¤. ë²•ì • ê·¼ë¡œì‹œê°„ì„ ì´ˆê³¼í•˜ëŠ” ì—°ì¥ê·¼ë¡œì— ëŒ€í•´ì„œëŠ” ê°€ì‚°ì„ê¸ˆì„ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ê·¼ë¬´ì‹œê°„ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”\n- ì—°ì¥ê·¼ë¡œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”\n- íœ´ê²Œì‹œê°„ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ê·¼ë¡œì‹œê°„ê³¼ ê°€ì‚°ì„ê¸ˆì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "probation": {
                        "ë²•ì  ê´€ì ": "ìˆ˜ìŠµê¸°ê°„ì€ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¸ì •ë©ë‹ˆë‹¤. ìˆ˜ìŠµê¸°ê°„ ì¤‘ì—ë„ ê·¼ë¡œê¸°ì¤€ë²•ìƒ ë³´í˜¸ë¥¼ ë°›ìœ¼ë©°, ë¶€ë‹¹í•œ í•´ê³ ëŠ” ì œí•œë©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìˆ˜ìŠµ ê¸°ê°„ê³¼ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”\n- ê·¼ë¡œê³„ì•½ì„œì˜ ìˆ˜ìŠµ ì¡°í•­ì„ ê²€í† í•˜ì„¸ìš”\n- ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ í‰ê°€ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ìˆ˜ìŠµê¸°ê°„ê³¼ í‰ê°€ ê¸°ì¤€ì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "freelancer": {
                        "ë²•ì  ê´€ì ": "í”„ë¦¬ëœì„œ/ìš©ì—­ ê³„ì•½ì—ì„œ ëŒ€ê¸ˆ ë¯¸ì§€ê¸‰ì€ ë¯¼ë²•ìƒ ì±„ë¬´ë¶ˆì´í–‰ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì•½ì„œì— ëª…ì‹œëœ ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹¤ì œ ì§€ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìš©ì—­ ê³„ì•½ì„œì™€ ëŒ€ê¸ˆ ì§€ê¸‰ ì•½ì •ì„ í™•ì¸í•˜ì„¸ìš”\n- ì‘ì—… ì™„ë£Œ ì¦ë¹™ ìë£Œë¥¼ ì •ë¦¬í•˜ì„¸ìš”\n- ëŒ€ê¸ˆ ì§€ê¸‰ ë‚´ì—­ê³¼ ë¯¸ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "ë°œì£¼ì‚¬ì— ëŒ€ê¸ˆ ì§€ê¸‰ì„ ìš”ì²­í•˜ëŠ” ì •ì¤‘í•œ ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "stock_option": {
                        "ë²•ì  ê´€ì ": "ìŠ¤í†¡ì˜µì…˜ì´ë‚˜ ì„±ê³¼ê¸‰ì€ ê³„ì•½ì„œë‚˜ ì•½ì •ì„œì— ëª…ì‹œëœ ì¡°ê±´ì— ë”°ë¼ ì§€ê¸‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. êµ¬ë‘ ì•½ì†ë§Œìœ¼ë¡œëŠ” ë²•ì  êµ¬ì†ë ¥ì´ ì•½í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ì„œí™”ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìŠ¤í†¡ì˜µì…˜/ì„±ê³¼ê¸‰ ì•½ì • ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”\n- ê³„ì•½ì„œë‚˜ ì•½ì •ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹œê¸°ë¥¼ ì •ë¦¬í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ì— ìŠ¤í†¡ì˜µì…˜/ì„±ê³¼ê¸‰ ì§€ê¸‰ ì¡°ê±´ê³¼ ì‹œê¸°ì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "other": {
                        "ë²•ì  ê´€ì ": "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”\n- ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                        "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ë‚˜ ìƒë‹´ ê¸°ê´€ì— ìƒí™©ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                }
                
                default_content = default_content_by_type.get(classified_type, {
                    "ë²•ì  ê´€ì ": "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
                    "ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™": "- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”\n- ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                    "ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”": "íšŒì‚¬ë‚˜ ìƒë‹´ ê¸°ê´€ì— ìƒí™©ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                })
                
                # ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€
                for section_info in missing_sections:
                    section_key = section_info.get("keywords", [""])[0] if section_info.get("keywords") else ""
                    
                    # ì„¹ì…˜ í‚¤ì›Œë“œë¡œ ë§¤ì¹­
                    section_key_matched = None
                    for key in default_content.keys():
                        if any(keyword in section_key for keyword in section_info.get("keywords", [])):
                            section_key_matched = key
                            break
                    
                    default_text = default_content.get(section_key_matched or section_key, "í•´ë‹¹ ì„¹ì…˜ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
                    # ê¸°ë³¸ê°’ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ì„¹ì…˜ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                    if default_text and default_text != "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤." and default_text != "í•´ë‹¹ ì„¹ì…˜ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.":
                        summary += f"\n\n{section_info['title']}\n\n{default_text}"
        
        return {
            "summary": summary,
            "criteria": criteria,
            "action_plan": action_plan,
            "scripts": validated_scripts,
            "organizations": organizations,
        }
    
    async def _llm_generate_summary(
        self,
        situation_text: str,
        classification: Dict[str, Any],
        grounding_chunks: List[LegalGroundingChunk],
        legal_basis: List[Dict[str, Any]],
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> str:
        """summaryë§Œ ìƒì„± (ë¶„ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] _llm_generate_summary ì‹œì‘")
        prompt = build_situation_summary_prompt(
            situation_text=situation_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        
        try:
            response = await self._call_llm(prompt)
            
            # í”„ë¡¬í”„íŠ¸ëŠ” ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ë„ë¡ ì§€ì‹œí•˜ë¯€ë¡œ, JSON íŒŒì‹± ì—†ì´ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if response:
                # ì½”ë“œ ë¸”ë¡ ì œê±° (```markdown, ``` ë“±)
                import re
                response_clean = response.strip()
                
                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                if response_clean.startswith("```markdown"):
                    response_clean = response_clean[11:]  # ```markdown ì œê±°
                elif response_clean.startswith("```"):
                    response_clean = response_clean[3:]  # ``` ì œê±°
                
                if response_clean.endswith("```"):
                    response_clean = response_clean[:-3]  # ëì˜ ``` ì œê±°
                
                response_clean = response_clean.strip()
                
                # JSON í˜•ì‹ìœ¼ë¡œ ì˜ëª» ë°˜í™˜ëœ ê²½ìš° ì²˜ë¦¬ (í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš° ëŒ€ë¹„)
                json_match = re.search(r'\{.*"summary".*\}', response_clean, re.DOTALL)
                if json_match:
                    try:
                        import json
                        result = json.loads(json_match.group())
                        summary = result.get('summary', '')
                        if summary:
                            logger.info(f"[ì›Œí¬í”Œë¡œìš°] summary JSONì—ì„œ ì¶”ì¶œ ì„±ê³µ - ê¸¸ì´: {len(summary)}ì")
                            return summary
                    except json.JSONDecodeError:
                        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‘ë‹µ ì‚¬ìš©
                        pass
                
                # ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ê¸°ë³¸ ì¼€ì´ìŠ¤)
                if response_clean:
                    # 4ê°œ ì„¹ì…˜ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
                    has_situation = "ğŸ“Š" in response_clean or "ìƒí™© ë¶„ì„" in response_clean
                    has_legal = "âš–ï¸" in response_clean or "ë²•ì  íŒë‹¨" in response_clean or "ë²•ì  ê´€ì " in response_clean
                    has_scenario = "ğŸ”®" in response_clean or "ì˜ˆìƒ ì‹œë‚˜ë¦¬ì˜¤" in response_clean
                    has_warning = "ğŸ’¡" in response_clean or "ì£¼ì˜ì‚¬í•­" in response_clean
                    
                    if has_situation and has_legal and (has_scenario or has_warning):
                        logger.info(f"[ì›Œí¬í”Œë¡œìš°] summary ìƒì„± ì„±ê³µ (ë§ˆí¬ë‹¤ìš´) - ê¸¸ì´: {len(response_clean)}ì")
                        return response_clean
                    else:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summaryì— í•„ìˆ˜ ì„¹ì…˜ì´ ëˆ„ë½ë¨ - ìƒí™©: {has_situation}, ë²•ì : {has_legal}, ì‹œë‚˜ë¦¬ì˜¤: {has_scenario}, ì£¼ì˜: {has_warning}")
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ì‘ë‹µ (ì²˜ìŒ 500ì): {response_clean[:500]}")
                else:
                    logger.warning("[ì›Œí¬í”Œë¡œìš°] summary ì‘ë‹µì´ ë¹„ì–´ìˆìŒ")
            else:
                logger.warning("[ì›Œí¬í”Œë¡œìš°] LLM ì‘ë‹µì´ None")
                
        except Exception as e:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] summary LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}", exc_info=True)
            # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ summary ë°˜í™˜ (4ê°œ ì„¹ì…˜ êµ¬ì¡° ìœ ì§€)
            return "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\nìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²•ì  ê´€ì ê³¼ í–‰ë™ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ ìƒí™©\n\nê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ summary ë°˜í™˜
        logger.warning("[ì›Œí¬í”Œë¡œìš°] summary ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
        return "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\nìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²•ì  ê´€ì ê³¼ í–‰ë™ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ ìƒí™©\n\nê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    
    async def _llm_generate_findings(
        self,
        situation_text: str,
        classification: Dict[str, Any],
        grounding_chunks: List[LegalGroundingChunk],
        legal_basis: List[Dict[str, Any]],
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """findingsë§Œ ìƒì„± (ë¶„ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] _llm_generate_findings ì‹œì‘")
        prompt = build_situation_findings_prompt(
            situation_text=situation_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        response = await self._call_llm(prompt)
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] findings LLM ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
        logger.debug(f"[ì›Œí¬í”Œë¡œìš°] findings LLM ì‘ë‹µ (ì²˜ìŒ 500ì): {response[:500]}")
        
        try:
            import json
            import re
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” {...} íŒ¨í„´)
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
            if not json_match:
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1) if json_match.lastindex else json_match.group()
                # JSON ë¬¸ìì—´ ì •ë¦¬
                json_str = json_str.strip()
                result = json.loads(json_str)
                findings = result.get('findings', [])
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] findings íŒŒì‹± ì„±ê³µ: {len(findings)}ê°œ")
                if findings:
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] ì²« ë²ˆì§¸ finding ì˜ˆì‹œ: {findings[0] if len(findings) > 0 else 'ì—†ìŒ'}")
                    # documentTitle í™•ì¸ (source ì—†ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸)
                    for idx, finding in enumerate(findings):
                        if isinstance(finding, dict) and "documentTitle" not in finding:
                            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] finding[{idx}]ì— documentTitleì´ ì—†ìŒ: {finding.get('title', 'unknown')}")
                return findings
            else:
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] findings JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‘ë‹µ: {response[:200]}")
        except json.JSONDecodeError as e:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] findings JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ì‘ë‹µ (ì²˜ìŒ 500ì): {response[:500]}")
        except Exception as e:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] findings ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
        return []
    
    async def _llm_generate_scripts(
        self,
        situation_text: str,
        classification: Dict[str, Any],
        grounding_chunks: List[LegalGroundingChunk],
        legal_basis: List[Dict[str, Any]],
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> Dict[str, Any]:
        """scriptsë§Œ ìƒì„± (ë¶„ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] _llm_generate_scripts ì‹œì‘")
        prompt = build_situation_scripts_prompt(
            situation_text=situation_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        response = await self._call_llm(prompt)
        try:
            import json
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result.get('scripts', {})
        except Exception as e:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] scripts JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {}
    
    async def _llm_generate_organizations(
        self,
        situation_text: str,
        classification: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        """organizationsë§Œ ìƒì„± (ë¶„ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] _llm_generate_organizations ì‹œì‘")
        prompt = build_situation_organizations_prompt(
            situation_text=situation_text,
            classification=classification,
        )
        response = await self._call_llm(prompt)
        try:
            import json
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return result.get('organizations', [])
        except Exception as e:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] organizations JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []
    
    async def _call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ (Groq/Ollama) - íƒ€ì„ì•„ì›ƒ ë° ë¡œê¹… í¬í•¨"""
        from config import settings
        
        # í”„ë¡¬í”„íŠ¸ ì •ë³´ ë¡œê¹…
        prompt_length = len(prompt)
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM í˜¸ì¶œ ì‹œì‘ - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length}ì")
        
        if settings.use_groq:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] Groq ì‚¬ìš© (ëª¨ë¸: {settings.groq_model})")
            from llm_api import ask_groq_with_messages
            messages = [
                {"role": "system", "content": "ë„ˆëŠ” ìœ ëŠ¥í•œ ë²•ë¥  AIì•¼. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ]
            try:
                # GroqëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¹ ë¥´ë¯€ë¡œ íƒ€ì„ì•„ì›ƒ 2ë¶„
                response = await asyncio.wait_for(
                    asyncio.to_thread(
                        ask_groq_with_messages,
                        messages=messages,
                        temperature=settings.llm_temperature,
                        model=settings.groq_model
                    ),
                    timeout=120.0  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] Groq ì‘ë‹µ ì™„ë£Œ - ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
                return response
            except asyncio.TimeoutError:
                logger.error("[ì›Œí¬í”Œë¡œìš°] Groq í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (2ë¶„ ì´ˆê³¼)")
                raise TimeoutError("Groq LLM í˜¸ì¶œì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤ (2ë¶„ ì´ˆê³¼)")
        elif settings.use_ollama:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] Ollama ì‚¬ìš© (ëª¨ë¸: {settings.ollama_model}, URL: {settings.ollama_base_url})")
            
            # Ollama ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
            async def check_ollama_model():
                """Ollama ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
                try:
                    import httpx
                    async with httpx.AsyncClient(timeout=5.0) as client:
                        response = await client.get(f"{settings.ollama_base_url}/api/tags")
                        if response.status_code == 200:
                            models_data = response.json()
                            available_models = [model.get("name", "") for model in models_data.get("models", [])]
                            # ëª¨ë¸ ì´ë¦„ì—ì„œ íƒœê·¸ ì œê±° (ì˜ˆ: "mistral:latest" -> "mistral")
                            available_model_names = [name.split(":")[0] for name in available_models]
                            
                            if settings.ollama_model not in available_model_names:
                                error_msg = (
                                    f"Ollama ëª¨ë¸ '{settings.ollama_model}'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                                    f"ì„¤ì¹˜ëœ ëª¨ë¸: {', '.join(available_model_names) if available_model_names else '(ì—†ìŒ)'}\n"
                                    f"ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:\n"
                                    f"  ollama pull {settings.ollama_model}\n"
                                    f"ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì—ì„œ OLLAMA_MODELì„ ë³€ê²½í•˜ì„¸ìš”."
                                )
                                logger.error(f"[ì›Œí¬í”Œë¡œìš°] {error_msg}")
                                raise ValueError(error_msg)
                            else:
                                logger.info(f"[ì›Œí¬í”Œë¡œìš°] Ollama ëª¨ë¸ í™•ì¸ ì™„ë£Œ: {settings.ollama_model}")
                        else:
                            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ (HTTP {response.status_code}), ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                except ImportError:
                    # httpxê°€ ì—†ìœ¼ë©´ requestsë¡œ ì‹œë„
                    try:
                        import requests
                        response = requests.get(f"{settings.ollama_base_url}/api/tags", timeout=5.0)
                        if response.status_code == 200:
                            models_data = response.json()
                            available_models = [model.get("name", "") for model in models_data.get("models", [])]
                            available_model_names = [name.split(":")[0] for name in available_models]
                            
                            if settings.ollama_model not in available_model_names:
                                error_msg = (
                                    f"Ollama ëª¨ë¸ '{settings.ollama_model}'ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                                    f"ì„¤ì¹˜ëœ ëª¨ë¸: {', '.join(available_model_names) if available_model_names else '(ì—†ìŒ)'}\n"
                                    f"ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:\n"
                                    f"  ollama pull {settings.ollama_model}\n"
                                    f"ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì—ì„œ OLLAMA_MODELì„ ë³€ê²½í•˜ì„¸ìš”."
                                )
                                logger.error(f"[ì›Œí¬í”Œë¡œìš°] {error_msg}")
                                raise ValueError(error_msg)
                            else:
                                logger.info(f"[ì›Œí¬í”Œë¡œìš°] Ollama ëª¨ë¸ í™•ì¸ ì™„ë£Œ: {settings.ollama_model}")
                        else:
                            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨ (HTTP {response.status_code}), ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                    except ImportError:
                        logger.warning("[ì›Œí¬í”Œë¡œìš°] httpx/requestsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ëª¨ë¸ í™•ì¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                except Exception as e:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ëª¨ë¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            
            # ëª¨ë¸ í™•ì¸ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ 5ì´ˆ)
            try:
                await asyncio.wait_for(check_ollama_model(), timeout=5.0)
            except asyncio.TimeoutError:
                logger.warning("[ì›Œí¬í”Œë¡œìš°] ëª¨ë¸ í™•ì¸ íƒ€ì„ì•„ì›ƒ (5ì´ˆ), ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            except ValueError:
                # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ ì¤‘ë‹¨
                raise
            except Exception as e:
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {str(e)}, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            
            # Ollama ì‚¬ìš© - ë¹„ë™ê¸° ì²˜ë¦¬ ë° íƒ€ì„ì•„ì›ƒ ì¶”ê°€
            # langchain-communityë¥¼ ìš°ì„  ì‚¬ìš© (think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ë°©ì§€)
            try:
                from langchain_community.llms import Ollama
                llm = Ollama(
                    base_url=settings.ollama_base_url,
                    model=settings.ollama_model
                )
            except ImportError:
                # ëŒ€ì•ˆ: langchain-ollama ì‚¬ìš© (think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ê°€ëŠ¥)
                try:
                    from langchain_ollama import OllamaLLM
                    llm = OllamaLLM(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                except Exception as e:
                    if "think" in str(e).lower():
                        logger.warning("[ì›Œí¬í”Œë¡œìš°] langchain-ollamaì—ì„œ think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ë°œìƒ. langchain-communityë¡œ ì¬ì‹œë„...")
                        from langchain_community.llms import Ollama
                        llm = Ollama(
                            base_url=settings.ollama_base_url,
                            model=settings.ollama_model
                        )
                    else:
                        raise
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…ì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬
            async def log_progress():
                """ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ë¡œê¹…"""
                elapsed = 0
                while elapsed < 600:  # ìµœëŒ€ 10ë¶„ê¹Œì§€
                    await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤
                    elapsed += 30
                    logger.info(f"[ì›Œí¬í”Œë¡œìš°] Ollama ì‘ë‹µ ëŒ€ê¸° ì¤‘... ({elapsed}ì´ˆ ê²½ê³¼)")
            
            progress_task = asyncio.create_task(log_progress())
            
            try:
                # Ollama í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ê³  íƒ€ì„ì•„ì›ƒ ì„¤ì • (10ë¶„)
                logger.info("[ì›Œí¬í”Œë¡œìš°] Ollama LLM í˜¸ì¶œ ì‹œì‘...")
                # ëŒ€ëµì ì¸ ì…ë ¥ í† í° ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€: 1í† í° â‰ˆ 2-3ì)
                estimated_input_tokens = len(prompt) // 2.5
                logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì…ë ¥ ì¶”ì •: ì•½ {int(estimated_input_tokens)}í† í° (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì)")
                
                response_text = await asyncio.wait_for(
                    asyncio.to_thread(llm.invoke, prompt),
                    timeout=600.0  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                progress_task.cancel()  # ì„±ê³µ ì‹œ ì§„í–‰ ë¡œê¹… ì¤‘ì§€
                
                # ëŒ€ëµì ì¸ ì¶œë ¥ í† í° ì¶”ì •
                estimated_output_tokens = len(response_text) // 2.5
                estimated_total_tokens = int(estimated_input_tokens) + int(estimated_output_tokens)
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] Ollama ì‘ë‹µ ì™„ë£Œ - ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
                logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì¶œë ¥ ì¶”ì •: ì•½ {int(estimated_output_tokens)}í† í°, ì´ ì¶”ì •: ì•½ {estimated_total_tokens}í† í° (ëª¨ë¸: {settings.ollama_model})")
                return response_text
            except asyncio.TimeoutError:
                progress_task.cancel()
                logger.error("[ì›Œí¬í”Œë¡œìš°] Ollama í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (10ë¶„ ì´ˆê³¼)")
                raise TimeoutError("Ollama LLM í˜¸ì¶œì´ íƒ€ì„ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤ (10ë¶„ ì´ˆê³¼). ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
            except Exception as e:
                progress_task.cancel()
                logger.error(f"[ì›Œí¬í”Œë¡œìš°] Ollama í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}", exc_info=True)
                raise
        else:
            # Groqì™€ Ollama ëª¨ë‘ ì‚¬ìš© ì•ˆ í•¨
            raise ValueError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM_PROVIDER í™˜ê²½ë³€ìˆ˜ë¥¼ 'groq' ë˜ëŠ” 'ollama'ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    
    # ==================== ê³µê°œ ë©”ì„œë“œ ====================
    
    async def run(self, initial_state: Dict[str, Any]) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] ì‹¤í–‰ ì‹œì‘")
        
        # Stateë¡œ ë³€í™˜
        state: SituationWorkflowState = {
            "situation_text": initial_state.get("situation_text", ""),
            "category_hint": initial_state.get("category_hint"),
            "summary": initial_state.get("summary"),
            "details": initial_state.get("details"),
            "employment_type": initial_state.get("employment_type"),
            "work_period": initial_state.get("work_period"),
            "weekly_hours": initial_state.get("weekly_hours"),
            "is_probation": initial_state.get("is_probation"),
            "social_insurance": initial_state.get("social_insurance"),
            "query_text": None,
            "query_embedding": None,
            "classification": None,
            "filtered_categories": None,
            "grounding_chunks": None,
            "related_cases": None,
            "legal_basis": None,
            "action_plan": None,
            "scripts": None,
            "criteria": None,
            "summary_report": None,
            "final_output": None,
        }
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        final_state = await self.graph.ainvoke(state)
        
        # ìµœì¢… ì¶œë ¥ ë°˜í™˜
        return final_state.get("final_output", {})

