"""
Legal RAG Service - ë²•ë¥  ë„ë©”ì¸ RAG ì„œë¹„ìŠ¤
ê³„ì•½ì„œ ë¶„ì„, ìƒí™© ë¶„ì„, ì¼€ì´ìŠ¤ ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""

from typing import List, Optional, OrderedDict, Dict, Any
from pathlib import Path
from collections import OrderedDict as OrderedDictType
import asyncio
import logging
import json
import re
import warnings

# langchain-communityì˜ Ollama Deprecated ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=DeprecationWarning, module="langchain")

from models.schemas import (
    LegalAnalysisResult,
    LegalIssue,
    LegalRecommendation,
    LegalGroundingChunk,
    LegalCasePreview,
)
from core.supabase_vector_store import SupabaseVectorStore
from core.generator_v2 import LLMGenerator
from core.document_processor_v2 import DocumentProcessor
from core.prompts import (
    build_legal_chat_prompt,
    build_situation_chat_prompt,
    build_contract_analysis_prompt,
    build_situation_analysis_prompt,
    LEGAL_CHAT_SYSTEM_PROMPT,
)


LEGAL_BASE_PATH = Path(__file__).resolve().parent.parent / "data" / "legal"

logger = logging.getLogger(__name__)


class LRUEmbeddingCache:
    """
    LRU (Least Recently Used) ìºì‹œë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìºì‹œ
    ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì œí•œí•˜ê¸° ìœ„í•´ ìµœëŒ€ í¬ê¸°ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŒ
    """
    
    def __init__(self, max_size: int = 100):
        """
        Args:
            max_size: ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜ (ê¸°ë³¸ê°’: 100)
        """
        self.max_size = max_size
        self._cache: OrderedDictType[str, List[float]] = OrderedDictType()
    
    def get(self, key: str) -> Optional[List[float]]:
        """ìºì‹œì—ì„œ ê°’ì„ ê°€ì ¸ì˜¤ê³ , ì‚¬ìš©ëœ í•­ëª©ì„ ìµœì‹ ìœ¼ë¡œ ì´ë™"""
        if key in self._cache:
            # OrderedDictì—ì„œ í•­ëª©ì„ ì œê±°í•˜ê³  ë‹¤ì‹œ ì¶”ê°€í•˜ì—¬ ìµœì‹ ìœ¼ë¡œ ì´ë™
            value = self._cache.pop(key)
            self._cache[key] = value
            return value
        return None
    
    def put(self, key: str, value: List[float]) -> None:
        """ìºì‹œì— ê°’ì„ ì €ì¥í•˜ê³ , í¬ê¸° ì œí•œì„ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if key in self._cache:
            # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì œê±°í•˜ê³  ë‹¤ì‹œ ì¶”ê°€ (ìµœì‹ ìœ¼ë¡œ ì´ë™)
            self._cache.pop(key)
        elif len(self._cache) >= self.max_size:
            # ìºì‹œê°€ ê°€ë“ ì°¨ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (FIFO)
            self._cache.popitem(last=False)  # last=False: ê°€ì¥ ì˜¤ë˜ëœ í•­ëª©
        
        self._cache[key] = value
    
    def clear(self) -> None:
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        self._cache.clear()
    
    def size(self) -> int:
        """í˜„ì¬ ìºì‹œ í¬ê¸° ë°˜í™˜"""
        return len(self._cache)
    
    def __contains__(self, key: str) -> bool:
        """ìºì‹œì— í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        return key in self._cache


class LegalRAGService:
    """
    ë²•ë¥  ë„ë©”ì¸ RAG ì„œë¹„ìŠ¤.
    - laws/: ìš”ì•½ ë²•ë ¹/ì²´í¬ë¦¬ìŠ¤íŠ¸
    - manuals/: ê³„ì•½/ë…¸ë™ ê°€ì´ë“œ
    - cases/: ìš°ë¦¬ê°€ ë§Œë“  ì‹œë‚˜ë¦¬ì˜¤ md íŒŒì¼
    """

    def __init__(self, embedding_cache_size: int = 100):
        """
        ë²¡í„°ìŠ¤í† ì–´/ì„ë² ë”©/LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            embedding_cache_size: ì„ë² ë”© ìºì‹œ ìµœëŒ€ í¬ê¸° (ê¸°ë³¸ê°’: 100)
        """
        self.vector_store = SupabaseVectorStore()
        self.generator = LLMGenerator()
        self.processor = DocumentProcessor()
        # LRU ìºì‹œë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìºì‹œ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ)
        self._embedding_cache = LRUEmbeddingCache(max_size=embedding_cache_size)

    # 1) ê³„ì•½ì„œ + ìƒí™© ì„¤ëª… ê¸°ë°˜ ë¶„ì„
    async def analyze_contract(
        self,
        extracted_text: str,
        description: Optional[str] = None,
        doc_id: Optional[str] = None,
        clauses: Optional[List[Dict]] = None,
        contract_type: Optional[str] = None,
        user_role: Optional[str] = None,
        field: Optional[str] = None,
    ) -> LegalAnalysisResult:
        """
        ê³„ì•½ì„œ ë¶„ì„ (Dual RAG ì§€ì›)
        
        - extracted_text: ì—…ë¡œë“œëœ ê³„ì•½ì„œ OCR/íŒŒì‹± ê²°ê³¼ í…ìŠ¤íŠ¸
        - description: ì‚¬ìš©ìê°€ ë§ë¶™ì¸ ìƒí™© ì„¤ëª…
        - doc_id: ê³„ì•½ì„œ ID (ìˆìœ¼ë©´ contract_chunksë„ ê²€ìƒ‰)
        """
        # 1. ì¿¼ë¦¬ ë¬¸ì¥ êµ¬ì„±
        query = self._build_query_from_contract(extracted_text, description)

        # 2. Dual RAG ê²€ìƒ‰: ê³„ì•½ì„œ ë‚´ë¶€ + ì™¸ë¶€ ë²•ë ¹
        contract_chunks = []
        legal_chunks = []
        
        # 2-1. ê³„ì•½ì„œ ë‚´ë¶€ ê²€ìƒ‰ (doc_idê°€ ìˆê³  contract_chunksê°€ ì €ì¥ëœ ê²½ìš°)
        if doc_id:
            try:
                contract_chunks = await self._search_contract_chunks(
                    doc_id=doc_id,
                    query=query,
                    top_k=5,  # ë¶„ì„ ì‹œì—ëŠ” ìƒìœ„ 5ê°œ ì‚¬ìš©
                    selected_issue=None
                )
            except Exception as e:
                # contract_chunksê°€ ì•„ì§ ì €ì¥ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‹œ
                logger.warning(f"[ê³„ì•½ì„œ ë¶„ì„] contract_chunks ê²€ìƒ‰ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(e)}")
                contract_chunks = []
        
        # 2-2. ì™¸ë¶€ ë²•ë ¹ ê²€ìƒ‰ (íƒ€ì… ë‹¤ì–‘ì„± í™•ë³´)
        legal_chunks = await self._search_legal_chunks(
            query=query, 
            top_k=8,
            category=None,  # ì „ì²´ ê³„ì•½ì„œ ë¶„ì„ì´ë¯€ë¡œ category í•„í„° ì—†ìŒ
            ensure_diversity=True,  # íƒ€ì… ë‹¤ì–‘ì„± í™•ë³´
        )

        # 3. í”„ë¦¬í”„ë¡œì„¸ì‹±: ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸° íŒ¨í„´ ê°ì§€
        risk_hint = description
        if self._detect_wage_waiver_phrases(extracted_text):
            risk_hint = (
                f"{description or ''}\n\n"
                "â€» ì‹œìŠ¤í…œ íŒíŠ¸: ì´ ê³„ì•½ì„œì—ëŠ” "
                "'ì¶”ê°€ ìˆ˜ë‹¹ì„ ì‚¬ì—…ì£¼ì—ê²Œ ì²­êµ¬í•˜ì§€ ì•Šê¸°ë¡œ í•©ì˜í•œë‹¤' ì™€ ê°™ì´ "
                "ê·¼ë¡œìê°€ ë²•ì—ì„œ ì •í•œ ì—°ì¥Â·ì•¼ê°„Â·íœ´ì¼ê·¼ë¡œ ìˆ˜ë‹¹ ë“± ë²•ì • ì„ê¸ˆ ì²­êµ¬ê¶Œì„ "
                "ë¯¸ë¦¬ í¬ê¸°í•˜ëŠ” ì·¨ì§€ì˜ ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
                "ì´ ì¡°í•­ì˜ ìœ„ë²• ê°€ëŠ¥ì„±ê³¼ ìœ„í—˜ë„ë¥¼ ë°˜ë“œì‹œ ë³„ë„ì˜ ì´ìŠˆë¡œ í‰ê°€í•˜ì„¸ìš”."
            ).strip()

        # 4. LLMìœ¼ë¡œ ë¦¬ìŠ¤í¬ ìš”ì•½/ë¶„ë¥˜ (Dual RAG ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        result = await self._llm_summarize_risk(
            query=query,
            contract_text=extracted_text,
            contract_chunks=contract_chunks,
            grounding_chunks=legal_chunks,
            clauses=clauses,
            contract_type=contract_type,
            user_role=user_role,
            field=field,
            concerns=risk_hint,
        )
        return result

    # 2) í…ìŠ¤íŠ¸ ìƒí™© ì„¤ëª… ê¸°ë°˜ ë¶„ì„ (ë ˆê±°ì‹œ)
    async def analyze_situation(self, text: str) -> LegalAnalysisResult:
        query = text
        grounding_chunks = await self._search_legal_chunks(
            query=query, 
            top_k=8,
            category=None,
            ensure_diversity=True,  # íƒ€ì… ë‹¤ì–‘ì„± í™•ë³´
        )
        result = await self._llm_summarize_risk(
            query=query,
            contract_text=None,
            grounding_chunks=grounding_chunks,
            contract_chunks=None,  # ìƒí™© ë¶„ì„ì—ëŠ” ê³„ì•½ì„œ ì²­í¬ ì—†ìŒ
        )
        return result

    # 2-2) ìƒí™© ê¸°ë°˜ ìƒì„¸ ì§„ë‹¨ (ìƒˆë¡œìš´ API)
    async def analyze_situation_detailed(
        self,
        category_hint: str,
        situation_text: str,
        summary: Optional[str] = None,
        details: Optional[str] = None,
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
        use_workflow: bool = False,  # LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì—¬ë¶€
    ) -> dict:
        """
        ìƒí™© ê¸°ë°˜ ìƒì„¸ ì§„ë‹¨
        
        Args:
            use_workflow: Trueë©´ LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ìš©, Falseë©´ ê¸°ì¡´ ë‹¨ì¼ ìŠ¤í… ë°©ì‹
        
        Returns:
            {
                "classified_type": str,
                "risk_score": int,
                "summary": str,
                "criteria": List[CriteriaItem],
                "action_plan": ActionPlan,
                "scripts": Scripts,
                "related_cases": List[RelatedCase]
            }
        """
        # LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
        if use_workflow:
            try:
                from core.situation_workflow import SituationWorkflow
                workflow = SituationWorkflow()
                initial_state = {
                    "situation_text": situation_text,
                    "category_hint": category_hint,
                    "summary": summary,
                    "details": details,
                    "employment_type": employment_type,
                    "work_period": work_period,
                    "weekly_hours": weekly_hours,
                    "is_probation": is_probation,
                    "social_insurance": social_insurance,
                }
                result = await workflow.run(initial_state)
                logger.info("[ìƒí™©ë¶„ì„] LangGraph ì›Œí¬í”Œë¡œìš°ë¡œ ë¶„ì„ ì™„ë£Œ")
                
                # ì›Œí¬í”Œë¡œìš° ê²°ê³¼ê°€ final_output ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                # final_outputì—ëŠ” summary, findings, organizations ë“±ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨
                if not isinstance(result, dict):
                    logger.warning(f"[ìƒí™©ë¶„ì„] ì›Œí¬í”Œë¡œìš° ê²°ê³¼ê°€ dictê°€ ì•„ë‹˜: {type(result)}")
                    result = {}
                
                # findingsì™€ organizationsê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
                if "findings" not in result:
                    logger.warning("[ìƒí™©ë¶„ì„] ì›Œí¬í”Œë¡œìš° ê²°ê³¼ì— findings í•„ë“œê°€ ì—†ìŒ, ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •")
                    result["findings"] = []
                if "organizations" not in result:
                    logger.warning("[ìƒí™©ë¶„ì„] ì›Œí¬í”Œë¡œìš° ê²°ê³¼ì— organizations í•„ë“œê°€ ì—†ìŒ, ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •")
                    result["organizations"] = []
                
                return result
            except ImportError as e:
                logger.warning(f"[ìƒí™©ë¶„ì„] LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ë¶ˆê°€, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì „í™˜: {str(e)}")
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
            except Exception as e:
                logger.error(f"[ìƒí™©ë¶„ì„] LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì „í™˜: {str(e)}", exc_info=True)
                logger.error(f"[ìƒí™©ë¶„ì„] ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ ìƒì„¸ - íƒ€ì…: {type(e).__name__}, ë©”ì‹œì§€: {str(e)}")
                # ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì—ëŸ¬ë¥¼ ì¬ë°œìƒì‹œí‚¤ì§€ ì•ŠìŒ)
                # RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë°˜í™˜
                try:
                    # RAG ê²€ìƒ‰ì€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
                    query_embedding = await self._get_embedding(situation_text)
                    
                    # ë²¡í„°ìŠ¤í† ì–´ ì§ì ‘ ì‚¬ìš©
                    async def search_legal():
                        rows = self.vector_store.search_similar_legal_chunks(
                            query_embedding=query_embedding,
                            top_k=8,
                            filters=None
                        )
                        results = []
                        for r in rows:
                            source_type = r.get("source_type", "law")
                            if source_type not in ["law", "manual"]:
                                continue
                            results.append(
                                LegalGroundingChunk(
                                    source_id=r.get("id", ""),
                                    source_type=source_type,
                                    title=r.get("title", "ì œëª© ì—†ìŒ"),
                                    snippet=r.get("content", "")[:300],
                                    score=r.get("score", 0.0),
                                )
                            )
                        return results
                    
                    async def search_cases():
                        rows = self.vector_store.search_similar_legal_chunks(
                            query_embedding=query_embedding,
                            top_k=3,
                            filters={"source_type": "case"}
                        )
                        cases = []
                        for row in rows:
                            cases.append({
                                "id": row.get("external_id", ""),
                                "title": row.get("title", "ì œëª© ì—†ìŒ"),
                                "summary": row.get("content", "")[:200],
                            })
                        return cases
                    
                    grounding_chunks, related_cases = await asyncio.gather(
                        search_legal(),
                        search_cases(),
                        return_exceptions=False
                    )
                except Exception as search_error:
                    logger.error(f"[ìƒí™©ë¶„ì„] RAG ê²€ìƒ‰ë„ ì‹¤íŒ¨: {str(search_error)}")
                    grounding_chunks = []
                    related_cases = []
                
                # ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ ì‹œ)
                # findingsì™€ organizationsëŠ” ë¹ˆ ë°°ì—´ë¡œ ë°˜í™˜ (ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ ì‹œ LLM ê²°ê³¼ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ)
                return {
                    "classified_type": category_hint or "unknown",
                    "risk_score": 50,
                    "summary": "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\nìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²•ì  ê´€ì ê³¼ í–‰ë™ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ ìƒí™©\n\nê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                    "findings": [],  # ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´
                    "criteria": [],
                    "action_plan": {"steps": []},
                    "scripts": {
                        "to_company": {
                            "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                            "body": "ìƒí™©ì„ ë¶„ì„í•œ ê²°ê³¼, ê´€ë ¨ ë²•ë ¹ ë° í‘œì¤€ê³„ì•½ì„œë¥¼ ì°¸ê³ í•˜ì—¬ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                        },
                        "to_advisor": {
                            "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                            "body": "ê·¼ë¡œ ê´€ë ¨ ë¬¸ì œë¡œ ìƒë‹´ì„ ë°›ê³ ì í•©ë‹ˆë‹¤. ìƒí™©ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ìƒë‹´ ì‹œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                        }
                    },
                    "related_cases": [],
                    "grounding_chunks": grounding_chunks,
                    "organizations": [],  # ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´
                }
        
        # ê¸°ì¡´ ë‹¨ì¼ ìŠ¤í… ë°©ì‹ (ë ˆê±°ì‹œ)
        # 1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„±
        # summaryì™€ detailsê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ situation_text ì‚¬ìš©
        query_text = situation_text
        if summary:
            query_text = summary
            if details:
                query_text = f"{summary}\n\n{details}"
        
        # 2. ë³‘ë ¬ ì²˜ë¦¬: RAG ê²€ìƒ‰ê³¼ ì¼€ì´ìŠ¤ ê²€ìƒ‰ì„ ë™ì‹œì— ì‹¤í–‰
        # ê°™ì€ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì„ë² ë”©ì„ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©
        query_embedding = await self._get_embedding(query_text)
        
        # ì„ë² ë”©ì„ ê³µìœ í•˜ì—¬ ë³‘ë ¬ ê²€ìƒ‰
        async def search_legal_with_embedding():
            rows = self.vector_store.search_similar_legal_chunks(
                query_embedding=query_embedding,
                top_k=8,
                filters=None
            )
            results: List[LegalGroundingChunk] = []
            for r in rows:
                source_type = r.get("source_type", "law")
                title = r.get("title", "ì œëª© ì—†ìŒ")
                content = r.get("content", "")
                score = r.get("score", 0.0)
                results.append(
                    LegalGroundingChunk(
                        source_id=r.get("id", ""),
                        source_type=source_type,
                        title=title,
                        snippet=content[:300],
                        score=score,
                    )
                )
            return results
        
        async def search_cases_with_embedding():
            rows = self.vector_store.search_similar_legal_chunks(
                query_embedding=query_embedding,
                top_k=3,
                filters={"source_type": "case"}
            )
            cases: List[LegalCasePreview] = []
            for row in rows:
                external_id = row.get("external_id", "")
                title = row.get("title", "ì œëª© ì—†ìŒ")
                content = row.get("content", "")
                metadata = row.get("metadata", {})
                cases.append(
                    LegalCasePreview(
                        id=external_id,
                        title=title,
                        situation=metadata.get("situation", content[:200]),
                        main_issues=metadata.get("issues", []),
                    )
                )
            return cases
        
        grounding_chunks, related_cases = await asyncio.gather(
            search_legal_with_embedding(),
            search_cases_with_embedding(),
            return_exceptions=False
        )
        
        # 3. LLMìœ¼ë¡œ ìƒì„¸ ì§„ë‹¨ ìˆ˜í–‰
        result = await self._llm_situation_diagnosis(
            category_hint=category_hint,
            situation_text=query_text,  # summary + details ë˜ëŠ” situation_text
            grounding_chunks=grounding_chunks,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        
        # 4. ìœ ì‚¬ ì¼€ì´ìŠ¤ ì¶”ê°€
        result["related_cases"] = [
            {
                "id": case.id,
                "title": case.title,
                "summary": case.situation[:200] if len(case.situation) > 200 else case.situation,
            }
            for case in related_cases
        ]
        
        # 5. grounding_chunks ì¶”ê°€ (LLM ì‹¤íŒ¨ ì‹œì—ë„ RAG ê²°ê³¼ëŠ” í¬í•¨)
        result["grounding_chunks"] = [
            {
                "source_id": chunk.source_id,
                "source_type": chunk.source_type,
                "title": chunk.title,
                "snippet": chunk.snippet,
                "score": chunk.score,
                "external_id": getattr(chunk, 'external_id', None),
                "file_url": getattr(chunk, 'file_url', None),
            }
            for chunk in grounding_chunks
        ]
        
        return result

    # 3) ë²•ë¥  ìƒë‹´ ì±— (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)
    async def chat_with_context(
        self,
        query: str,
        doc_ids: List[str] = None,
        selected_issue_id: Optional[str] = None,
        selected_issue: Optional[dict] = None,
        analysis_summary: Optional[str] = None,
        risk_score: Optional[int] = None,
        total_issues: Optional[int] = None,
        top_k: int = 8,
        context_type: Optional[str] = None,
        context_data: Optional[dict] = None,
    ) -> dict:
        """
        ë²•ë¥  ìƒë‹´ ì±— (ì»¨í…ìŠ¤íŠ¸ ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            doc_ids: ê³„ì•½ì„œ ë¬¸ì„œ ID ëª©ë¡
            selected_issue_id: ì„ íƒëœ ì´ìŠˆ ID
            selected_issue: ì„ íƒëœ ì´ìŠˆ ì •ë³´
            analysis_summary: ë¶„ì„ ìš”ì•½
            risk_score: ìœ„í—˜ë„ ì ìˆ˜
            total_issues: ì´ ì´ìŠˆ ê°œìˆ˜
            top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            context_type: ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ('none' | 'situation' | 'contract')
            context_data: ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° (ìƒí™© ë¶„ì„ ë˜ëŠ” ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸)
        
        Returns:
            {
                "answer": str,  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë‹µë³€
                "markdown": str,
                "query": str,
                "used_chunks": List[dict]
            }
        """
        # 1. Dual RAG ê²€ìƒ‰: ë‚´ ê³„ì•½ì„œ + ì™¸ë¶€ ë²•ë ¹
        # ê°™ì€ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì„ë² ë”©ì„ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©
        query_embedding = await self._get_embedding(query)
        
        contract_chunks = []
        legal_chunks = []
        
        # 1-1. ê³„ì•½ì„œ ë‚´ë¶€ ê²€ìƒ‰ (doc_idsê°€ ìˆëŠ” ê²½ìš°)
        async def search_contract_with_embedding():
            if doc_ids and len(doc_ids) > 0:
                doc_id = doc_ids[0]
                # Issue ê¸°ë°˜ boosting
                boost_article = None
                if selected_issue:
                    boost_article = selected_issue.get("article_number")
                    if isinstance(boost_article, str):
                        import re
                        match = re.search(r'(\d+)', str(boost_article))
                        if match:
                            boost_article = int(match.group(1))
                        else:
                            boost_article = None
                    elif not isinstance(boost_article, int):
                        boost_article = None
                
                return self.vector_store.search_similar_contract_chunks(
                    contract_id=doc_id,
                    query_embedding=query_embedding,
                    top_k=3,
                    boost_article=boost_article,
                    boost_factor=1.5
                )
            return []
        
        # 1-2. ì™¸ë¶€ ë²•ë ¹ ê²€ìƒ‰
        # selected_issueê°€ ìˆìœ¼ë©´ ì´ìŠˆ ê¸°ë°˜ ì¿¼ë¦¬ ì‚¬ìš©
        async def search_legal_with_embedding():
            if selected_issue:
                # ì´ìŠˆ ì¤‘ì‹¬ ì¿¼ë¦¬ ìƒì„±
                issue_query = self._build_query_from_issue(selected_issue)
                issue_category = selected_issue.get("category")
                # ì´ìŠˆ ê¸°ë°˜ ê²€ìƒ‰ (íƒ€ì… ë‹¤ì–‘ì„± í™•ë³´)
                return await self._search_legal_chunks(
                    query=issue_query,
                    top_k=top_k,
                    category=issue_category,
                    ensure_diversity=True,
                )
            else:
                # ì¼ë°˜ ì¿¼ë¦¬ ê²€ìƒ‰
                return await self._search_legal_chunks(
                    query=query,
                    top_k=top_k,
                    category=None,
                    ensure_diversity=True,
                )
        
        # ë³‘ë ¬ ê²€ìƒ‰
        contract_chunks, legal_chunks_raw = await asyncio.gather(
            search_contract_with_embedding(),
            search_legal_with_embedding(),
            return_exceptions=False
        )
        # legal_chunks_rawëŠ” LegalGroundingChunk ê°ì²´ì´ë¯€ë¡œ metadata í¬í•¨
        legal_chunks = []
        for chunk in legal_chunks_raw:
            chunk_dict = {
                "id": chunk.source_id,
                "source_type": chunk.source_type,
                "title": chunk.title,
                "content": chunk.snippet,
                "snippet": chunk.snippet,
                "score": chunk.score,
                "external_id": getattr(chunk, "external_id", None),
                "externalId": getattr(chunk, "external_id", None),
            }
            # metadata ì¶”ê°€ (LegalGroundingChunk ê°ì²´ì— metadata í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©)
            if hasattr(chunk, "metadata") and chunk.metadata:
                chunk_dict["metadata"] = chunk.metadata
            legal_chunks.append(chunk_dict)
        
        # 2. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        answer = await self._llm_chat_response(
            query=query,
            contract_chunks=contract_chunks,
            legal_chunks=legal_chunks_raw,
            selected_issue=selected_issue,
            analysis_summary=analysis_summary,
            risk_score=risk_score,
            total_issues=total_issues,
            context_type=context_type,
            context_data=context_data,
        )
        
        return {
            "answer": answer,
            "markdown": answer,
            "query": query,
            "used_chunks": {
                "contract": contract_chunks,
                "legal": legal_chunks
            },
        }

    # 4) ì‹œë‚˜ë¦¬ì˜¤/ì¼€ì´ìŠ¤ ê²€ìƒ‰
    async def search_cases(self, query: str, limit: int = 5) -> List[LegalCasePreview]:
        """
        cases/*.md ì—ì„œë§Œ ê²€ìƒ‰í•˜ëŠ” ë¼ì´íŠ¸í•œ ê²€ìƒ‰ (ìƒˆ ìŠ¤í‚¤ë§ˆ).
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)
        query_embedding = await self._get_embedding(query)
        
        # ë²¡í„° ê²€ìƒ‰ (case íƒ€ì…ë§Œ í•„í„°ë§)
        rows = self.vector_store.search_similar_legal_chunks(
            query_embedding=query_embedding,
            top_k=limit,
            filters={"source_type": "case"}
        )

        cases: List[LegalCasePreview] = []
        for row in rows:
            # ìƒˆ ìŠ¤í‚¤ë§ˆì—ì„œ ì •ë³´ ì¶”ì¶œ
            external_id = row.get("external_id", "")
            title = row.get("title", "ì œëª© ì—†ìŒ")
            content = row.get("content", "")
            metadata = row.get("metadata", {})
            
            cases.append(
                LegalCasePreview(
                    id=external_id,
                    title=title,
                    situation=metadata.get("situation", content[:200]),
                    main_issues=metadata.get("issues", []),
                )
            )
        return cases

    # ================= ë‚´ë¶€ ìœ í‹¸ =================
    
    def _detect_wage_waiver_phrases(self, text: str) -> bool:
        """
        'ì¶”ê°€ ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸°'ë¥˜ ë¬¸êµ¬ê°€ ìˆëŠ”ì§€ ê°„ë‹¨íˆ ê°ì§€
        
        Args:
            text: ê³„ì•½ì„œ í…ìŠ¤íŠ¸
            
        Returns:
            íŒ¨í„´ì´ ë°œê²¬ë˜ë©´ True
        """
        if not text:
            return False
        
        import re
        
        patterns = [
            r"ì¶”ê°€\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Šê¸°ë¡œ\s+í•©ì˜í•œë‹¤",
            r"ë²•ì—ì„œ\s*ì •í•œ\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Šê¸°ë¡œ",
            r"ì—°ì¥.?ì•¼ê°„.?íœ´ì¼\s*ê·¼ë¡œ\s*ìˆ˜ë‹¹[^\n]*ë³„ë„ë¡œ\s*ì²­êµ¬í•˜ì§€\s+ì•ŠëŠ”ë‹¤",
            r"í¬ê´„ì„ê¸ˆ[^\n]*ì¶”ê°€[^\n]*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Š",
            r"ì‹¤ì œ\s*ê·¼ë¡œì‹œê°„[^\n]*í¬ê´„ì„ê¸ˆ[^\n]*ì´ˆê³¼[^\n]*ì¶”ê°€\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€",
            r"ì—°ì¥.?ì•¼ê°„.?íœ´ì¼\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Š",
            r"ë²•ì •\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Š",
        ]
        
        for pattern in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                logger.info(f"[í”„ë¦¬í”„ë¡œì„¸ì‹±] ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸° íŒ¨í„´ ê°ì§€ë¨: {pattern}")
                return True
        
        return False
    
    def _ensure_wage_waiver_issue(
        self,
        contract_text: str,
        issues: List[LegalIssue],
    ) -> None:
        """
        ê³„ì•½ì„œì— 'ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸°'ë¥˜ ë¬¸êµ¬ê°€ ìˆëŠ”ë°
        LLMì´ ì´ìŠˆë¥¼ ì•ˆ ë§Œë“¤ì–´ ì¤¬ë‹¤ë©´, ê°•ì œë¡œ í•˜ë‚˜ ì¶”ê°€í•œë‹¤.
        
        Args:
            contract_text: ê³„ì•½ì„œ í…ìŠ¤íŠ¸
            issues: ê¸°ì¡´ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ (in-place ìˆ˜ì •)
        """
        if not contract_text:
            return
        
        # ì´ë¯¸ ë¹„ìŠ·í•œ ì´ìŠˆê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
        for issue in issues:
            issue_desc = (issue.description or "") + (issue.summary or "") + (issue.rationale or "")
            if issue.category in ("wage", "working_hours") and \
               "ìˆ˜ë‹¹" in issue_desc and \
               ("ì²­êµ¬" in issue_desc or "í¬ê¸°" in issue_desc or "í•©ì˜" in issue_desc):
                logger.info("[í›„ì²˜ë¦¬] ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸° ê´€ë ¨ ì´ìŠˆê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì¶”ê°€í•˜ì§€ ì•ŠìŒ")
                return  # ì´ë¯¸ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¢…ë£Œ
        
        # í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¬¸êµ¬ ê²€ìƒ‰
        import re
        
        # ë” ì •í™•í•œ íŒ¨í„´ë¶€í„° ì‹œë„
        pattern = r"ì œ\s*\d+\s*ì¡°[\s\S]{0,100}?íŠ¹ì•½ì‚¬í•­[\s\S]{0,200}?ì¶”ê°€\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Šê¸°ë¡œ\s+í•©ì˜í•œë‹¤[^\n]*"
        match = re.search(pattern, contract_text)
        
        if not match:
            # ì¡°ê¸ˆ ë” ëŠìŠ¨í•œ íŒ¨í„´ë“¤
            patterns = [
                r"ì¶”ê°€\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Šê¸°ë¡œ\s+í•©ì˜í•œë‹¤",
                r"ì‹¤ì œ\s*ê·¼ë¡œì‹œê°„[^\n]*í¬ê´„ì„ê¸ˆ[^\n]*ì´ˆê³¼[^\n]*ì¶”ê°€\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€",
                r"ì—°ì¥.?ì•¼ê°„.?íœ´ì¼\s*ê·¼ë¡œ\s*ìˆ˜ë‹¹[^\n]*ë³„ë„ë¡œ\s*ì²­êµ¬í•˜ì§€\s+ì•ŠëŠ”ë‹¤",
                r"ë²•ì—ì„œ\s*ì •í•œ\s*ìˆ˜ë‹¹[^\n]*ì²­êµ¬í•˜ì§€\s+ì•Šê¸°ë¡œ",
            ]
            for p in patterns:
                match = re.search(p, contract_text, re.IGNORECASE)
                if match:
                    break
        
        if not match:
            return  # ë°œê²¬ ëª»í•˜ë©´ ì¢…ë£Œ
        
        clause_text = match.group(0)
        if len(clause_text) > 500:
            clause_text = clause_text[:500] + "..."
        
        # LegalIssue ê°ì²´ ìƒì„±
        from models.schemas import LegalIssue
        
        waiver_issue = LegalIssue(
            name="issue-wage-waiver",
            description=(
                "í¬ê´„ì„ê¸ˆì œ í•˜ì—ì„œ ì‹¤ì œ ê·¼ë¡œì‹œê°„ì´ í¬ê´„ì„ê¸ˆì— í¬í•¨ëœ ì‹œê°„ì„ ì´ˆê³¼í•˜ë”ë¼ë„ "
                "ê·¼ë¡œìê°€ ì¶”ê°€ ìˆ˜ë‹¹ì„ ì‚¬ì—…ì£¼ì—ê²Œ ì²­êµ¬í•˜ì§€ ì•Šê¸°ë¡œ ì•½ì •í•œ ì¡°í•­ì…ë‹ˆë‹¤. "
                "ì—°ì¥Â·ì•¼ê°„Â·íœ´ì¼ê·¼ë¡œ ìˆ˜ë‹¹ ë“± ë²•ì • ì„ê¸ˆ ì²­êµ¬ê¶Œì„ ì‚¬ì „ì— í¬ê¸°ì‹œí‚¤ëŠ” ë‚´ìš©ìœ¼ë¡œ "
                "ê·¼ë¡œê¸°ì¤€ë²• ì œ15ì¡° ë“±ì˜ ê°•í–‰ê·œì •ì— ìœ„ë°˜ë˜ì–´ ë¬´íš¨ë¡œ ë³¼ ì—¬ì§€ê°€ í¬ë©°, "
                "ì„ê¸ˆì²´ë¶ˆ ë¶„ìŸ ìœ„í—˜ì´ ë§¤ìš° í½ë‹ˆë‹¤."
            ),
            severity="high",
            legal_basis=[
                "ê·¼ë¡œê¸°ì¤€ë²• ì œ15ì¡°: ë²•ì—ì„œ ì •í•œ ê¸°ì¤€ì— ë¯¸ì¹˜ì§€ ëª»í•˜ëŠ” ê·¼ë¡œì¡°ê±´ì„ ì •í•œ ê·¼ë¡œê³„ì•½ ë¶€ë¶„ì€ ë¬´íš¨",
                "ê·¼ë¡œê¸°ì¤€ë²• ì œ56ì¡°: ì—°ì¥Â·ì•¼ê°„Â·íœ´ì¼ê·¼ë¡œì— ëŒ€í•œ ê°€ì‚°ìˆ˜ë‹¹ ì§€ê¸‰ ì˜ë¬´",
            ],
            start_index=match.start(),
            end_index=match.end(),
            suggested_text=None,
            rationale=(
                "í¬ê´„ì„ê¸ˆì œ ê³„ì•½ì„ ì²´ê²°í–ˆë”ë¼ë„, ì‹¤ì œ ê·¼ë¡œì‹œê°„ì„ ì‚°ì •í•˜ì—¬ ë²•ì • ìˆ˜ë‹¹(ì—°ì¥, ì•¼ê°„, íœ´ì¼)ì´ "
                "í¬ê´„ì„ê¸ˆì•¡ì„ ì´ˆê³¼í•  ê²½ìš° ì°¨ì•¡ì„ ì§€ê¸‰í•´ì•¼ í•  ì˜ë¬´ê°€ ìˆìŠµë‹ˆë‹¤. "
                "'ì²­êµ¬í•˜ì§€ ì•Šê¸°ë¡œ í•©ì˜'ëŠ” ê·¼ë¡œê¸°ì¤€ë²• ì œ15ì¡° ìœ„ë°˜ìœ¼ë¡œ ë¬´íš¨ì´ë©°, ì„ê¸ˆ ì²´ë¶ˆ ì†Œì§€ê°€ í½ë‹ˆë‹¤. "
                "ê·¼ë¡œìëŠ” ë²•ì—ì„œ ë³´ì¥í•˜ëŠ” ì„ê¸ˆÂ·ìˆ˜ë‹¹ì„ ì‚¬ì „ì— í¬ê¸°í•  ìˆ˜ ì—†ìœ¼ë©°, ì´ëŸ¬í•œ í•©ì˜ëŠ” ë¬´íš¨ê°€ ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
            ),
            suggested_questions=[
                "ì‹¤ì œ ì—°ì¥Â·ì•¼ê°„Â·íœ´ì¼ê·¼ë¡œ ì‹œê°„ì´ í¬ê´„ì„ê¸ˆì— í¬í•¨ëœ ì‹œê°„ë³´ë‹¤ ë§ì„ ê²½ìš°, ì°¨ì•¡ ìˆ˜ë‹¹ì„ ë³„ë„ë¡œ ì§€ê¸‰í•˜ë‚˜ìš”?",
                "í¬ê´„ì„ê¸ˆì— í¬í•¨ëœ ì‹œê°„(ì›” ëª‡ ì‹œê°„ë¶„)ê³¼ ê·¸ ê³„ì‚° ê¸°ì¤€ì„ ê³„ì•½ì„œì— ëª…ì‹œí•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
            ],
            original_text=clause_text,
            clause_id=None,  # clause_idëŠ” ë‚˜ì¤‘ì— ë§¤ì¹­ë  ìˆ˜ ìˆìŒ
            category="wage",
            summary="ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œì„ ì‚¬ì „ì— í¬ê¸°ì‹œí‚¤ëŠ” í¬ê´„ì„ê¸ˆ íŠ¹ì•½",
            toxic_clause_detail=None,
        )
        
        issues.append(waiver_issue)
        logger.info(f"[í›„ì²˜ë¦¬] ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸° ì´ìŠˆ ê°•ì œ ì¶”ê°€ë¨ (ìœ„ì¹˜: {match.start()}-{match.end()})")
    
    def _build_file_path(self, source_type: str, external_id: str) -> str:
        """
        Storage íŒŒì¼ ê²½ë¡œ ìƒì„±
        
        Args:
            source_type: 'law' | 'manual' | 'case' | 'standard_contract'
            external_id: íŒŒì¼ í‚¤ (MD5 or filename)
        
        Returns:
            Storage object key (ì˜ˆ: "standard_contract/437f9719fcdf4fb0a3b011315b75c56c.pdf")
        """
        # source_typeì„ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜
        folder_mapping = {
            "law": "laws",
            "manual": "manuals",
            "case": "cases",
            "standard_contract": "standard_contracts",
        }
        folder_name = folder_mapping.get(source_type, source_type)
        
        # external_idì— í™•ì¥ìê°€ ì—†ë‹¤ëŠ” ê°€ì •ì´ë©´ .pdf ì¶”ê°€
        if not external_id.lower().endswith(".pdf"):
            object_name = f"{external_id}.pdf"
        else:
            object_name = external_id
        
        # ê²½ë¡œ ê·œì¹™: {folder_name}/{object_name}
        return f"{folder_name}/{object_name}"
    
    async def _build_reason(
        self,
        issue_summary: str,
        clause_text: str,
        basis_snippet: str,
    ) -> Optional[str]:
        """
        "ì™œ ì´ ê·¼ê±°ë¥¼ ë¶™ì˜€ëŠ”ì§€" LLM í•œ ì¤„ ì„¤ëª… ìƒì„±
        
        Args:
            issue_summary: ì´ìŠˆ ìš”ì•½
            clause_text: ê³„ì•½ì„œ ì¡°í•­ í…ìŠ¤íŠ¸
            basis_snippet: ë²•ë ¹/í‘œì¤€ê³„ì•½ì„œ ìŠ¤ë‹ˆí«
        
        Returns:
            ì´ìœ  ì„¤ëª… (1~2ë¬¸ì¥) ë˜ëŠ” None (ìƒì„± ì‹¤íŒ¨ ì‹œ)
        """
        if self.generator.disable_llm:
            return None
        
        try:
            prompt = f"""ì•„ë˜ ì„¸ ì •ë³´ë¥¼ ë³´ê³ , ì™œ ì´ ë²•ë ¹/í‘œì¤€ê³„ì•½ì„œ ìŠ¤ë‹ˆí«ì´ ì´ ì´ìŠˆì˜ ê·¼ê±°ê°€ ë˜ëŠ”ì§€
í•œêµ­ì–´ë¡œ 1~2ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì¤˜.

[ì´ìŠˆ ìš”ì•½]
{issue_summary[:500]}

[ê³„ì•½ì„œ ì¡°í•­]
{clause_text[:500]}

[ë²•ë ¹/í‘œì¤€ê³„ì•½ì„œ ìŠ¤ë‹ˆí«]
{basis_snippet[:500]}

ë‹µë³€ì€ ì„¤ëª…ë§Œ ê°„ë‹¨íˆ ì‘ì„±í•˜ê³ , ë‹¤ë¥¸ ë¶€ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
            
            # Groq ì‚¬ìš© (ìš°ì„ )
            from config import settings
            if settings.use_groq:
                from llm_api import ask_groq_with_messages
                messages = [
                    {"role": "system", "content": "ë„ˆëŠ” ìœ ëŠ¥í•œ ë²•ë¥  AIì•¼. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ]
                response_text = ask_groq_with_messages(
                    messages=messages,
                    temperature=0.3,  # reason ìƒì„±ì€ ë‚®ì€ temperature ì‚¬ìš©
                    model=settings.groq_model
                )
                return response_text.strip() if response_text else None
            # Ollama ì‚¬ìš© (ë ˆê±°ì‹œ)
            elif self.generator.use_ollama:
                from langchain_ollama import OllamaLLM
                from config import settings
                llm = OllamaLLM(
                    base_url=settings.ollama_base_url,
                    model=settings.ollama_model
                )
                # ëŒ€ëµì ì¸ ì…ë ¥ í† í° ì¶”ì •
                estimated_input_tokens = len(prompt) // 2.5
                logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì…ë ¥ ì¶”ì •: ì•½ {int(estimated_input_tokens)}í† í° (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì)")
                
                response_text = llm.invoke(prompt)
                
                # ëŒ€ëµì ì¸ ì¶œë ¥ í† í° ì¶”ì •
                if response_text:
                    estimated_output_tokens = len(response_text) // 2.5
                    estimated_total_tokens = int(estimated_input_tokens) + int(estimated_output_tokens)
                    logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì¶œë ¥ ì¶”ì •: ì•½ {int(estimated_output_tokens)}í† í°, ì´ ì¶”ì •: ì•½ {estimated_total_tokens}í† í° (ëª¨ë¸: {settings.ollama_model})")
                
                return response_text.strip() if response_text else None
            else:
                return None
        except Exception as e:
            logger.debug(f"[reason ìƒì„±] LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _get_embeddings_batch(
        self,
        queries: List[str],
        use_cache: bool = True
    ) -> List[List[float]]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„± (ìºì‹± ì§€ì›)
        
        Args:
            queries: ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not queries:
            return []
        
        # ìºì‹œì—ì„œ ì°¾ê¸°
        uncached_queries = []
        uncached_indices = []
        embeddings = [None] * len(queries)
        
        for idx, query in enumerate(queries):
            if use_cache:
                cached_embedding = self._embedding_cache.get(query)
                if cached_embedding is not None:
                    embeddings[idx] = cached_embedding
                    continue
            uncached_queries.append(query)
            uncached_indices.append(idx)
        
        # ìºì‹œì— ì—†ëŠ” ì¿¼ë¦¬ë§Œ ë°°ì¹˜ë¡œ ìƒì„±
        if uncached_queries:
            # ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
            new_embeddings = await asyncio.to_thread(
                self.generator.embed,
                uncached_queries
            )
            
            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜í•˜ê³  ìºì‹œì— ì €ì¥
            for cache_idx, original_idx in enumerate(uncached_indices):
                embedding = new_embeddings[cache_idx]
                embeddings[original_idx] = embedding
                if use_cache:
                    self._embedding_cache.put(uncached_queries[cache_idx], embedding)
        
        return embeddings
    
    async def _get_embedding(
        self,
        query: str,
        use_cache: bool = True
    ) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)
        
        Args:
            query: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        if use_cache:
            cached_embedding = self._embedding_cache.get(query)
            if cached_embedding is not None:
                return cached_embedding
        
        # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ë¸”ë¡œí‚¹ ë°©ì§€
        embedding = await asyncio.to_thread(self.generator.embed_one, query)
        
        if use_cache:
            self._embedding_cache.put(query, embedding)
        
        return embedding

    def _build_query_from_contract(
        self,
        extracted_text: str,
        description: Optional[str],
    ) -> str:
        # ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„/ì¡°í•­ ì œëª©ë§Œ ì‚¬ìš©
        snippet = extracted_text[:2000]
        if description:
            return f"ì‚¬ìš©ì ì„¤ëª…: {description}\n\nê³„ì•½ì„œ ì£¼ìš” ë‚´ìš©:\n{snippet}"
        return f"ê³„ì•½ì„œ ì£¼ìš” ë‚´ìš©:\n{snippet}"

    async def _search_contract_chunks(
        self,
        doc_id: str,
        query: str,
        top_k: int = 3,
        selected_issue: Optional[dict] = None
    ) -> List[dict]:
        """
        ê³„ì•½ì„œ ë‚´ë¶€ ì²­í¬ ê²€ìƒ‰ (issue ê¸°ë°˜ boosting)
        
        Args:
            doc_id: ê³„ì•½ì„œ ID
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            selected_issue: ì„ íƒëœ ì´ìŠˆ (article_number í¬í•¨)
        
        Returns:
            ê³„ì•½ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)
        query_embedding = await self._get_embedding(query)
        
        # Issue ê¸°ë°˜ boosting: ê°™ì€ ì¡°í•­ì´ë©´ ê°€ì 
        boost_article = None
        if selected_issue:
            # selected_issueì—ì„œ article_number ì¶”ì¶œ
            boost_article = selected_issue.get("article_number")
            if isinstance(boost_article, str):
                # "ì œ5ì¡°" í˜•ì‹ì—ì„œ ìˆ«ì ì¶”ì¶œ
                import re
                match = re.search(r'(\d+)', str(boost_article))
                if match:
                    boost_article = int(match.group(1))
                else:
                    boost_article = None
            elif not isinstance(boost_article, int):
                boost_article = None
        
        # ë²¡í„° ê²€ìƒ‰
        chunks = self.vector_store.search_similar_contract_chunks(
            contract_id=doc_id,
            query_embedding=query_embedding,
            top_k=top_k,
            boost_article=boost_article,
            boost_factor=1.5
        )
        
        return chunks
    
    def _build_query_from_issue(
        self,
        issue: Dict[str, Any],
    ) -> str:
        """
        ì´ìŠˆ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„± (ì´ìŠˆ ì¤‘ì‹¬ ê²€ìƒ‰ìš©)
        
        Args:
            issue: ì´ìŠˆ ì •ë³´ (clause_text, rationale, category í¬í•¨)
        
        Returns:
            ì´ìŠˆ ì¤‘ì‹¬ ì¿¼ë¦¬ ë¬¸ìì—´
        """
        clause_text = issue.get("original_text") or issue.get("clause_text") or issue.get("originalText", "")
        rationale = issue.get("rationale") or issue.get("reason") or issue.get("description", "")
        category = issue.get("category", "")
        summary = issue.get("summary", "")
        
        query_parts = []
        
        if clause_text:
            query_parts.append(f"ë¬¸ì œëœ ê³„ì•½ ì¡°í•­:\n{clause_text[:500]}")
        
        if rationale:
            query_parts.append(f"ì´ ì¡°í•­ì˜ ìœ„í—˜ ìš”ì•½:\n{rationale[:500]}")
        elif summary:
            query_parts.append(f"ì´ìŠˆ ìš”ì•½:\n{summary[:500]}")
        
        if category:
            query_parts.append(f"ì´ ì´ìŠˆì˜ ì¹´í…Œê³ ë¦¬: {category}")
        
        if not query_parts:
            # í´ë°±: issue ì „ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(issue)[:1000]
        
        return "\n\n".join(query_parts)
    
    async def _search_legal_chunks(
        self,
        query: str,
        top_k: int = 8,
        category: Optional[str] = None,
        ensure_diversity: bool = True,
    ) -> List[LegalGroundingChunk]:
        """
        ë²¡í„°ìŠ¤í† ì–´ + ë©”íƒ€ë°ì´í„°ë¡œ
        - laws
        - manuals
        - cases
        ì„ì–´ì„œ ê²€ìƒ‰ (ìƒˆ ìŠ¤í‚¤ë§ˆ).
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            category: ì´ìŠˆ ì¹´í…Œê³ ë¦¬ (í•„í„°ë§ìš©, ì˜ˆ: "wage", "working_hours")
            ensure_diversity: íƒ€ì… ë‹¤ì–‘ì„± í™•ë³´ ì—¬ë¶€ (ìƒìœ„ 20ê°œì—ì„œ source_typeë³„ quota ì±„ì›Œì„œ ì„ ì •)
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)
        query_embedding = await self._get_embedding(query)
        
        # í•„í„° êµ¬ì„± (categoryê°€ ìˆìœ¼ë©´ metadataì—ì„œ topic_main í•„í„°ë§)
        filters = None
        if category:
            # categoryë¥¼ topic_mainìœ¼ë¡œ ë§¤í•‘
            # category ì˜ˆ: "wage", "working_hours", "job_stability", "dismissal", "payment", "ip", "nda", "non_compete", "liability", "dispute"
            # topic_mainì€ legal_chunksì˜ metadata JSONB í•„ë“œì— ì €ì¥ë˜ì–´ ìˆìŒ
            filters = {"topic_main": category}
        
        # íƒ€ì… ë‹¤ì–‘ì„±ì„ ìœ„í•´ ìƒìœ„ 20ê°œë¥¼ ë¨¼ì € ê°€ì ¸ì˜´ (RPCì—ì„œ ë” ë§ì€ í›„ë³´ë¥¼ ë°›ì•„ì„œ Pythonì—ì„œ ë‹¤ì–‘ì„± í™•ë³´)
        candidate_top_k = 20 if ensure_diversity else top_k
        
        # ë²¡í„° ê²€ìƒ‰ (RPC í•¨ìˆ˜ ì‚¬ìš©)
        rows = self.vector_store.search_similar_legal_chunks(
            query_embedding=query_embedding,
            top_k=candidate_top_k,  # íƒ€ì… ë‹¤ì–‘ì„±ì„ ìœ„í•´ 20ê°œ í›„ë³´ë¥¼ ë°›ìŒ
            filters=filters
        )

        results: List[LegalGroundingChunk] = []
        for r in rows:
            # ìƒˆ ìŠ¤í‚¤ë§ˆì—ì„œ source_typeì€ ì§ì ‘ ì»¬ëŸ¼
            source_type = r.get("source_type", "law")
            title = r.get("title", "ì œëª© ì—†ìŒ")
            content = r.get("content", "")
            score = r.get("score", 0.0)
            file_path = r.get("file_path", None)
            external_id = r.get("external_id", None)
            chunk_index = r.get("chunk_index", None)
            
            # file_pathê°€ ì—†ìœ¼ë©´ external_idë¡œ ìƒì„±
            if not file_path and external_id:
                file_path = self._build_file_path(source_type, external_id)
            
            # ìŠ¤í† ë¦¬ì§€ íŒŒì¼ URL ìƒì„±
            file_url = None
            if external_id:
                try:
                    file_url = self.vector_store.get_storage_file_url(
                        external_id=external_id,
                        source_type=source_type,
                        expires_in=3600  # 1ì‹œê°„
                    )
                except Exception as e:
                    logger.warning(f"ìŠ¤í† ë¦¬ì§€ URL ìƒì„± ì‹¤íŒ¨ (external_id={external_id}): {str(e)}")
            
            # metadata ì¶”ì¶œ
            metadata = r.get("metadata", {}) or {}
            
            # LegalGroundingChunk ê°ì²´ ìƒì„± (metadata í¬í•¨)
            results.append(
                LegalGroundingChunk(
                    source_id=r.get("id", ""),
                    source_type=source_type,
                    title=title,
                    snippet=content[:300],
                    score=score,
                    file_path=file_path,
                    external_id=external_id,
                    chunk_index=chunk_index,
                    file_url=file_url,
                    metadata=metadata,
                )
            )
        
        # threshold ì²´í¬: ìƒìœ„ 1ê°œ ìŠ¤ì½”ì–´ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if results and len(results) > 0:
            top_score = results[0].score
            if top_score < 0.4:  # threshold: 0.4
                logger.info(f"[ë²•ë ¹ ê²€ìƒ‰] ìƒìœ„ ìŠ¤ì½”ì–´ê°€ ë„ˆë¬´ ë‚®ìŒ (score={top_score:.3f} < 0.4), ê²°ê³¼ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬")
                return []  # threshold ë¯¸ë§Œì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        # íƒ€ì… ë‹¤ì–‘ì„± í™•ë³´: source_typeë³„ quota ì±„ì›Œì„œ 8ê°œ ì„ ì •
        if ensure_diversity and len(results) > top_k:
            results = self._ensure_source_type_diversity(results, top_k)
        
        return results[:top_k]
    
    def _ensure_source_type_diversity(
        self,
        candidates: List[LegalGroundingChunk],
        target_count: int = 8,
    ) -> List[LegalGroundingChunk]:
        """
        source_typeë³„ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ì—¬ ê²°ê³¼ ì„ ì •
        
        ëª©í‘œ:
        - ìµœì†Œ 1ê°œ: ë²•ë ¹ (law)
        - ìµœì†Œ 1ê°œ: ê°€ì´ë“œ/í‘œì¤€ê³„ì•½ (manual, standard_contract)
        - ìˆìœ¼ë©´ 1ê°œ: íŒë¡€/ì¼€ì´ìŠ¤ (case)
        
        Args:
            candidates: í›„ë³´ ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬ë¨)
            target_count: ìµœì¢… ë°˜í™˜í•  ê°œìˆ˜
        
        Returns:
            ë‹¤ì–‘ì„±ì„ í™•ë³´í•œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if len(candidates) <= target_count:
            return candidates
        
        # source_typeë³„ë¡œ ë¶„ë¥˜
        by_type: Dict[str, List[LegalGroundingChunk]] = {
            "law": [],
            "manual": [],
            "standard_contract": [],
            "case": [],
            "other": [],
        }
        
        for chunk in candidates:
            source_type = chunk.source_type or "other"
            if source_type in by_type:
                by_type[source_type].append(chunk)
            else:
                by_type["other"].append(chunk)
        
        # ê°€ì´ë“œì™€ í‘œì¤€ê³„ì•½ì„ í•©ì¹¨
        guide_chunks = by_type["manual"] + by_type["standard_contract"]
        
        selected: List[LegalGroundingChunk] = []
        used_indices = set()
        
        # 1. ë²•ë ¹ ìµœì†Œ 1ê°œ
        if by_type["law"]:
            selected.append(by_type["law"][0])
            used_indices.add(0)
        
        # 2. ê°€ì´ë“œ/í‘œì¤€ê³„ì•½ ìµœì†Œ 1ê°œ
        if guide_chunks:
            selected.append(guide_chunks[0])
            # candidatesì—ì„œ í•´ë‹¹ chunkì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            for idx, chunk in enumerate(candidates):
                if chunk.source_id == guide_chunks[0].source_id:
                    used_indices.add(idx)
                    break
        
        # 3. ì¼€ì´ìŠ¤ ìˆìœ¼ë©´ 1ê°œ
        if by_type["case"]:
            selected.append(by_type["case"][0])
            for idx, chunk in enumerate(candidates):
                if chunk.source_id == by_type["case"][0].source_id:
                    used_indices.add(idx)
                    break
        
        # 4. ë‚˜ë¨¸ì§€ëŠ” ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì±„ìš°ê¸°
        for idx, chunk in enumerate(candidates):
            if len(selected) >= target_count:
                break
            if idx not in used_indices:
                selected.append(chunk)
                used_indices.add(idx)
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì¬ì •ë ¬ (ë‹¤ì–‘ì„± í™•ë³´ í›„ì—ë„ ìœ ì‚¬ë„ ìš°ì„ )
        selected.sort(key=lambda x: x.score, reverse=True)
        
        return selected[:target_count]

    async def _llm_summarize_risk(
        self,
        query: str,
        contract_text: Optional[str],
        grounding_chunks: List[LegalGroundingChunk],
        contract_chunks: Optional[List[dict]] = None,
        clauses: Optional[List[Dict]] = None,
        contract_type: Optional[str] = None,
        user_role: Optional[str] = None,
        field: Optional[str] = None,
        concerns: Optional[str] = None,
    ) -> LegalAnalysisResult:
        """
        LLM í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´:
        - risk_score, risk_level
        - issues[]
        - recommendations[]
        ë¥¼ ìƒì„±í•˜ë„ë¡ í•˜ëŠ” ë¶€ë¶„.
        """
        logger.info(f"[LLM í˜¸ì¶œ] _llm_summarize_risk ì‹œì‘: query ê¸¸ì´={len(query)}, contract_text ê¸¸ì´={len(contract_text) if contract_text else 0}, grounding_chunks={len(grounding_chunks)}, contract_chunks={len(contract_chunks) if contract_chunks else 0}")
        logger.info(f"[LLM í˜¸ì¶œ] disable_llm={self.generator.disable_llm}, use_ollama={self.generator.use_ollama}")
        
        if self.generator.disable_llm:
            # LLM ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ì‘ë‹µ
            dummy_issue = LegalIssue(
                name="LLM ë¶„ì„ ë¹„í™œì„±í™”",
                description="LLM ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                severity="low",
                legal_basis=[],
            )
            return LegalAnalysisResult(
                risk_score=50,
                risk_level="medium",
                summary="LLM ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ë§Œ ì œê³µë©ë‹ˆë‹¤.",
                issues=[dummy_issue],
                recommendations=[],
                grounding=grounding_chunks,
            )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš© (Dual RAG ì§€ì›)
        prompt = build_contract_analysis_prompt(
            contract_text=contract_text or "",
            grounding_chunks=grounding_chunks,
            contract_chunks=contract_chunks,
            description=concerns or query if query else None,
            clauses=clauses,
            contract_type=contract_type,
            user_role=user_role,
            field=field,
            concerns=concerns,
        )
        

        try:
            # Groq ì‚¬ìš© (ìš°ì„ )
            from config import settings
            import json
            import re
            
            if settings.use_groq:
                from llm_api import ask_groq_with_messages
                
                # í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                messages = [
                    {"role": "system", "content": "ë„ˆëŠ” ìœ ëŠ¥í•œ ë²•ë¥  AIì•¼. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ]
                
                try:
                    response_text = ask_groq_with_messages(
                        messages=messages,
                        temperature=settings.llm_temperature,
                        model=settings.groq_model,
                        max_tokens=8192  # ê³„ì•½ì„œ ë¶„ì„ì€ ê¸´ JSON ì‘ë‹µì´ í•„ìš”í•˜ë¯€ë¡œ í† í° ìˆ˜ ì¦ê°€
                    )
                    logger.info(f"[Groq í˜¸ì¶œ ì„±ê³µ] ì‘ë‹µ ê¸¸ì´: {len(response_text) if response_text else 0}ì")
                except Exception as groq_error:
                    logger.error(f"[Groq í˜¸ì¶œ ì‹¤íŒ¨] {str(groq_error)}", exc_info=True)
                    raise  # ìƒìœ„ exceptë¡œ ì „ë‹¬
            # Ollama ì‚¬ìš© (ë ˆê±°ì‹œ)
            elif self.generator.use_ollama:
                logger.info(f"[LLM í˜¸ì¶œ] Ollama í˜¸ì¶œ ì‹œì‘: base_url={settings.ollama_base_url}, model={settings.ollama_model}")
                
                # langchain-community ìš°ì„  ì‚¬ìš© (think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ë°©ì§€)
                try:
                    from langchain_community.llms import Ollama
                    llm = Ollama(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                    logger.info("[LLM í˜¸ì¶œ] langchain_community.llms.Ollama ì‚¬ìš©")
                except ImportError:
                    # ëŒ€ì•ˆ: langchain-ollama ì‚¬ìš© (think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ê°€ëŠ¥)
                    try:
                        from langchain_ollama import OllamaLLM
                        llm = OllamaLLM(
                            base_url=settings.ollama_base_url,
                            model=settings.ollama_model
                        )
                        logger.info("[LLM í˜¸ì¶œ] langchain_ollama.OllamaLLM ì‚¬ìš©")
                    except Exception as e:
                        if "think" in str(e).lower():
                            logger.warning("[LLM í˜¸ì¶œ] langchain-ollamaì—ì„œ think íŒŒë¼ë¯¸í„° ì—ëŸ¬ ë°œìƒ. langchain-communityë¡œ ì¬ì‹œë„...")
                            from langchain_community.llms import Ollama
                            llm = Ollama(
                                base_url=settings.ollama_base_url,
                                model=settings.ollama_model
                            )
                            logger.info("[LLM í˜¸ì¶œ] langchain_community.llms.Ollama ì‚¬ìš© (fallback)")
                        else:
                            raise
                
                logger.info(f"[LLM í˜¸ì¶œ] í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì, invoke í˜¸ì¶œ ì¤‘...")
                logger.debug(f"[LLM í˜¸ì¶œ] í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì): {prompt[:500]}")
                # ëŒ€ëµì ì¸ ì…ë ¥ í† í° ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€: 1í† í° â‰ˆ 2-3ì)
                estimated_input_tokens = len(prompt) // 2.5
                logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì…ë ¥ ì¶”ì •: ì•½ {int(estimated_input_tokens)}í† í° (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì)")
                
                response_text = llm.invoke(prompt)
                
                # ëŒ€ëµì ì¸ ì¶œë ¥ í† í° ì¶”ì •
                estimated_output_tokens = len(response_text) // 2.5 if response_text else 0
                estimated_total_tokens = int(estimated_input_tokens) + int(estimated_output_tokens)
                logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì¶œë ¥ ì¶”ì •: ì•½ {int(estimated_output_tokens)}í† í°, ì´ ì¶”ì •: ì•½ {estimated_total_tokens}í† í° (ëª¨ë¸: {settings.ollama_model})")
            else:
                # Groqì™€ Ollama ëª¨ë‘ ì‚¬ìš© ì•ˆ í•¨
                raise ValueError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. use_groq ë˜ëŠ” use_ollamaë¥¼ Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            
            # JSON ì¶”ì¶œ ë° íŒŒì‹± (Groqì™€ Ollama ëª¨ë‘ ê³µí†µ)
            logger.info(f"[LLM í˜¸ì¶œ] ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ì‘ë‹µ ê¸¸ì´: {len(response_text) if response_text else 0}ì")
            # [DEBUG] Groq raw output ì¶œë ¥
            logger.info(f"[DEBUG] Groq raw output (ì²˜ìŒ 500ì): {response_text[:500] if response_text else 'None'}")
            if response_text and len(response_text) > 1000:
                logger.info(f"[DEBUG] Groq raw output (ë§ˆì§€ë§‰ 500ì): ...{response_text[-500:]}")
            logger.info(f"[LLM í˜¸ì¶œ] ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 1000ì): {response_text[:1000] if response_text else 'None'}")
            if response_text and len(response_text) > 1000:
                logger.info(f"[LLM í˜¸ì¶œ] ì‘ë‹µ ì›ë¬¸ (ë§ˆì§€ë§‰ 500ì): ...{response_text[-500:]}")
            
            # JSON ì¶”ì¶œ (ë” robustí•œ íŒŒì‹±)
            try:
                # 1. ì½”ë“œ ë¸”ë¡ ì œê±°
                response_clean = response_text.strip()
                if response_clean.startswith("```json"):
                    response_clean = response_clean[7:]
                elif response_clean.startswith("```"):
                    response_clean = response_clean[3:]
                if response_clean.endswith("```"):
                    response_clean = response_clean[:-3]
                response_clean = response_clean.strip()
                
                # 2. JSON ê°ì²´ ì°¾ê¸° (ë” ì •í™•í•œ ì •ê·œì‹)
                json_match = re.search(r'\{[\s\S]*\}', response_clean, re.DOTALL)
                if json_match:
                    json_str = json_match.group()
                    logger.debug(f"[JSON íŒŒì‹±] ì¶”ì¶œëœ JSON ë¬¸ìì—´ ê¸¸ì´: {len(json_str)}ì")
                    logger.debug(f"[JSON íŒŒì‹±] JSON ë¬¸ìì—´ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì): {json_str[:500]}")
                    # 3. JSON ìœ íš¨ì„± ê²€ì‚¬ ë° íŒŒì‹±
                    try:
                        analysis = json.loads(json_str)
                        logger.info(f"[JSON íŒŒì‹±] âœ… JSON íŒŒì‹± ì„±ê³µ")
                    except json.JSONDecodeError as json_err:
                        # JSONì´ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ì • ì‹œë„
                        logger.warning(f"[JSON íŒŒì‹±] âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(json_err)}")
                        logger.warning(f"[JSON íŒŒì‹±] ì—ëŸ¬ ìœ„ì¹˜: line {json_err.lineno}, col {json_err.colno}")
                        logger.warning(f"[JSON íŒŒì‹±] ë¬¸ì œê°€ ìˆëŠ” ë¶€ë¶„: {json_str[max(0, json_err.pos-50):json_err.pos+50]}")
                        
                        # ë” robustí•œ JSON ë³µêµ¬ ì‹œë„
                        analysis = None
                        recovery_attempted = False
                        
                        # ë³µêµ¬ ë°©ë²• 1: ì¤‘ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ë¥¼ ëª¨ë‘ ì¶”ì í•˜ì—¬ ì™„ì „í•œ êµ¬ì¡° ì°¾ê¸° (ì—ëŸ¬ ìœ„ì¹˜ ì´ì „ê¹Œì§€)
                        try:
                            brace_count = 0
                            bracket_count = 0
                            in_string = False
                            escape_next = False
                            last_valid_pos = -1
                            
                            # ì—ëŸ¬ ìœ„ì¹˜ ì´ì „ê¹Œì§€ ì „ì²´ ë¬¸ìì—´ í™•ì¸
                            error_pos = min(json_err.pos, len(json_str))
                            
                            for i, char in enumerate(json_str[:error_pos]):
                                if escape_next:
                                    escape_next = False
                                    continue
                                
                                if char == '\\':
                                    escape_next = True
                                    continue
                                
                                if char == '"' and not escape_next:
                                    in_string = not in_string
                                    continue
                                
                                if not in_string:
                                    if char == '{':
                                        brace_count += 1
                                    elif char == '}':
                                        brace_count -= 1
                                        if brace_count == 0 and bracket_count == 0:
                                            last_valid_pos = i + 1
                                    elif char == '[':
                                        bracket_count += 1
                                    elif char == ']':
                                        bracket_count -= 1
                                        if brace_count == 0 and bracket_count == 0:
                                            last_valid_pos = i + 1
                            
                            if last_valid_pos > 0:
                                json_str_truncated = json_str[:last_valid_pos]
                                try:
                                    analysis = json.loads(json_str_truncated)
                                    logger.warning(f"[JSON íŒŒì‹±] âš ï¸ ë°©ë²•1: ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ë³µêµ¬ ì„±ê³µ (ì›ë³¸: {len(json_str)}ì, ë³µêµ¬: {len(json_str_truncated)}ì)")
                                    recovery_attempted = True
                                except Exception as parse_err:
                                    logger.debug(f"[JSON íŒŒì‹±] ë°©ë²•1 ë³µêµ¬ í›„ íŒŒì‹± ì‹¤íŒ¨: {str(parse_err)}")
                                    # ë³µêµ¬ëœ ë¬¸ìì—´ì´ ì—¬ì „íˆ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ë‹¤ìŒ ë°©ë²• ì‹œë„
                                    pass
                        except Exception as e:
                            logger.debug(f"[JSON íŒŒì‹±] ë³µêµ¬ ë°©ë²•1 ì‹¤íŒ¨: {str(e)}")
                        
                        # ë³µêµ¬ ë°©ë²• 2: ì—ëŸ¬ ìœ„ì¹˜ ì´ì „ì—ì„œ ì™„ì „í•œ issues ë°°ì—´ ì°¾ê¸°
                        if analysis is None:
                            try:
                                # issues ë°°ì—´ì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
                                issues_start = json_str.find('"issues"')
                                if issues_start != -1 and issues_start < json_err.pos:
                                    # issues ë°°ì—´ ì‹œì‘ë¶€í„° ì—ëŸ¬ ìœ„ì¹˜ê¹Œì§€ ì¶”ì¶œ
                                    issues_section = json_str[issues_start:json_err.pos]
                                    
                                    # ì™„ì „í•œ issue ê°ì²´ë“¤ì„ ì°¾ê¸° (ì¤‘ê´„í˜¸ ë§¤ì¹­)
                                    brace_count = 0
                                    bracket_count = 0
                                    in_string = False
                                    escape_next = False
                                    last_complete_issue_end = -1
                                    
                                    issue_start = issues_section.find('[')
                                    if issue_start != -1:
                                        bracket_count = 1
                                        for i in range(issue_start + 1, len(issues_section)):
                                            char = issues_section[i]
                                            
                                            if escape_next:
                                                escape_next = False
                                                continue
                                            
                                            if char == '\\':
                                                escape_next = True
                                                continue
                                            
                                            if char == '"' and not escape_next:
                                                in_string = not in_string
                                                continue
                                            
                                            if not in_string:
                                                if char == '{':
                                                    brace_count += 1
                                                elif char == '}':
                                                    brace_count -= 1
                                                    if brace_count == 0:
                                                        last_complete_issue_end = i + 1
                                                elif char == '[':
                                                    bracket_count += 1
                                                elif char == ']':
                                                    bracket_count -= 1
                                        
                                        if last_complete_issue_end > 0:
                                            # issues ë°°ì—´ì„ ë‹«ê³  ë‚˜ë¨¸ì§€ í•„ë“œ ì¶”ê°€
                                            json_str_fixed = json_str[:issues_start + issue_start + last_complete_issue_end]
                                            # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ê°ì²´ ì œê±°
                                            json_str_fixed = re.sub(r',\s*\{[^}]*$', '', json_str_fixed)
                                            json_str_fixed += '\n  ]\n}'
                                            
                                            try:
                                                analysis = json.loads(json_str_fixed)
                                                logger.warning(f"[JSON íŒŒì‹±] âš ï¸ ë°©ë²•2: issues ë°°ì—´ ë³µêµ¬ë¡œ JSON íŒŒì‹± ì„±ê³µ")
                                                recovery_attempted = True
                                            except:
                                                pass
                            except Exception as e:
                                logger.debug(f"[JSON íŒŒì‹±] ë³µêµ¬ ë°©ë²•2 ì‹¤íŒ¨: {str(e)}")
                        
                        # ë³µêµ¬ ë°©ë²• 3: ì—ëŸ¬ ìœ„ì¹˜ ì´ì „ì˜ ì™„ì „í•œ êµ¬ì¡°ë§Œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ í•„ë“œ ì¶”ê°€
                        if analysis is None:
                            try:
                                # ì—ëŸ¬ ìœ„ì¹˜ ì´ì „ì—ì„œ ì™„ì „í•œ í•„ë“œë“¤ë§Œ ì¶”ì¶œ
                                error_pos = json_err.pos
                                
                                # ë§ˆì§€ë§‰ ì™„ì „í•œ í•„ë“œ ì°¾ê¸° (ì‰¼í‘œë¡œ êµ¬ë¶„)
                                last_comma = json_str.rfind(',', 0, error_pos)
                                if last_comma > 0:
                                    # ë§ˆì§€ë§‰ ì‰¼í‘œ ì´ì „ê¹Œì§€ê°€ ì™„ì „í•œ êµ¬ì¡°
                                    json_str_fixed = json_str[:last_comma]
                                    
                                    # issues ë°°ì—´ì´ ì—´ë ¤ìˆìœ¼ë©´ ë‹«ê¸°
                                    if '"issues"' in json_str_fixed:
                                        issues_start = json_str_fixed.find('"issues"')
                                        issues_array_start = json_str_fixed.find('[', issues_start)
                                        if issues_array_start != -1:
                                            # issues ë°°ì—´ ë‹«ê¸°
                                            json_str_fixed = json_str_fixed[:issues_array_start]
                                            # ì™„ì „í•œ issues í•­ëª©ë“¤ ì°¾ê¸°
                                            issues_match = re.search(r'"issues"\s*:\s*\[([\s\S]*?)\]', json_str[:error_pos])
                                            if issues_match:
                                                json_str_fixed += f'\n  "issues": [{issues_match.group(1)}]\n'
                                            else:
                                                json_str_fixed += '\n  "issues": []\n'
                                    
                                    json_str_fixed += '}'
                                    
                                    try:
                                        analysis = json.loads(json_str_fixed)
                                        logger.warning(f"[JSON íŒŒì‹±] âš ï¸ ë°©ë²•3: ë§ˆì§€ë§‰ ì™„ì „í•œ í•„ë“œê¹Œì§€ ë³µêµ¬ ì„±ê³µ")
                                        recovery_attempted = True
                                    except:
                                        pass
                            except Exception as e:
                                logger.debug(f"[JSON íŒŒì‹±] ë³µêµ¬ ë°©ë²•3 ì‹¤íŒ¨: {str(e)}")
                        
                        # ë³µêµ¬ ë°©ë²• 4: ìµœì†Œí•œì˜ í•„ìˆ˜ í•„ë“œë§Œ í¬í•¨í•˜ì—¬ ë³µêµ¬
                        if analysis is None:
                            try:
                                # í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œ
                                risk_score_match = re.search(r'"risk_score"\s*:\s*(\d+)', json_str)
                                risk_level_match = re.search(r'"risk_level"\s*:\s*"([^"]+)"', json_str)
                                summary_match = re.search(r'"summary"\s*:\s*"([^"]*)"', json_str, re.DOTALL)
                                
                                if risk_score_match and risk_level_match and summary_match:
                                    json_str_fixed = '{\n'
                                    json_str_fixed += f'  "risk_score": {risk_score_match.group(1)},\n'
                                    json_str_fixed += f'  "risk_level": "{risk_level_match.group(1)}",\n'
                                    # summaryëŠ” ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                                    summary_text = summary_match.group(1).replace('\\', '\\\\').replace('"', '\\"').replace('\n', '\\n')
                                    json_str_fixed += f'  "summary": "{summary_text}",\n'
                                    json_str_fixed += '  "issues": []\n'
                                    json_str_fixed += '}'
                                    
                                    try:
                                        analysis = json.loads(json_str_fixed)
                                        logger.warning(f"[JSON íŒŒì‹±] âš ï¸ ë°©ë²•4: í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ë³µêµ¬ ì„±ê³µ (issuesëŠ” ë¹ˆ ë°°ì—´)")
                                        recovery_attempted = True
                                    except:
                                        pass
                            except Exception as e:
                                logger.debug(f"[JSON íŒŒì‹±] ë³µêµ¬ ë°©ë²•4 ì‹¤íŒ¨: {str(e)}")
                        
                        # ëª¨ë“  ë³µêµ¬ ì‹œë„ ì‹¤íŒ¨
                        if analysis is None:
                            logger.error(f"[JSON íŒŒì‹±] âŒ ëª¨ë“  JSON ë³µêµ¬ ì‹œë„ ì‹¤íŒ¨")
                            logger.error(f"[JSON íŒŒì‹±] LLM ì‘ë‹µ ì›ë¬¸ ì „ì²´ ê¸¸ì´: {len(response_text)}ì")
                            logger.error(f"[JSON íŒŒì‹±] LLM ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 2000ì): {response_text[:2000] if response_text else 'None'}")
                            if response_text and len(response_text) > 2000:
                                logger.error(f"[JSON íŒŒì‹±] LLM ì‘ë‹µ ì›ë¬¸ (ë§ˆì§€ë§‰ 1000ì): ...{response_text[-1000:]}")
                            
                            # ë°œê²¬ëœ JSON ê°ì²´ ê°œìˆ˜ í™•ì¸ (ë””ë²„ê¹…ìš©)
                            json_objects = re.findall(r'\{[^{}]*\}', json_str)
                            logger.error(f"[JSON íŒŒì‹±] ë°œê²¬ëœ JSON ê°ì²´ ê°œìˆ˜: {len(json_objects)}")
                            if json_objects:
                                logger.error(f"[JSON íŒŒì‹±] ì²« ë²ˆì§¸ JSON ê°ì²´ (ì²˜ìŒ 500ì): {json_objects[0][:500]}")
                            
                            raise json_err
                    risk_score = analysis.get("risk_score", 50)
                    risk_level = analysis.get("risk_level", "medium")
                    summary = analysis.get("summary", "")
                    
                    logger.info(f"[LLM ì‘ë‹µ íŒŒì‹±] âœ… JSON íŒŒì‹± ì„±ê³µ: risk_score={risk_score}, risk_level={risk_level}, summary ê¸¸ì´={len(summary)}")
                    logger.info(f"[LLM ì‘ë‹µ íŒŒì‹±] issues ë°°ì—´ ê¸¸ì´: {len(analysis.get('issues', []))}")
                    
                    # [DEBUG] rawIssues í™•ì¸
                    raw_issues = analysis.get("issues", [])
                    logger.info(f"[DEBUG] rawIssues ê°œìˆ˜: {len(raw_issues)}")
                    if not raw_issues:
                        logger.warning(f"[DEBUG] âš ï¸ issues ë°°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                        logger.warning(f"[DEBUG] analysis í‚¤ ëª©ë¡: {list(analysis.keys())}")
                        logger.warning(f"[DEBUG] analysis ì „ì²´ ë‚´ìš© (ì²˜ìŒ 1000ì): {str(analysis)[:1000]}")
                    else:
                        logger.info(f"[DEBUG] rawIssues[0] ìƒ˜í”Œ: {raw_issues[0]}")
                        logger.info(f"[DEBUG] rawIssues[0] í‚¤ ëª©ë¡: {list(raw_issues[0].keys()) if isinstance(raw_issues[0], dict) else 'N/A'}")
                    
                    issues = []
                    for idx, issue_data in enumerate(raw_issues):
                        logger.debug(f"[DEBUG] issue[{idx}] íŒŒì‹± ì‹œì‘: {issue_data}, íƒ€ì…: {type(issue_data)}")
                        
                        # issue_dataê°€ dictê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                        if not isinstance(issue_data, dict):
                            logger.warning(f"[DEBUG] issue[{idx}]ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤ (íƒ€ì…: {type(issue_data)}). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        
                        # ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ: issue_id, clause_id, category, summary, reason ë“±
                        # ë ˆê±°ì‹œ ìŠ¤í‚¤ë§ˆ: name, description, original_text ë“±
                        issue_id = issue_data.get("issue_id") or issue_data.get("name", f"issue-{idx+1}")
                        clause_id = issue_data.get("clause_id") or issue_data.get("clauseId")
                        category = issue_data.get("category", "unknown")
                        summary = issue_data.get("summary") or issue_data.get("description", "")
                        reason = issue_data.get("reason") or issue_data.get("rationale", "")
                        
                        # original_textëŠ” clause_id ê¸°ë°˜ìœ¼ë¡œ ë‚˜ì¤‘ì— ì±„ì›Œì§€ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¹ˆ ë¬¸ìì—´
                        # ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•´ original_textê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                        original_text = issue_data.get("original_text", "")
                        
                        # descriptionì€ summary ë˜ëŠ” reasonìœ¼ë¡œ ëŒ€ì²´
                        description = summary or reason
                        
                        # toxic_clause_detail íŒŒì‹±
                        toxic_clause_detail = None
                        toxic_detail_data = issue_data.get("toxic_clause_detail")
                        if toxic_detail_data and isinstance(toxic_detail_data, dict):
                            try:
                                from models.schemas import ToxicClauseDetail
                                toxic_clause_detail = ToxicClauseDetail(
                                    clauseLocation=toxic_detail_data.get("clause_location", ""),
                                    contentSummary=toxic_detail_data.get("content_summary", ""),
                                    whyRisky=toxic_detail_data.get("why_risky", ""),
                                    realWorldProblems=toxic_detail_data.get("real_world_problems", ""),
                                    suggestedRevisionLight=toxic_detail_data.get("suggested_revision_light", ""),
                                    suggestedRevisionFormal=toxic_detail_data.get("suggested_revision_formal", ""),
                                )
                            except Exception as toxic_err:
                                logger.warning(f"[LLM ì‘ë‹µ íŒŒì‹±] issue[{idx}] toxic_clause_detail ë³€í™˜ ì‹¤íŒ¨: {str(toxic_err)}")
                        
                        logger.debug(f"[DEBUG] issue[{idx}] ì¶”ì¶œëœ í•„ë“œ: issue_id={issue_id}, clause_id={clause_id}, category={category}, summary ê¸¸ì´={len(summary)}")
                        
                        # ê³„ì•½ì„œ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì¡°í•­ ìœ„ì¹˜ ì°¾ê¸°
                        # ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ì—ì„œëŠ” clause_id ê¸°ë°˜ìœ¼ë¡œ original_textë¥¼ ë‚˜ì¤‘ì— ì±„ìš°ë¯€ë¡œ
                        # ì—¬ê¸°ì„œëŠ” start_index/end_indexë¥¼ Noneìœ¼ë¡œ ì„¤ì •
                        start_index = None
                        end_index = None
                        
                        # ë ˆê±°ì‹œ í˜¸í™˜ì„±: original_textê°€ ìˆê³  contract_textê°€ ìˆìœ¼ë©´ ìœ„ì¹˜ ì°¾ê¸° ì‹œë„
                        if contract_text and original_text and isinstance(original_text, str):
                            try:
                                # original_textë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ìœ„ì¹˜ ì°¾ê¸°
                                start_index = contract_text.find(original_text)
                                if start_index >= 0:
                                    end_index = start_index + len(original_text)
                                else:
                                    # ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                                    if len(original_text) > 100:
                                        # 1. ì²˜ìŒ 100ìë¡œ ê²€ìƒ‰
                                        start_index = contract_text.find(original_text[:100])
                                        if start_index >= 0:
                                            end_index = start_index + len(original_text)
                                    if start_index is None and len(original_text) > 50:
                                        # 2. ì²˜ìŒ 50ìë¡œ ê²€ìƒ‰
                                        start_index = contract_text.find(original_text[:50])
                                        if start_index >= 0:
                                            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ í™•ì¥
                                            end_pos = min(start_index + len(original_text), len(contract_text))
                                            while end_pos < len(contract_text) and contract_text[end_pos] not in ['\n', 'ã€‚', '.']:
                                                end_pos += 1
                                            end_index = end_pos
                                    if start_index is None:
                                        logger.debug(f"[LLM ì‘ë‹µ íŒŒì‹±] originalTextë¥¼ ê³„ì•½ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ (clause_id ê¸°ë°˜ìœ¼ë¡œ ë‚˜ì¤‘ì— ì±„ì›Œì§): {original_text[:50] if isinstance(original_text, str) else original_text}...")
                            except Exception as find_err:
                                logger.warning(f"[LLM ì‘ë‹µ íŒŒì‹±] originalText ìœ„ì¹˜ ì°¾ê¸° ì‹¤íŒ¨: {str(find_err)}")
                                # ì—ëŸ¬ê°€ ë‚˜ë„ ê³„ì† ì§„í–‰ (clause_id ê¸°ë°˜ìœ¼ë¡œ ë‚˜ì¤‘ì— ì±„ì›Œì§)
                        
                        try:
                            issue_obj = LegalIssue(
                                name=issue_id,  # issue_idë¥¼ name í•„ë“œì— ì €ì¥ (ë ˆê±°ì‹œ í˜¸í™˜)
                                description=description,  # summary ë˜ëŠ” reasonì„ descriptionì— ì €ì¥
                                severity=issue_data.get("severity", "medium"),
                                legal_basis=issue_data.get("legal_basis", []),
                                start_index=start_index,
                                end_index=end_index,
                                suggested_text=issue_data.get("suggested_revision") or issue_data.get("suggested_text"),
                                rationale=reason or issue_data.get("rationale"),
                                suggested_questions=issue_data.get("suggested_questions", []),
                                original_text=original_text,  # original_text í•„ë“œ ì¶”ê°€
                                clause_id=clause_id,  # clause_id í•„ë“œ ì¶”ê°€ (ìƒˆ ìŠ¤í‚¤ë§ˆ)
                                category=category,  # category í•„ë“œ ì¶”ê°€ (ìƒˆ ìŠ¤í‚¤ë§ˆ)
                                summary=summary,  # summary í•„ë“œ ì¶”ê°€ (ìƒˆ ìŠ¤í‚¤ë§ˆ)
                                toxic_clause_detail=toxic_clause_detail,  # toxic_clause_detail ì¶”ê°€
                            )
                            issues.append(issue_obj)
                            logger.debug(f"[LLM ì‘ë‹µ íŒŒì‹±] issue[{len(issues)}]: name={issue_obj.name[:50]}, clause_id={clause_id}, severity={issue_obj.severity}, description ê¸¸ì´={len(description)}")
                        except Exception as issue_create_err:
                            logger.error(f"[LLM ì‘ë‹µ íŒŒì‹±] issue[{idx}] LegalIssue ìƒì„± ì‹¤íŒ¨: {str(issue_create_err)}", exc_info=True)
                            # ê°œë³„ issue ìƒì„± ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            continue
                    
                    # [DEBUG] normalizedDataIssues í™•ì¸ (ì´ ë‹¨ê³„ì—ì„œëŠ” ì•„ì§ ì •ê·œí™” ì „ì´ë¯€ë¡œ rawIssuesì™€ ë™ì¼)
                    logger.info(f"[DEBUG] normalizedDataIssues (rawIssuesì™€ ë™ì¼): {len(issues)}ê°œ")
                    logger.info(f"[LLM ì‘ë‹µ íŒŒì‹±] ìµœì¢… ì´ìŠˆ ê°œìˆ˜: {len(issues)}ê°œ")
                    
                    # ê° ì´ìŠˆë³„ë¡œ legal ê²€ìƒ‰ ìˆ˜í–‰ (ì´ìŠˆ ì¤‘ì‹¬ ì¿¼ë¦¬ ì‚¬ìš©)
                    logger.info(f"[ë²•ë ¹ ê²€ìƒ‰] ì´ìŠˆë³„ legal ê²€ìƒ‰ ì‹œì‘: {len(issues)}ê°œ ì´ìŠˆ")
                    for issue in issues:
                        try:
                            # ì´ìŠˆ ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„±
                            issue_dict = {
                                "original_text": issue.original_text or "",
                                "clause_text": issue.original_text or "",
                                "rationale": issue.rationale or issue.description or "",
                                "category": issue.category or "",
                                "summary": issue.summary or issue.description or "",
                            }
                            issue_query = self._build_query_from_issue(issue_dict)
                            
                            # ì´ìŠˆë³„ legal ê²€ìƒ‰ (category í•„í„° ì ìš©, boilerplate ì œì™¸)
                            issue_legal_chunks = await self._search_legal_chunks(
                                query=issue_query,
                                top_k=5,  # ì´ìŠˆë³„ë¡œ 5ê°œë§Œ
                                category=issue.category,  # category í•„í„° ì ìš©
                                ensure_diversity=False,  # ì´ìŠˆë³„ ê²€ìƒ‰ì´ë¯€ë¡œ ë‹¤ì–‘ì„± í™•ë³´ ë¶ˆí•„ìš”
                            )
                            
                            # legal_basisë¥¼ ì´ìŠˆë³„ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                            if issue_legal_chunks:
                                from models.schemas import LegalBasisItemV2
                                issue_legal_basis = []
                                for chunk in issue_legal_chunks:
                                    # file_pathê°€ ì—†ìœ¼ë©´ external_idë¡œ ìƒì„±
                                    file_path = chunk.file_path
                                    if not file_path and chunk.external_id:
                                        file_path = self._build_file_path(chunk.source_type, chunk.external_id)
                                    
                                    # reason ìƒì„± (ì„ íƒì , LLM ì‚¬ìš©)
                                    reason = None
                                    try:
                                        reason = await self._build_reason(
                                            issue_summary=issue.summary or issue.description or "",
                                            clause_text=issue.original_text or "",
                                            basis_snippet=chunk.snippet,
                                        )
                                    except Exception as reason_err:
                                        logger.debug(f"[ë²•ë ¹ ê²€ìƒ‰] reason ìƒì„± ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {str(reason_err)}")
                                    
                                    issue_legal_basis.append(
                                        LegalBasisItemV2(
                                            title=chunk.title,
                                            snippet=chunk.snippet,
                                            sourceType=chunk.source_type,
                                            status="unclear",  # LLMì´ íŒë‹¨í•œ statusê°€ ìˆë‹¤ë©´ ì‚¬ìš©
                                            filePath=file_path,  # ìŠ¤í† ë¦¬ì§€ í‚¤
                                            similarityScore=chunk.score,  # ë²¡í„° ìœ ì‚¬ë„
                                            chunkIndex=chunk.chunk_index,  # ì²­í¬ ì¸ë±ìŠ¤
                                            externalId=chunk.external_id,  # external_id
                                            reason=reason,  # LLMìœ¼ë¡œ ìƒì„±í•œ ì´ìœ  ì„¤ëª…
                                        )
                                    )
                                # ê¸°ì¡´ legal_basisê°€ ìˆìœ¼ë©´ ë³‘í•© (ì´ìŠˆë³„ ê²€ìƒ‰ ê²°ê³¼ ìš°ì„ )
                                if issue.legal_basis:
                                    # ê¸°ì¡´ legal_basisê°€ ë¬¸ìì—´ ë°°ì—´ì´ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                                    if isinstance(issue.legal_basis[0], str):
                                        issue.legal_basis = issue_legal_basis + issue.legal_basis
                                    else:
                                        issue.legal_basis = issue_legal_basis + list(issue.legal_basis)
                                else:
                                    issue.legal_basis = issue_legal_basis
                                
                                logger.debug(f"[ë²•ë ¹ ê²€ìƒ‰] ì´ìŠˆ '{issue.name[:30]}' ({issue.category}): {len(issue_legal_chunks)}ê°œ ë²•ë ¹ ê²€ìƒ‰ë¨")
                            else:
                                logger.debug(f"[ë²•ë ¹ ê²€ìƒ‰] ì´ìŠˆ '{issue.name[:30]}' ({issue.category}): ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (threshold ë¯¸ë§Œ ë˜ëŠ” í•„í„°ë§ë¨)")
                        except Exception as issue_search_err:
                            logger.warning(f"[ë²•ë ¹ ê²€ìƒ‰] ì´ìŠˆ '{issue.name[:30]}' legal ê²€ìƒ‰ ì‹¤íŒ¨: {str(issue_search_err)}")
                            # ì´ìŠˆë³„ ê²€ìƒ‰ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            continue
                    
                    logger.info(f"[ë²•ë ¹ ê²€ìƒ‰] ì´ìŠˆë³„ legal ê²€ìƒ‰ ì™„ë£Œ")
                    
                    recommendations = []
                    for rec_data in analysis.get("recommendations", []):
                        # rec_dataê°€ dictê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                        if not isinstance(rec_data, dict):
                            logger.warning(f"[LLM ì‘ë‹µ íŒŒì‹±] recommendationì´ dictê°€ ì•„ë‹™ë‹ˆë‹¤ (íƒ€ì…: {type(rec_data)}). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        recommendations.append(LegalRecommendation(
                            title=rec_data.get("title", ""),
                            description=rec_data.get("description", ""),
                            steps=rec_data.get("steps", [])
                        ))
                    
                    # ìƒˆë¡œìš´ ë…ì†Œì¡°í•­ íƒì§€ í•„ë“œ íŒŒì‹±
                    one_line_summary = analysis.get("one_line_summary")
                    risk_traffic_light = analysis.get("risk_traffic_light")
                    top3_action_points = analysis.get("top3_action_points", [])
                    negotiation_questions = analysis.get("negotiation_questions", [])
                    
                    # risk_summary_table íŒŒì‹±
                    risk_summary_table = []
                    for item_data in analysis.get("risk_summary_table", []):
                        if isinstance(item_data, dict):
                            from models.schemas import RiskSummaryItem
                            try:
                                risk_summary_table.append(RiskSummaryItem(
                                    item=item_data.get("item", ""),
                                    riskLevel=item_data.get("risk_level", "medium"),
                                    problemPoint=item_data.get("problem_point", ""),
                                    simpleExplanation=item_data.get("simple_explanation", ""),
                                    revisionKeyword=item_data.get("revision_keyword", ""),
                                ))
                            except Exception as risk_item_err:
                                logger.warning(f"[LLM ì‘ë‹µ íŒŒì‹±] risk_summary_table í•­ëª© ë³€í™˜ ì‹¤íŒ¨: {str(risk_item_err)}")
                    
                    # toxic_clauses íŒŒì‹±
                    toxic_clauses = []
                    for toxic_data in analysis.get("toxic_clauses", []):
                        if isinstance(toxic_data, dict):
                            from models.schemas import ToxicClauseDetail
                            try:
                                toxic_clauses.append(ToxicClauseDetail(
                                    clauseLocation=toxic_data.get("clause_location", ""),
                                    contentSummary=toxic_data.get("content_summary", ""),
                                    whyRisky=toxic_data.get("why_risky", ""),
                                    realWorldProblems=toxic_data.get("real_world_problems", ""),
                                    suggestedRevisionLight=toxic_data.get("suggested_revision_light", ""),
                                    suggestedRevisionFormal=toxic_data.get("suggested_revision_formal", ""),
                                ))
                            except Exception as toxic_err:
                                logger.warning(f"[LLM ì‘ë‹µ íŒŒì‹±] toxic_clause ë³€í™˜ ì‹¤íŒ¨: {str(toxic_err)}")
                    
                    # â‘  ê·œì¹™ ê¸°ë°˜ ê°•ì œ ì´ìŠˆ ì¶”ê°€ (ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸° íŒ¨í„´)
                    try:
                        self._ensure_wage_waiver_issue(
                            contract_text=contract_text or "",
                            issues=issues,
                        )
                    except Exception as ensure_err:
                        logger.warning(f"[í›„ì²˜ë¦¬] ë²•ì • ìˆ˜ë‹¹ ì²­êµ¬ê¶Œ í¬ê¸° ì´ìŠˆ ë³´ì • ì¤‘ ì˜¤ë¥˜: {str(ensure_err)}", exc_info=True)
                    
                    result = LegalAnalysisResult(
                        risk_score=risk_score,
                        risk_level=risk_level,
                        summary=summary,
                        issues=issues,  # ë¹ˆ ë°°ì—´ì´ì–´ë„ ë°˜í™˜ (ìµœì†Œí•œ í‚¤ëŠ” ì±„ì›Œì¤Œ)
                        recommendations=recommendations,
                        grounding=grounding_chunks,
                        one_line_summary=one_line_summary,
                        risk_traffic_light=risk_traffic_light,
                        top3_action_points=top3_action_points,
                        risk_summary_table=risk_summary_table,
                        toxic_clauses=toxic_clauses,
                        negotiation_questions=negotiation_questions,
                    )
                    
                    # [DEBUG] validIssues í™•ì¸ (ì´ ë‹¨ê³„ì—ì„œëŠ” issuesì™€ ë™ì¼)
                    logger.info(f"[DEBUG] validIssues (issuesì™€ ë™ì¼): {len(issues)}ê°œ")
                    logger.info(f"[LLM ì‘ë‹µ íŒŒì‹±] ìµœì¢… ê²°ê³¼:")
                    logger.info(f"  - risk_score: {result.risk_score}, risk_level: {result.risk_level}")
                    logger.info(f"  - summary: {result.summary[:100]}..." if len(result.summary) > 100 else f"  - summary: {result.summary}")
                    logger.info(f"  - issues ê°œìˆ˜: {len(result.issues)}")
                    logger.info(f"  - recommendations ê°œìˆ˜: {len(result.recommendations)}")
                    logger.info(f"  - grounding_chunks ê°œìˆ˜: {len(result.grounding)}")
                    for idx, issue in enumerate(result.issues[:3]):  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…
                        logger.info(f"  - issue[{idx}]: name={issue.name[:50]}, severity={issue.severity}, description ê¸¸ì´={len(issue.description)}")
                    
                    return result
                else:
                    # json_matchê°€ Noneì¸ ê²½ìš°
                    raise ValueError("JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"[ERROR] âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}", exc_info=True)
                logger.error(f"[ERROR] ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
                logger.error(f"[ERROR] LLM ì‘ë‹µ ì›ë¬¸ ì „ì²´ ê¸¸ì´: {len(response_text) if response_text else 0}ì")
                logger.error(f"[ERROR] LLM ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 2000ì): {response_text[:2000] if response_text else 'None'}")
                if response_text and len(response_text) > 2000:
                    logger.error(f"[ERROR] LLM ì‘ë‹µ ì›ë¬¸ (ë§ˆì§€ë§‰ 1000ì): ...{response_text[-1000:]}")
                # JSON ê°ì²´ê°€ ìˆëŠ”ì§€ í™•ì¸
                if response_text:
                    json_objects = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response_text, re.DOTALL)
                    logger.error(f"[ERROR] ë°œê²¬ëœ JSON ê°ì²´ ê°œìˆ˜: {len(json_objects)}")
                    if json_objects:
                        logger.error(f"[ERROR] ì²« ë²ˆì§¸ JSON ê°ì²´ (ì²˜ìŒ 500ì): {json_objects[0][:500]}")
                
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ìµœì†Œí•œì˜ ì •ë³´ ì¶”ì¶œ ì‹œë„ (issuesëŠ” ë¹ˆ ë°°ì—´ë¡œ ë°˜í™˜)
                try:
                    # risk_score, risk_level, summaryë§Œì´ë¼ë„ ì¶”ì¶œ ì‹œë„
                    risk_score_match = re.search(r'"risk_score"\s*:\s*(\d+)', response_text)
                    risk_level_match = re.search(r'"risk_level"\s*:\s*"([^"]+)"', response_text)
                    summary_match = re.search(r'"summary"\s*:\s*"([^"]+)"', response_text)
                    
                    risk_score = int(risk_score_match.group(1)) if risk_score_match else 50
                    risk_level = risk_level_match.group(1) if risk_level_match else "medium"
                    summary = summary_match.group(1) if summary_match else f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” {len(grounding_chunks)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
                    
                    # [DEBUG] íŒŒì‹± ì‹¤íŒ¨ ì‹œ issuesëŠ” ë¹ˆ ë°°ì—´
                    logger.warning(f"[DEBUG] íŒŒì‹± ì‹¤íŒ¨ë¡œ ì¸í•´ issuesëŠ” ë¹ˆ ë°°ì—´ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.")
                    
                    # issues ë°°ì—´ì—ì„œ ìµœì†Œí•œì˜ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    issues = []
                    issues_matches = re.finditer(r'\{\s*"name"\s*:\s*"([^"]+)"\s*,\s*"description"\s*:\s*"([^"]+)"\s*,\s*"severity"\s*:\s*"([^"]+)"', response_text)
                    for match in issues_matches:
                        issues.append(LegalIssue(
                            name=match.group(1),
                            description=match.group(2),
                            severity=match.group(3),
                            legal_basis=[],
                            suggested_text=None,
                            rationale=None,
                            suggested_questions=[]
                        ))
                    
                    if issues:
                        logger.info(f"íŒŒì‹± ì‹¤íŒ¨í–ˆì§€ë§Œ {len(issues)}ê°œ ì´ìŠˆë¥¼ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
                    
                    return LegalAnalysisResult(
                        risk_score=risk_score,
                        risk_level=risk_level,
                        summary=summary,
                        issues=issues,
                        recommendations=[],
                        grounding=grounding_chunks,
                    )
                except Exception as fallback_error:
                    logger.error(f"Fallback íŒŒì‹±ë„ ì‹¤íŒ¨: {str(fallback_error)}")
                    # ìµœì¢… fallback: ë¹ˆ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                    return LegalAnalysisResult(
                        risk_score=50,
                        risk_level="medium",
                        summary=f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” {len(grounding_chunks)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                        issues=[],
                        recommendations=[],
                        grounding=grounding_chunks,
                    )
        except Exception as e:
            logger.error(f"[LLM í˜¸ì¶œ] LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            logger.error(f"[LLM í˜¸ì¶œ] ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
        
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ì´ìŠˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì—ëŸ¬ ì²˜ë¦¬)
        logger.warning(f"[LLM í˜¸ì¶œ] LLM í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜: RAG ê²€ìƒ‰ ê²°ê³¼ {len(grounding_chunks)}ê°œ")
        return LegalAnalysisResult(
            risk_score=50,
            risk_level="medium",
            summary=f"LLM ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” {len(grounding_chunks)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            issues=[],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ë”ë¯¸ ì´ìŠˆ ì œê±°)
            recommendations=[],
            grounding=grounding_chunks,
        )

    async def _llm_chat_response(
        self,
        query: str,
        contract_chunks: Optional[List[dict]] = None,
        legal_chunks: Optional[List[LegalGroundingChunk]] = None,
        grounding_chunks: Optional[List[LegalGroundingChunk]] = None,  # ë ˆê±°ì‹œ í˜¸í™˜
        selected_issue: Optional[dict] = None,
        analysis_summary: Optional[str] = None,
        risk_score: Optional[int] = None,
        total_issues: Optional[int] = None,
        context_type: Optional[str] = None,
        context_data: Optional[dict] = None,
    ) -> str:
        """
        ë²•ë¥  ìƒë‹´ ì±—ìš© LLM ì‘ë‹µ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ì§€ì›)
        
        Args:
            contract_chunks: ê³„ì•½ì„œ ë‚´ë¶€ ì²­í¬ (ìƒˆë¡œìš´ ë°©ì‹)
            legal_chunks: ë²•ë ¹ ì²­í¬ (ìƒˆë¡œìš´ ë°©ì‹)
            grounding_chunks: ë²•ë ¹ ì²­í¬ (ë ˆê±°ì‹œ í˜¸í™˜)
            context_type: ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ('none' | 'situation' | 'contract')
            context_data: ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° (ìƒí™© ë¶„ì„ ë˜ëŠ” ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸)
        """
        if self.generator.disable_llm:
            # LLM ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ì‘ë‹µ
            total_chunks = len(legal_chunks or grounding_chunks or []) + len(contract_chunks or [])
            return f"LLM ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” {total_chunks}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        
        # ê³„ì•½ì„œ ì²­í¬ ì¶”ê°€
        if contract_chunks:
            context_parts.append("=== ê³„ì•½ì„œ ë‚´ìš© ===")
            for chunk in contract_chunks[:3]:  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
                article_num = chunk.get("article_number", "")
                content = chunk.get("content", "")[:500]  # 500ìë¡œ ì œí•œ
                context_parts.append(f"ì œ{article_num}ì¡°:\n{content}")
        
        # ë²•ë ¹ ì²­í¬ ì¶”ê°€
        chunks_to_use = legal_chunks or grounding_chunks or []
        if chunks_to_use:
            context_parts.append("\n=== ê´€ë ¨ ë²•ë ¹/ê°€ì´ë“œë¼ì¸ ===")
            for chunk in chunks_to_use[:5]:  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
                context_parts.append(
                    f"[{chunk.source_type}] {chunk.title}\n{chunk.snippet}"
                )
        context = "\n\n".join(context_parts)
        
        # ì„ íƒëœ ì´ìŠˆ ì •ë³´ ì¶”ê°€
        issue_context = ""
        if selected_issue:
            # [DEBUG] legalBasis íƒ€ì… í™•ì¸
            legal_basis_raw = selected_issue.get('legalBasis', [])
            logger.debug(f"[chat] selected_issue.legalBasis íƒ€ì…: {[type(x).__name__ for x in legal_basis_raw]}")
            logger.debug(f"[chat] selected_issue.legalBasis ìƒ˜í”Œ: {legal_basis_raw[:2] if legal_basis_raw else 'ì—†ìŒ'}")
            
            # legalBasis ì²˜ë¦¬: string[] ë˜ëŠ” LegalBasisItemV2[] í˜•ì‹ ëª¨ë‘ ì§€ì›
            legal_basis_list = legal_basis_raw
            legal_basis_texts = []
            for basis in legal_basis_list[:3]:
                try:
                    if isinstance(basis, dict):
                        # LegalBasisItemV2 í˜•ì‹: { title, snippet, sourceType }
                        title = basis.get('title', '')
                        snippet = basis.get('snippet', '')
                        legal_basis_texts.append(title or snippet or str(basis))
                    elif isinstance(basis, str):
                        # string í˜•ì‹
                        legal_basis_texts.append(basis)
                    else:
                        # ê¸°íƒ€ í˜•ì‹ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                        legal_basis_texts.append(str(basis))
                except Exception as basis_err:
                    logger.warning(f"[chat] legalBasis í•­ëª© ë³€í™˜ ì‹¤íŒ¨: {str(basis_err)}, basis={basis}")
                    legal_basis_texts.append(str(basis) if basis else 'ì•Œ ìˆ˜ ì—†ìŒ')
            
            legal_basis_str = ', '.join(legal_basis_texts) if legal_basis_texts else 'ì—†ìŒ'
            logger.debug(f"[chat] legalBasis ë³€í™˜ ê²°ê³¼: {legal_basis_str}")
            
            issue_context = f"""
ì„ íƒëœ ìœ„í—˜ ì¡°í•­ ì •ë³´:
- ì¹´í…Œê³ ë¦¬: {selected_issue.get('category', 'ì•Œ ìˆ˜ ì—†ìŒ')}
- ìš”ì•½: {selected_issue.get('summary', '')}
- ìœ„í—˜ë„: {selected_issue.get('severity', 'medium')}
- ì¡°í•­ ë‚´ìš©: {selected_issue.get('originalText', '')[:500]}
- ê´€ë ¨ ë²•ë ¹: {legal_basis_str}
"""
        
        # ë¶„ì„ ìš”ì•½ ì •ë³´ ì¶”ê°€
        analysis_context = ""
        if analysis_summary:
            analysis_context = f"""
**ë¶„ì„ ìš”ì•½:**
{analysis_summary}
"""
        if risk_score is not None:
            analysis_context += f"\n**ìœ„í—˜ë„ ì ìˆ˜:** {risk_score}ì "
        if total_issues is not None:
            analysis_context += f"\n**ë°œê²¬ëœ ì´ìŠˆ ìˆ˜:** {total_issues}ê°œ"
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€ (ìƒí™© ë¶„ì„ ë˜ëŠ” ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸)
        context_report = ""
        if context_data and context_type:
            if context_type == 'situation':
                # ìƒí™© ë¶„ì„ ë¦¬í¬íŠ¸ ì»¨í…ìŠ¤íŠ¸
                situation_summary = context_data.get("summary", "")
                situation_risk = context_data.get("risk_score", 0)
                situation_criteria = context_data.get("criteria", [])
                situation_checklist = context_data.get("checklist", [])
                
                criteria_text = "\n".join([
                    f"- {c.get('name', '')}: {c.get('reason', '')}" 
                    for c in situation_criteria[:5]
                ]) if situation_criteria else "ì—†ìŒ"
                
                checklist_text = "\n".join([
                    f"- {item}" for item in situation_checklist[:5]
                ]) if situation_checklist else "ì—†ìŒ"
                
                context_report = f"""
**ğŸ“‹ í˜„ì¬ ì°¸ì¡° ì¤‘ì¸ ìƒí™© ë¶„ì„ ë¦¬í¬íŠ¸:**
- ìƒí™© ìš”ì•½: {situation_summary[:300]}
- ìœ„í—˜ë„ ì ìˆ˜: {situation_risk}ì 
- ë²•ì  íŒë‹¨ ê¸°ì¤€:
{criteria_text}
- ì²´í¬ë¦¬ìŠ¤íŠ¸:
{checklist_text}

ì´ ìƒí™© ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
            elif context_type == 'contract':
                # ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸ ì»¨í…ìŠ¤íŠ¸
                contract_summary = context_data.get("summary", "")
                contract_risk = context_data.get("risk_score", 0)
                contract_issues = context_data.get("issues", [])
                
                issues_text = "\n".join([
                    f"- [{issue.get('severity', 'medium')}] {issue.get('summary', '')}" 
                    for issue in contract_issues[:5]
                ]) if contract_issues else "ì—†ìŒ"
                
                context_report = f"""
**ğŸ“„ í˜„ì¬ ì°¸ì¡° ì¤‘ì¸ ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸:**
- ë¶„ì„ ìš”ì•½: {contract_summary[:300]}
- ìœ„í—˜ë„ ì ìˆ˜: {contract_risk}ì 
- ë°œê²¬ëœ ì´ìŠˆ:
{issues_text}

ì´ ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        # context_typeì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        if context_type == 'situation':
            # ìƒí™©ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            situation_criteria = context_data.get("criteria", []) if context_data else []
            situation_checklist = context_data.get("checklist", []) if context_data else []
            situation_related_cases = context_data.get("related_cases", []) if context_data else []
            
            prompt = build_situation_chat_prompt(
                query=query,
                legal_chunks=chunks_to_use,
                analysis_summary=analysis_summary,
                criteria=situation_criteria,
                checklist=situation_checklist,
                related_cases=situation_related_cases,
            )
        else:
            # ê³„ì•½ì„œ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ê¸°ë³¸)
            prompt = build_legal_chat_prompt(
                query=query,
                contract_chunks=contract_chunks,
                legal_chunks=chunks_to_use,
                selected_issue=selected_issue,
                analysis_summary=analysis_summary,
                risk_score=risk_score,
                total_issues=total_issues,
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            if context_report:
                # í”„ë¡¬í”„íŠ¸ ëë¶€ë¶„ì— ì»¨í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì¶”ê°€
                prompt = prompt.rstrip() + "\n\n" + context_report

        try:
            # Groq ì‚¬ìš© (ìš°ì„ )
            from config import settings
            if settings.use_groq:
                from llm_api import ask_groq_with_messages
                
                # í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                # promptëŠ” ì´ë¯¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ì´ë¯€ë¡œ, systemê³¼ userë¡œ ë¶„ë¦¬
                messages = [
                    {"role": "system", "content": "ë„ˆëŠ” ìœ ëŠ¥í•œ ë²•ë¥  AIì•¼. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ]
                
                response_text = ask_groq_with_messages(
                    messages=messages,
                    temperature=settings.llm_temperature,
                    model=settings.groq_model
                )
                
                # LLM ì¶œë ¥ ë¡œê¹…
                logger.info("=" * 80)
                logger.info("[LLM OUTPUT] Legal Chat Response")
                logger.info("=" * 80)
                logger.info(f"Response Length: {len(response_text)} characters")
                logger.info(f"Response Content:\n{response_text}")
                logger.info("=" * 80)
                
                # ìƒí™©ë¶„ì„ì¼ ë•ŒëŠ” ```json ì½”ë“œ ë¸”ë¡ í˜•ì‹ ê·¸ëŒ€ë¡œ ë°˜í™˜
                if context_type == 'situation':
                    # ```json ì½”ë“œ ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                    response_clean = response_text.strip()
                    if response_clean.startswith('```json') or response_clean.startswith('```'):
                        # ì´ë¯¸ ì½”ë“œ ë¸”ë¡ í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
                        logger.info(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] ì½”ë“œ ë¸”ë¡ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê¸¸ì´: {len(response_clean)} characters)")
                        return response_clean
                    else:
                        # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                        # JSON ê°ì²´ ì°¾ê¸°
                        first_brace = response_clean.find('{')
                        if first_brace != -1:
                            json_str = response_clean[first_brace:]
                            brace_count = 0
                            last_valid_pos = -1
                            for i, char in enumerate(json_str):
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        last_valid_pos = i + 1
                                        break
                            
                            if last_valid_pos > 0:
                                json_content = json_str[:last_valid_pos].strip()
                                # JSON ìœ íš¨ì„± ê²€ì¦
                                try:
                                    json.loads(json_content)
                                    logger.info(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] JSON ê²€ì¦ ì„±ê³µ, ì½”ë“œ ë¸”ë¡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
                                    return f"```json\n{json_content}\n```"
                                except json.JSONDecodeError:
                                    logger.warning(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜")
                                    return response_text
                        # JSONì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
                        logger.warning(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
                        return response_text
                
                # ê³„ì•½ì„œ ë¶„ì„ì¼ ë•Œë„ JSONë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
                if context_type == 'contract' or context_type == 'none':
                    # JSON ì¶”ì¶œ ë¡œì§ (ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì œê±°)
                    response_clean = response_text.strip()
                    
                    # 1. JSON ì½”ë“œ ë¸”ë¡ ì°¾ê¸° (```json ... ```) - ì²« ë²ˆì§¸ ê²ƒë§Œ
                    json_block_match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', response_clean, re.DOTALL)
                    if json_block_match:
                        response_clean = json_block_match.group(1).strip()
                    else:
                        # 2. ì§ì ‘ JSON ê°ì²´ ì°¾ê¸° (ì²« ë²ˆì§¸ { ... } ì¶”ì¶œ)
                        # ì¤‘ê´„í˜¸ ë§¤ì¹­í•˜ì—¬ ì™„ì „í•œ JSON ê°ì²´ ì¶”ì¶œ
                        first_brace = response_clean.find('{')
                        if first_brace != -1:
                            json_str = response_clean[first_brace:]
                            brace_count = 0
                            last_valid_pos = -1
                            for i, char in enumerate(json_str):
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        last_valid_pos = i + 1
                                        break
                            
                            if last_valid_pos > 0:
                                response_clean = json_str[:last_valid_pos].strip()
                            else:
                                # ì¤‘ê´„í˜¸ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ ì‹œë„
                                json_match = re.search(r'\{[\s\S]*\}', response_clean, re.DOTALL)
                                if json_match:
                                    response_clean = json_match.group(0).strip()
                    
                    # JSON ìœ íš¨ì„± ê²€ì¦
                    try:
                        parsed_json = json.loads(response_clean)
                        logger.info(f"[JSON ì¶”ì¶œ ì„±ê³µ] ì¶”ì¶œëœ JSON ê¸¸ì´: {len(response_clean)} characters")
                        logger.info(f"[JSON ì¶”ì¶œ ì„±ê³µ] summary: {parsed_json.get('summary', 'N/A')[:50]}...")
                        
                        # riskLevel ê°’ ì •ê·œí™” (ì˜ëª»ëœ ê°’ ìˆ˜ì •)
                        if "riskLevel" in parsed_json:
                            original_risk_level = parsed_json["riskLevel"]
                            valid_risk_levels = ["ê²½ë¯¸", "ë³´í†µ", "ë†’ìŒ", "ë§¤ìš° ë†’ìŒ", None]
                            
                            # ì˜ëª»ëœ ê°’ ë§¤í•‘
                            risk_level_mapping = {
                                "ì¤‘ë“±": "ë³´í†µ",
                                "ì¤‘ê°„": "ë³´í†µ",
                                "ë‚®ìŒ": "ê²½ë¯¸",
                                "ë³´í†µ ì´ìƒ": "ë³´í†µ",
                                "ë³´í†µ ì´ìƒ ë†’ìŒ": "ë†’ìŒ",
                                "medium": "ë³´í†µ",
                                "low": "ê²½ë¯¸",
                                "high": "ë†’ìŒ",
                                "very high": "ë§¤ìš° ë†’ìŒ",
                            }
                            
                            if original_risk_level not in valid_risk_levels:
                                # ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì°¾ê¸°
                                normalized = risk_level_mapping.get(original_risk_level)
                                if normalized:
                                    logger.warning(f"[riskLevel ì •ê·œí™”] '{original_risk_level}' -> '{normalized}'ë¡œ ë³€ê²½")
                                    parsed_json["riskLevel"] = normalized
                                else:
                                    # ë§¤í•‘ í…Œì´ë¸”ì— ì—†ìœ¼ë©´ nullë¡œ ì„¤ì •
                                    logger.warning(f"[riskLevel ì •ê·œí™”] ì•Œ ìˆ˜ ì—†ëŠ” ê°’ '{original_risk_level}' -> nullë¡œ ë³€ê²½")
                                    parsed_json["riskLevel"] = None
                            
                            # ì •ê·œí™”ëœ JSONì„ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜
                            response_clean = json.dumps(parsed_json, ensure_ascii=False, indent=2)
                        
                        # ì°¸ê³  ë¬¸êµ¬ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì¶”ê°€í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” JSONë§Œ ë°˜í™˜
                        return response_clean
                    except json.JSONDecodeError as e:
                        logger.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        logger.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 500ì): {response_text[:500]}")
                        logger.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] ì¶”ì¶œ ì‹œë„í•œ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì): {response_clean[:500]}")
                        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬)
                        if "ì „ë¬¸ê°€ ìƒë‹´" not in response_text and "ë²•ë¥  ìë¬¸" not in response_text:
                            response_text += "\n\n---\n\n**âš ï¸ ì°¸ê³ :** ì´ ë‹µë³€ì€ ì •ë³´ ì•ˆë‚´ë¥¼ ìœ„í•œ ê²ƒì´ë©° ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬ì•ˆì€ ì „ë¬¸ ë³€í˜¸ì‚¬ë‚˜ ë…¸ë™ìœ„ì›íšŒ ë“± ì „ë¬¸ ê¸°ê´€ì— ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                        return response_text
                
                return response_text
            
            # Ollama ì‚¬ìš© (ë ˆê±°ì‹œ)
            if self.generator.use_ollama:
                # langchain-ollama ìš°ì„  ì‚¬ìš©
                try:
                    from langchain_ollama import OllamaLLM
                    llm = OllamaLLM(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                except ImportError:
                    # ëŒ€ì•ˆ: langchain-community ì‚¬ìš©
                    from langchain_community.llms import Ollama
                    llm = Ollama(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                
                # ëŒ€ëµì ì¸ ì…ë ¥ í† í° ì¶”ì •
                estimated_input_tokens = len(prompt) // 2.5
                logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì…ë ¥ ì¶”ì •: ì•½ {int(estimated_input_tokens)}í† í° (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì)")
                
                response_text = llm.invoke(prompt)
                
                # ëŒ€ëµì ì¸ ì¶œë ¥ í† í° ì¶”ì •
                if response_text:
                    estimated_output_tokens = len(response_text) // 2.5
                    estimated_total_tokens = int(estimated_input_tokens) + int(estimated_output_tokens)
                    logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì¶œë ¥ ì¶”ì •: ì•½ {int(estimated_output_tokens)}í† í°, ì´ ì¶”ì •: ì•½ {estimated_total_tokens}í† í° (ëª¨ë¸: {settings.ollama_model})")
                
                # LLM ì¶œë ¥ ë¡œê¹…
                logger.info("=" * 80)
                logger.info("[LLM OUTPUT] Legal Chat Response (Ollama)")
                logger.info("=" * 80)
                logger.info(f"Response Length: {len(response_text)} characters")
                logger.info(f"Response Content:\n{response_text}")
                logger.info("=" * 80)
                
                # ìƒí™©ë¶„ì„ì¼ ë•ŒëŠ” ```json ì½”ë“œ ë¸”ë¡ í˜•ì‹ ê·¸ëŒ€ë¡œ ë°˜í™˜
                if context_type == 'situation':
                    # ```json ì½”ë“œ ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                    response_clean = response_text.strip()
                    if response_clean.startswith('```json') or response_clean.startswith('```'):
                        # ì´ë¯¸ ì½”ë“œ ë¸”ë¡ í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
                        logger.info(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] ì½”ë“œ ë¸”ë¡ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê¸¸ì´: {len(response_clean)} characters)")
                        return response_clean
                    else:
                        # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                        # JSON ê°ì²´ ì°¾ê¸°
                        first_brace = response_clean.find('{')
                        if first_brace != -1:
                            json_str = response_clean[first_brace:]
                            brace_count = 0
                            last_valid_pos = -1
                            for i, char in enumerate(json_str):
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        last_valid_pos = i + 1
                                        break
                            
                            if last_valid_pos > 0:
                                json_content = json_str[:last_valid_pos].strip()
                                # JSON ìœ íš¨ì„± ê²€ì¦
                                try:
                                    json.loads(json_content)
                                    logger.info(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] JSON ê²€ì¦ ì„±ê³µ, ì½”ë“œ ë¸”ë¡ í˜•ì‹ìœ¼ë¡œ ë³€í™˜")
                                    return f"```json\n{json_content}\n```"
                                except json.JSONDecodeError:
                                    logger.warning(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ ë°˜í™˜")
                                    return response_text
                        # JSONì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
                        logger.warning(f"[ìƒí™©ë¶„ì„ ì‘ë‹µ] JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì›ë³¸ ë°˜í™˜")
                        return response_text
                
                # ê³„ì•½ì„œ ë¶„ì„ì¼ ë•Œë„ JSONë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
                if context_type == 'contract' or context_type == 'none':
                    # JSON ì¶”ì¶œ ë¡œì§ (ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ ì œê±°)
                    response_clean = response_text.strip()
                    
                    # 1. JSON ì½”ë“œ ë¸”ë¡ ì°¾ê¸° (```json ... ```) - ì²« ë²ˆì§¸ ê²ƒë§Œ
                    json_block_match = re.search(r'```(?:json)?\s*(\{[\s\S]*?\})\s*```', response_clean, re.DOTALL)
                    if json_block_match:
                        response_clean = json_block_match.group(1).strip()
                    else:
                        # 2. ì§ì ‘ JSON ê°ì²´ ì°¾ê¸° (ì²« ë²ˆì§¸ { ... } ì¶”ì¶œ)
                        # ì¤‘ê´„í˜¸ ë§¤ì¹­í•˜ì—¬ ì™„ì „í•œ JSON ê°ì²´ ì¶”ì¶œ
                        first_brace = response_clean.find('{')
                        if first_brace != -1:
                            json_str = response_clean[first_brace:]
                            brace_count = 0
                            last_valid_pos = -1
                            for i, char in enumerate(json_str):
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        last_valid_pos = i + 1
                                        break
                            
                            if last_valid_pos > 0:
                                response_clean = json_str[:last_valid_pos].strip()
                            else:
                                # ì¤‘ê´„í˜¸ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ ì‹œë„
                                json_match = re.search(r'\{[\s\S]*\}', response_clean, re.DOTALL)
                                if json_match:
                                    response_clean = json_match.group(0).strip()
                    
                    # JSON ìœ íš¨ì„± ê²€ì¦
                    try:
                        parsed_json = json.loads(response_clean)
                        logger.info(f"[JSON ì¶”ì¶œ ì„±ê³µ] ì¶”ì¶œëœ JSON ê¸¸ì´: {len(response_clean)} characters")
                        logger.info(f"[JSON ì¶”ì¶œ ì„±ê³µ] summary: {parsed_json.get('summary', 'N/A')[:50]}...")
                        
                        # riskLevel ê°’ ì •ê·œí™” (ì˜ëª»ëœ ê°’ ìˆ˜ì •)
                        if "riskLevel" in parsed_json:
                            original_risk_level = parsed_json["riskLevel"]
                            valid_risk_levels = ["ê²½ë¯¸", "ë³´í†µ", "ë†’ìŒ", "ë§¤ìš° ë†’ìŒ", None]
                            
                            # ì˜ëª»ëœ ê°’ ë§¤í•‘
                            risk_level_mapping = {
                                "ì¤‘ë“±": "ë³´í†µ",
                                "ì¤‘ê°„": "ë³´í†µ",
                                "ë‚®ìŒ": "ê²½ë¯¸",
                                "ë³´í†µ ì´ìƒ": "ë³´í†µ",
                                "ë³´í†µ ì´ìƒ ë†’ìŒ": "ë†’ìŒ",
                                "medium": "ë³´í†µ",
                                "low": "ê²½ë¯¸",
                                "high": "ë†’ìŒ",
                                "very high": "ë§¤ìš° ë†’ìŒ",
                            }
                            
                            if original_risk_level not in valid_risk_levels:
                                # ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì°¾ê¸°
                                normalized = risk_level_mapping.get(original_risk_level)
                                if normalized:
                                    logger.warning(f"[riskLevel ì •ê·œí™”] '{original_risk_level}' -> '{normalized}'ë¡œ ë³€ê²½")
                                    parsed_json["riskLevel"] = normalized
                                else:
                                    # ë§¤í•‘ í…Œì´ë¸”ì— ì—†ìœ¼ë©´ nullë¡œ ì„¤ì •
                                    logger.warning(f"[riskLevel ì •ê·œí™”] ì•Œ ìˆ˜ ì—†ëŠ” ê°’ '{original_risk_level}' -> nullë¡œ ë³€ê²½")
                                    parsed_json["riskLevel"] = None
                            
                            # ì •ê·œí™”ëœ JSONì„ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜
                            response_clean = json.dumps(parsed_json, ensure_ascii=False, indent=2)
                        
                        # ì°¸ê³  ë¬¸êµ¬ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì¶”ê°€í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” JSONë§Œ ë°˜í™˜
                        return response_clean
                    except json.JSONDecodeError as e:
                        logger.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        logger.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 500ì): {response_text[:500]}")
                        logger.warning(f"[JSON ì¶”ì¶œ ì‹¤íŒ¨] ì¶”ì¶œ ì‹œë„í•œ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì): {response_clean[:500]}")
                        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬)
                        if "ì „ë¬¸ê°€ ìƒë‹´" not in response_text and "ë²•ë¥  ìë¬¸" not in response_text:
                            response_text += "\n\n---\n\n**âš ï¸ ì°¸ê³ :** ì´ ë‹µë³€ì€ ì •ë³´ ì•ˆë‚´ë¥¼ ìœ„í•œ ê²ƒì´ë©° ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬ì•ˆì€ ì „ë¬¸ ë³€í˜¸ì‚¬ë‚˜ ë…¸ë™ìœ„ì›íšŒ ë“± ì „ë¬¸ ê¸°ê´€ì— ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                        return response_text
                
                # í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (í•œê¸€ ìœ ë‹ˆì½”ë“œ ë²”ìœ„: AC00-D7A3)
                # ì²« 200ì ì¤‘ í•œêµ­ì–´ê°€ ì—†ìœ¼ë©´ ì¬ì‹œë„
                if response_text and len(response_text) > 0:
                    first_chars = response_text[:200]
                    has_korean = any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in first_chars)
                    
                    if not has_korean:
                        # ì˜ì–´ë¡œ ë‹µë³€í•œ ê²½ìš° ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                        retry_prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì— ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.

{LEGAL_CHAT_SYSTEM_PROMPT}

**ì‚¬ìš©ì ì§ˆë¬¸:**
{query}
{issue_context}
{analysis_context}

**ê´€ë ¨ ë²•ë ¹/ê°€ì´ë“œ/ì¼€ì´ìŠ¤:**
{context}

**âš ï¸ ë§¤ìš° ì¤‘ìš”:**
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
- ì˜ì–´ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ êµ¬ì¡°ë¡œ **í•œêµ­ì–´ë¡œë§Œ** ë‹µë³€í•´ì£¼ì„¸ìš”:

## ìš”ì•½ ê²°ë¡ 
[í•œ ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‹µë³€ (í•œêµ­ì–´)]

## ì™œ ìœ„í—˜í•œì§€ (ë²•ì  ë¦¬ìŠ¤í¬)
[ê´€ë ¨ ë²•ë ¹ì„ ê·¼ê±°ë¡œ ìœ„í—˜ì„± ì„¤ëª… (í•œêµ­ì–´)]

## ì‹¤ë¬´ í˜‘ìƒ í¬ì¸íŠ¸
[í˜„ì‹¤ì ì¸ í˜‘ìƒ ì˜µì…˜ê³¼ ëŒ€ì•ˆ ì œì‹œ (í•œêµ­ì–´)]

## ì°¸ê³  ë²•ë ¹/í‘œì¤€ ê³„ì•½
[ê´€ë ¨ ë²•ë ¹ ìš”ì•½ ë° ì¶œì²˜ (í•œêµ­ì–´)]
"""
                        response_text = llm.invoke(retry_prompt)
                        
                        # ì¬ì‹œë„ í›„ LLM ì¶œë ¥ ë¡œê¹…
                        logger.info("=" * 80)
                        logger.info("[LLM OUTPUT] Legal Chat Response (Ollama - Retry)")
                        logger.info("=" * 80)
                        logger.info(f"Response Length: {len(response_text)} characters")
                        logger.info(f"Response Content:\n{response_text}")
                        logger.info("=" * 80)
                
                # ìƒí™©ë¶„ì„ì¼ ë•ŒëŠ” JSON í˜•ì‹ì´ë¯€ë¡œ ì°¸ê³  ë¬¸êµ¬ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ í¬í•¨ë¨)
                # ê³„ì•½ì„œ ë¶„ì„ì¼ ë•Œë§Œ ì°¸ê³  ë¬¸êµ¬ ì¶”ê°€
                if context_type != 'situation':
                    if "ì „ë¬¸ê°€ ìƒë‹´" not in response_text and "ë²•ë¥  ìë¬¸" not in response_text:
                        response_text += "\n\n---\n\n**âš ï¸ ì°¸ê³ :** ì´ ë‹µë³€ì€ ì •ë³´ ì•ˆë‚´ë¥¼ ìœ„í•œ ê²ƒì´ë©° ë²•ë¥  ìë¬¸ì´ ì•„ë‹™ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‚¬ì•ˆì€ ì „ë¬¸ ë³€í˜¸ì‚¬ë‚˜ ë…¸ë™ìœ„ì›íšŒ ë“± ì „ë¬¸ ê¸°ê´€ì— ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                
                return response_text
        except Exception as e:
            logger.error(f"LLM ì±„íŒ… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
        
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return f"ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” {len(grounding_chunks)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    async def _llm_situation_diagnosis(
        self,
        category_hint: str,
        situation_text: str,
        grounding_chunks: List[LegalGroundingChunk],
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> dict:
        """
        ìƒí™© ê¸°ë°˜ ìƒì„¸ ì§„ë‹¨ìš© LLM ì‘ë‹µ ìƒì„±
        """
        # loggerë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì°¸ì¡° (ìŠ¤ì½”í”„ ë¬¸ì œ ë°©ì§€)
        _logger = logging.getLogger(__name__)
        
        if self.generator.disable_llm:
            # LLM ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ ì‘ë‹µ (grounding_chunks í¬í•¨)
            return {
                "classified_type": category_hint,
                "risk_score": 50,
                "summary": "LLM ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. RAG ê²€ìƒ‰ ê²°ê³¼ë§Œ ì œê³µë©ë‹ˆë‹¤.",
                "findings": [],  # LLM ë¹„í™œì„±í™” ì‹œ ë¹ˆ ë°°ì—´
                "criteria": [],
                "action_plan": {"steps": []},
                "scripts": {
                    "to_company": {
                        "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                        "body": "ìƒí™©ì„ ë¶„ì„í•œ ê²°ê³¼, ê´€ë ¨ ë²•ë ¹ ë° í‘œì¤€ê³„ì•½ì„œë¥¼ ì°¸ê³ í•˜ì—¬ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                    },
                    "to_advisor": {
                        "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                        "body": "ê·¼ë¡œ ê´€ë ¨ ë¬¸ì œë¡œ ìƒë‹´ì„ ë°›ê³ ì í•©ë‹ˆë‹¤. ìƒí™©ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ìƒë‹´ ì‹œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    }
                },
                "related_cases": [],
                "grounding_chunks": grounding_chunks,  # RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” í¬í•¨
                "organizations": [],  # LLM ë¹„í™œì„±í™” ì‹œ ë¹ˆ ë°°ì—´
            }
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
        prompt = build_situation_analysis_prompt(
            situation_text=situation_text,
            category_hint=category_hint,
            grounding_chunks=grounding_chunks,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        

        try:
            # Groq ì‚¬ìš© (ìš°ì„ )
            from config import settings
            import json
            import re
            
            if settings.use_groq:
                from llm_api import ask_groq_with_messages
                
                # í”„ë¡¬í”„íŠ¸ë¥¼ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                messages = [
                    {"role": "system", "content": "ë„ˆëŠ” ìœ ëŠ¥í•œ ë²•ë¥  AIì•¼. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ]
                
                try:
                    response_text = ask_groq_with_messages(
                        messages=messages,
                        temperature=settings.llm_temperature,
                        model=settings.groq_model
                    )
                    _logger.info(f"[Groq í˜¸ì¶œ ì„±ê³µ] ì‘ë‹µ ê¸¸ì´: {len(response_text) if response_text else 0}ì")
                except Exception as groq_error:
                    _logger.error(f"[Groq í˜¸ì¶œ ì‹¤íŒ¨] {str(groq_error)}", exc_info=True)
                    raise  # ìƒìœ„ exceptë¡œ ì „ë‹¬
            # Ollama ì‚¬ìš© (ë ˆê±°ì‹œ)
            elif self.generator.use_ollama:
                # langchain-ollama ìš°ì„  ì‚¬ìš©
                try:
                    from langchain_ollama import OllamaLLM
                    llm = OllamaLLM(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                except ImportError:
                    # ëŒ€ì•ˆ: langchain-community ì‚¬ìš©
                    from langchain_community.llms import Ollama
                    llm = Ollama(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model
                    )
                
                # ëŒ€ëµì ì¸ ì…ë ¥ í† í° ì¶”ì •
                estimated_input_tokens = len(prompt) // 2.5
                _logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì…ë ¥ ì¶”ì •: ì•½ {int(estimated_input_tokens)}í† í° (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì)")
                
                response_text = llm.invoke(prompt)
                
                # ëŒ€ëµì ì¸ ì¶œë ¥ í† í° ì¶”ì •
                if response_text:
                    estimated_output_tokens = len(response_text) // 2.5
                    estimated_total_tokens = int(estimated_input_tokens) + int(estimated_output_tokens)
                    _logger.info(f"[í† í° ì‚¬ìš©ëŸ‰] ì¶œë ¥ ì¶”ì •: ì•½ {int(estimated_output_tokens)}í† í°, ì´ ì¶”ì •: ì•½ {estimated_total_tokens}í† í° (ëª¨ë¸: {settings.ollama_model})")
            else:
                # Groqì™€ Ollama ëª¨ë‘ ì‚¬ìš© ì•ˆ í•¨
                raise ValueError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. use_groq ë˜ëŠ” use_ollamaë¥¼ Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
            
            # JSON ì¶”ì¶œ ë° íŒŒì‹± (Groqì™€ Ollama ëª¨ë‘ ê³µí†µ)
            try:
                # 1. ì™¸ë¶€ ì½”ë“œ ë¸”ë¡ ì œê±° (```json, ``` ë“±)
                response_clean = response_text.strip()
                if response_clean.startswith("```json"):
                    response_clean = response_clean[7:]
                elif response_clean.startswith("```"):
                    response_clean = response_clean[3:]
                if response_clean.endswith("```"):
                    response_clean = response_clean[:-3]
                response_clean = response_clean.strip()
                
                # 2. JSON ê°ì²´ ì¶”ì¶œ
                json_match = re.search(r'\{.*\}', response_clean, re.DOTALL)
                if not json_match:
                    _logger.warning(f"LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‘ë‹µ (ì²˜ìŒ 500ì): {response_clean[:500]}")
                    raise ValueError("LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                json_str = json_match.group()
                
                # JSON íŒŒì‹± ì „ì— summary í•„ë“œì˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                # summary í•„ë“œ ì „ì²´ë¥¼ ì°¾ì•„ì„œ ì •ë¦¬ (ë‹¤ì¤‘ ë¼ì¸, ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ í¬í•¨)
                def clean_summary_field_in_json(json_str):
                    """summary í•„ë“œ ë‚´ë¶€ì˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ê³¼ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì •ë¦¬"""
                    try:
                        # Python ì‚¼ì¤‘ ë”°ì˜´í‘œ ì œê±° (""" ... """)
                        json_str = re.sub(r'"""\s*', '"', json_str)  # ì‹œì‘ ì‚¼ì¤‘ ë”°ì˜´í‘œ
                        json_str = re.sub(r'\s*"""', '"', json_str)  # ë ì‚¼ì¤‘ ë”°ì˜´í‘œ
                        
                        # summary í•„ë“œì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
                        summary_start = json_str.find('"summary"')
                        if summary_start == -1:
                            return json_str
                        
                        # summary í•„ë“œì˜ ê°’ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (ì½œë¡ ê³¼ ë”°ì˜´í‘œ ì´í›„)
                        value_start = json_str.find('"', summary_start + 9)  # "summary" ê¸¸ì´ + 1
                        if value_start == -1:
                            return json_str
                        
                        value_start += 1  # ë”°ì˜´í‘œ ë‹¤ìŒë¶€í„°
                        
                        # ë¬¸ìì—´ ë ì°¾ê¸° (ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ ê³ ë ¤)
                        # ë°±ìŠ¬ë˜ì‹œê°€ í™€ìˆ˜ ê°œ ì—°ì†ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ
                        value_end = value_start
                        brace_count = 0  # ì¤‘ì²©ëœ ê°ì²´/ë°°ì—´ ì¶”ì 
                        in_string = True
                        
                        while value_end < len(json_str):
                            char = json_str[value_end]
                            
                            # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ì ê±´ë„ˆë›°ê¸°
                            if char == '\\' and value_end + 1 < len(json_str):
                                value_end += 2
                                continue
                            
                            # ë”°ì˜´í‘œ ì²˜ë¦¬
                            if char == '"':
                                # ì•ì˜ ë°±ìŠ¬ë˜ì‹œ ê°œìˆ˜ ì„¸ê¸°
                                backslash_count = 0
                                i = value_end - 1
                                while i >= value_start and json_str[i] == '\\':
                                    backslash_count += 1
                                    i -= 1
                                # í™€ìˆ˜ ê°œì˜ ë°±ìŠ¬ë˜ì‹œë©´ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ, ì§ìˆ˜ ê°œë©´ ë¬¸ìì—´ ë
                                if backslash_count % 2 == 0:
                                    break
                            
                            value_end += 1
                        
                        if value_end >= len(json_str):
                            # ë¬¸ìì—´ ëì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë‹¤ìŒ í°ë”°ì˜´í‘œê¹Œì§€ ì°¾ê¸°
                            next_quote = json_str.find('"', value_start)
                            if next_quote > value_start:
                                value_end = next_quote
                            else:
                                return json_str
                        
                        # summary í•„ë“œ ë‚´ìš© ì¶”ì¶œ
                        content = json_str[value_start:value_end]
                        
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜ (ì¼ì‹œì )
                        content_decoded = content.replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t').replace('\\"', '"').replace('\\\\', '\\')
                        
                        # Python ì‚¼ì¤‘ ë”°ì˜´í‘œ ì œê±° (""" ... """)
                        content_decoded = re.sub(r'"""\s*', '', content_decoded)
                        content_decoded = re.sub(r'\s*"""', '', content_decoded)
                        
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                        content_decoded = re.sub(r'```markdown\s*', '', content_decoded, flags=re.IGNORECASE)
                        content_decoded = re.sub(r'```\s*', '', content_decoded, flags=re.MULTILINE)
                        
                        # ì œì–´ ë¬¸ìë¥¼ JSON ì´ìŠ¤ì¼€ì´í”„ë¡œ ë³€í™˜
                        result = []
                        for char in content_decoded:
                            if char == '\n':
                                result.append('\\n')
                            elif char == '\r':
                                result.append('\\r')
                            elif char == '\t':
                                result.append('\\t')
                            elif char == '"':
                                result.append('\\"')
                            elif char == '\\':
                                result.append('\\\\')
                            elif ord(char) < 32:
                                result.append(f'\\u{ord(char):04x}')
                            else:
                                result.append(char)
                        
                        # summary í•„ë“œ êµì²´
                        cleaned_content = ''.join(result)
                        return json_str[:value_start] + cleaned_content + json_str[value_end:]
                    except Exception as e:
                        _logger.warning(f"summary í•„ë“œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}, ì›ë³¸ JSON ì‚¬ìš©")
                        return json_str
                
                # summary í•„ë“œ ì •ë¦¬
                json_str_cleaned = clean_summary_field_in_json(json_str)
                
                # ì œì–´ ë¬¸ì ì²˜ë¦¬ (ì „ì²´ JSON ë¬¸ìì—´)
                json_str_cleaned = json_str_cleaned.replace('\t', ' ').replace('\r', '')
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    diagnosis = json.loads(json_str_cleaned)
                except json.JSONDecodeError as json_err:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë” ê°•ë ¥í•œ ì •ë¦¬ ì‹œë„
                    _logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì¶”ê°€ ì •ë¦¬ ì‹œë„ ì¤‘...: {str(json_err)}")
                    
                    # ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ìœ íš¨í•œ JSON ì¶”ì¶œ
                    brace_count = 0
                    last_valid_pos = -1
                    for i, char in enumerate(json_str_cleaned):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                last_valid_pos = i + 1
                                break
                    
                    if last_valid_pos > 0:
                        json_str_cleaned = json_str_cleaned[:last_valid_pos]
                        try:
                            diagnosis = json.loads(json_str_cleaned)
                        except json.JSONDecodeError:
                            _logger.error(f"JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {str(json_err)}")
                            raise json_err
                    else:
                        raise json_err
                
                # summary í•„ë“œì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (íŒŒì‹± í›„)
                summary = diagnosis.get("summary", "ìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.")
                if summary:
                    # ```markdown ... ``` ì œê±°
                    summary = re.sub(r'```markdown\s*', '', summary, flags=re.IGNORECASE)
                    summary = re.sub(r'```\s*$', '', summary, flags=re.MULTILINE)
                    
                    # í•œì/ì¼ë³¸ì–´ ë¬¸ìë¥¼ í•œê¸€ë¡œ ë³€í™˜ ë˜ëŠ” ì œê±°
                    def remove_cjk_japanese(text: str) -> str:
                        """í•œì, ì¼ë³¸ì–´ ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ í•œê¸€ë¡œ ë³€í™˜"""
                        import unicodedata
                        
                        # ì¼ë°˜ì ì¸ í•œì-í•œê¸€ ë§¤í•‘
                        hanja_to_hangul = {
                            'æœ€è¿‘': 'ìµœê·¼',
                            'å…¸å‹': 'ì „í˜•',
                            'å…¸å‹ì ì¸': 'ì „í˜•ì ì¸',
                        }
                        
                        # ë§¤í•‘ëœ í•œì ë³€í™˜
                        for hanja, hangul in hanja_to_hangul.items():
                            text = text.replace(hanja, hangul)
                        
                        # í•œì ë²”ìœ„ (CJK í†µí•© í•œì: U+4E00â€“U+9FFF, í•œì ë³´ì¶©: U+3400â€“U+4DBF)
                        # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: U+3040â€“U+309F, ê°€íƒ€ì¹´ë‚˜: U+30A0â€“U+30FF
                        result = []
                        for char in text:
                            code = ord(char)
                            # í•œì ë²”ìœ„ ì²´í¬
                            is_hanja = (0x4E00 <= code <= 0x9FFF) or (0x3400 <= code <= 0x4DBF)
                            # ì¼ë³¸ì–´ ë²”ìœ„ ì²´í¬
                            is_japanese = (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF)
                            
                            if is_hanja or is_japanese:
                                # í•œì/ì¼ë³¸ì–´ ë¬¸ìëŠ” ì œê±°
                                _logger.debug(f"[LegalRAG] í•œì/ì¼ë³¸ì–´ ë¬¸ì ì œê±°: {char} (U+{code:04X})")
                                continue
                            result.append(char)
                        
                        return ''.join(result)
                    
                    summary = remove_cjk_japanese(summary)
                    
                    summary = summary.strip()
                
                # scripts ë³€í™˜ (ë ˆê±°ì‹œ í˜•ì‹ ì§€ì›)
                scripts_raw = diagnosis.get("scripts", {})
                scripts = {}
                if isinstance(scripts_raw, dict):
                    # to_company ë³€í™˜
                    to_company_raw = scripts_raw.get("to_company", {})
                    if isinstance(to_company_raw, str):
                        # ë ˆê±°ì‹œ í˜•ì‹ (ë¬¸ìì—´)
                        scripts["to_company"] = {
                            "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                            "body": to_company_raw[:200] if len(to_company_raw) > 200 else to_company_raw
                        }
                    elif isinstance(to_company_raw, dict) and "subject" in to_company_raw and "body" in to_company_raw:
                        # ìƒˆë¡œìš´ í˜•ì‹
                        scripts["to_company"] = to_company_raw
                    else:
                        scripts["to_company"] = {
                            "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                            "body": ""
                        }
                    
                    # to_advisor ë³€í™˜
                    to_advisor_raw = scripts_raw.get("to_advisor", {})
                    if isinstance(to_advisor_raw, str):
                        # ë ˆê±°ì‹œ í˜•ì‹ (ë¬¸ìì—´)
                        scripts["to_advisor"] = {
                            "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                            "body": to_advisor_raw[:200] if len(to_advisor_raw) > 200 else to_advisor_raw
                        }
                    elif isinstance(to_advisor_raw, dict) and "subject" in to_advisor_raw and "body" in to_advisor_raw:
                        # ìƒˆë¡œìš´ í˜•ì‹
                        scripts["to_advisor"] = to_advisor_raw
                    else:
                        scripts["to_advisor"] = {
                            "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                            "body": ""
                        }
                else:
                    # scriptsê°€ ì—†ê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹
                    scripts = {
                        "to_company": {
                            "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                            "body": ""
                        },
                        "to_advisor": {
                            "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                            "body": ""
                        }
                    }
                
                # findings í•„ë“œ ì¶”ì¶œ (LLM ì‘ë‹µì—ì„œ)
                findings = diagnosis.get("findings", [])
                if not isinstance(findings, list):
                    findings = []
                
                # organizations í•„ë“œ ì¶”ì¶œ (LLM ì‘ë‹µì—ì„œ)
                organizations = diagnosis.get("organizations", [])
                if not isinstance(organizations, list):
                    organizations = []
                
                # ì‘ë‹µ í˜•ì‹ ë³€í™˜
                return {
                    "classified_type": diagnosis.get("classified_type", category_hint),
                    "risk_score": diagnosis.get("risk_score", 50),
                    "summary": summary,
                    "findings": findings,  # LLMì´ ìƒì„±í•œ findings í¬í•¨
                    "criteria": diagnosis.get("criteria", []),
                    "action_plan": diagnosis.get("action_plan", {"steps": []}),
                    "scripts": scripts,
                    "related_cases": [],  # ë‚˜ì¤‘ì— ì¶”ê°€ë¨
                    "organizations": organizations,  # LLMì´ ìƒì„±í•œ organizations í¬í•¨
                }
            except json.JSONDecodeError as e:
                _logger.error(f"LLM ì§„ë‹¨ ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}", exc_info=True)
                _logger.error(f"LLM ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 500ì): {response_text[:500] if response_text else 'None'}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
                raise  # ìƒìœ„ exceptë¡œ ì „ë‹¬í•˜ì—¬ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
            except Exception as e:
                _logger.error(f"LLM ì§„ë‹¨ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}", exc_info=True)
                _logger.error(f"LLM ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 500ì): {response_text[:500] if response_text else 'None'}")
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
                raise  # ìƒìœ„ exceptë¡œ ì „ë‹¬í•˜ì—¬ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜
        except Exception as e:
            _logger.error(f"LLM ì§„ë‹¨ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}", exc_info=True)
            _logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}, ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
        
        # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ (grounding_chunks í¬í•¨)
        # ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì´ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•¨
        logger.warning(f"[ìƒí™©ë¶„ì„] ë ˆê±°ì‹œ ì½”ë“œ ì‹¤í–‰ë¨ - ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì‹œ ì´ ë©”ì‹œì§€ê°€ ë‚˜ì˜¤ë©´ ì•ˆ ë¨")
        return {
            "classified_type": category_hint or "unknown",
            "risk_score": 50,
            "summary": "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\nìƒí™©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²•ì  ê´€ì ê³¼ í–‰ë™ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ ìƒí™©\n\nê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
            "findings": [],  # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´
            "criteria": [],
            "action_plan": {"steps": []},
            "scripts": {
                "to_company": {
                    "subject": "ê·¼ë¡œê³„ì•½ ê´€ë ¨ í™•ì¸ ìš”ì²­",
                    "body": "ìƒí™©ì„ ë¶„ì„í•œ ê²°ê³¼, ê´€ë ¨ ë²•ë ¹ ë° í‘œì¤€ê³„ì•½ì„œë¥¼ ì°¸ê³ í•˜ì—¬ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                },
                "to_advisor": {
                    "subject": "ë…¸ë¬´ ìƒë‹´ ìš”ì²­",
                    "body": "ê·¼ë¡œ ê´€ë ¨ ë¬¸ì œë¡œ ìƒë‹´ì„ ë°›ê³ ì í•©ë‹ˆë‹¤. ìƒí™©ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ìƒë‹´ ì‹œ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                }
            },
            "related_cases": [],
            "grounding_chunks": grounding_chunks,  # RAG ê²€ìƒ‰ ê²°ê³¼ëŠ” í¬í•¨
            "organizations": [],  # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë°°ì—´
        }

